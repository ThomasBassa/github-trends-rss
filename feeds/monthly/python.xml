<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Python, This month</title><link>https://github.com/trending/python?since=monthly</link><description>The top repositories on GitHub for python, measured monthly</description><pubDate>Mon, 11 Nov 2019 01:07:28 GMT</pubDate><lastBuildDate>Mon, 11 Nov 2019 01:07:28 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>facebookresearch/detectron2 #1 in Python, This month</title><link>https://github.com/facebookresearch/detectron2</link><description>&lt;p&gt;&lt;i&gt;Detectron2 is FAIR's next-generation research platform for object detection and segmentation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/Detectron2-Logo-Horz.svg"&gt;&lt;img src=".github/Detectron2-Logo-Horz.svg" width="300" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Detectron2 is Facebook AI Research's next generation software system
that implements state-of-the-art object detection algorithms.
It is a ground-up rewrite of the previous version,
&lt;a href="https://github.com/facebookresearch/Detectron/"&gt;Detectron&lt;/a&gt;,
and it originates from &lt;a href="https://github.com/facebookresearch/maskrcnn-benchmark/"&gt;maskrcnn-benchmark&lt;/a&gt;.&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-whats-new" class="anchor" aria-hidden="true" href="#whats-new"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's New&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It is powered by the &lt;a href="https://pytorch.org" rel="nofollow"&gt;PyTorch&lt;/a&gt; deep learning framework.&lt;/li&gt;
&lt;li&gt;Includes more features such as panoptic segmentation, densepose, Cascade R-CNN, rotated bounding boxes, etc.&lt;/li&gt;
&lt;li&gt;Can be used as a library to support &lt;a href="projects/"&gt;different projects&lt;/a&gt; on top of it.
We'll open source more research projects in this way.&lt;/li&gt;
&lt;li&gt;It &lt;a href="https://detectron2.readthedocs.io/notes/benchmarks.html" rel="nofollow"&gt;trains much faster&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See our &lt;a href="https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/" rel="nofollow"&gt;blog post&lt;/a&gt;
to see more demos and learn about detectron2.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;See &lt;a href="INSTALL.md"&gt;INSTALL.md&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;See &lt;a href="GETTING_STARTED.md"&gt;GETTING_STARTED.md&lt;/a&gt;,
or the &lt;a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5" rel="nofollow"&gt;Colab Notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Learn more at our &lt;a href="https://detectron2.readthedocs.org" rel="nofollow"&gt;documentation&lt;/a&gt;.
And see &lt;a href="projects/"&gt;projects/&lt;/a&gt; for some projects that are built on top of detectron2.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-zoo-and-baselines" class="anchor" aria-hidden="true" href="#model-zoo-and-baselines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model Zoo and Baselines&lt;/h2&gt;
&lt;p&gt;We provide a large set of baseline results and trained models available for download in the &lt;a href="MODEL_ZOO.md"&gt;Detectron2 Model Zoo&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Detectron2 is released under the &lt;a href="LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citing-detectron" class="anchor" aria-hidden="true" href="#citing-detectron"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing Detectron&lt;/h2&gt;
&lt;p&gt;If you use Detectron2 in your research or wish to refer to the baseline results published in the &lt;a href="MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt;, please use the following BibTeX entry.&lt;/p&gt;
&lt;div class="highlight highlight-text-bibtex"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;@misc&lt;/span&gt;{&lt;span class="pl-en"&gt;wu2019detectron2&lt;/span&gt;,
  &lt;span class="pl-s"&gt;author&lt;/span&gt; =       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;Yuxin Wu and Alexander Kirillov and Francisco Massa and&lt;/span&gt;
&lt;span class="pl-s"&gt;                  Wan-Yen Lo and Ross Girshick&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;title&lt;/span&gt; =        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;Detectron2&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;howpublished&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;\url{https://github.com/facebookresearch/detectron2}&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;year&lt;/span&gt; =         &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;2019&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>facebookresearch</author><guid isPermaLink="false">https://github.com/facebookresearch/detectron2</guid><pubDate>Mon, 11 Nov 2019 00:01:00 GMT</pubDate></item><item><title>evilsocket/pwnagotchi #2 in Python, This month</title><link>https://github.com/evilsocket/pwnagotchi</link><description>&lt;p&gt;&lt;i&gt;(⌐■_■) - Deep Reinforcement Learning instrumenting bettercap for WiFi pwning.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pwnagotchi" class="anchor" aria-hidden="true" href="#pwnagotchi"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pwnagotchi&lt;/h1&gt;
&lt;p align="center"&gt;
    &lt;a href="https://github.com/evilsocket/pwnagotchi/releases/latest"&gt;&lt;img alt="Release" src="https://camo.githubusercontent.com/9ae707a55ead5a1e951d8504ebc47fe6f2259198/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6576696c736f636b65742f70776e61676f746368692e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/github/release/evilsocket/pwnagotchi.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://github.com/evilsocket/pwnagotchi/blob/master/LICENSE.md"&gt;&lt;img alt="Software License" src="https://camo.githubusercontent.com/268d96c6dd81f1fff98b19675ef5867412a2a223/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c332d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/license-GPL3-brightgreen.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://github.com/evilsocket/pwnagotchi/graphs/contributors"&gt;&lt;img alt="Contributors" src="https://camo.githubusercontent.com/929754fc02f162895d1d3ac191ff93d5f0deefb2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6576696c736f636b65742f70776e61676f74636869" data-canonical-src="https://img.shields.io/github/contributors/evilsocket/pwnagotchi" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://travis-ci.org/evilsocket/pwnagotchi" rel="nofollow"&gt;&lt;img alt="Travis" src="https://camo.githubusercontent.com/bb71ac99b3141520709ab3799968d900a4e7d193/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f6576696c736f636b65742f70776e61676f746368692f6d61737465722e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/travis/evilsocket/pwnagotchi/master.svg?style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://invite.pwnagotchi.ai/" rel="nofollow"&gt;&lt;img alt="Slack" src="https://camo.githubusercontent.com/8b7d4aafb27072069011eadd46bdf045d3139533/68747470733a2f2f696e766974652e70776e61676f746368692e61692f62616467652e737667" data-canonical-src="https://invite.pwnagotchi.ai/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://community.pwnagotchi.ai/" rel="nofollow"&gt;&lt;img alt="Forum" src="https://camo.githubusercontent.com/daa0ff8b7d7ba2ee06e9bfb8e72eb7b0a54e0649/68747470733a2f2f696d672e736869656c64732e696f2f646973636f757273652f706f7374733f7365727665723d6874747073253341253246253246636f6d6d756e6974792e70776e61676f746368692e6169253246267374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/discourse/posts?server=https%3A%2F%2Fcommunity.pwnagotchi.ai%2F&amp;amp;style=flat-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://twitter.com/intent/follow?screen_name=pwnagotchi" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/342b3854fb5df1b53672e419d06fbd99da5a4eb6/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f70776e61676f746368693f7374796c653d736f6369616c266c6f676f3d74776974746572" alt="follow on Twitter" data-canonical-src="https://img.shields.io/twitter/follow/pwnagotchi?style=social&amp;amp;logo=twitter" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pwnagotchi.ai/" rel="nofollow"&gt;Pwnagotchi&lt;/a&gt; is an &lt;a href="https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752" rel="nofollow"&gt;A2C&lt;/a&gt;-based "AI" leveraging &lt;a href="https://www.bettercap.org/" rel="nofollow"&gt;bettercap&lt;/a&gt; that learns from its surrounding WiFi environment to maximize the crackable WPA key material it captures (either passively, or by performing authentication and association attacks). This material is collected as PCAP files containing any form of handshake supported by &lt;a href="https://hashcat.net/hashcat/" rel="nofollow"&gt;hashcat&lt;/a&gt;, including &lt;a href="https://www.evilsocket.net/2019/02/13/Pwning-WiFi-networks-with-bettercap-and-the-PMKID-client-less-attack/" rel="nofollow"&gt;PMKIDs&lt;/a&gt;,
full and half WPA handshakes.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e8bbbda8fbeb4da294be75c896026fc777c1a199/68747470733a2f2f692e696d6775722e636f6d2f5836384758726e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/e8bbbda8fbeb4da294be75c896026fc777c1a199/68747470733a2f2f692e696d6775722e636f6d2f5836384758726e2e706e67" alt="ui" data-canonical-src="https://i.imgur.com/X68GXrn.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Instead of merely playing &lt;a href="https://becominghuman.ai/getting-mario-back-into-the-gym-setting-up-super-mario-bros-in-openais-gym-8e39a96c1e41?gi=c4b66c3d5ced" rel="nofollow"&gt;Super Mario or Atari games&lt;/a&gt; like most reinforcement learning-based "AI" &lt;em&gt;(yawn)&lt;/em&gt;, Pwnagotchi tunes &lt;a href="https://github.com/evilsocket/pwnagotchi/blob/master/pwnagotchi/defaults.yml#L73"&gt;its parameters&lt;/a&gt; over time to &lt;strong&gt;get better at pwning WiFi things to&lt;/strong&gt; in the environments you expose it to.&lt;/p&gt;
&lt;p&gt;More specifically, Pwnagotchi is using an &lt;a href="https://stable-baselines.readthedocs.io/en/master/modules/policies.html#stable_baselines.common.policies.MlpLstmPolicy" rel="nofollow"&gt;LSTM with MLP feature extractor&lt;/a&gt; as its policy network for the &lt;a href="https://stable-baselines.readthedocs.io/en/master/modules/a2c.html" rel="nofollow"&gt;A2C agent&lt;/a&gt;. If you're unfamiliar with A2C, here is &lt;a href="https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752" rel="nofollow"&gt;a very good introductory explanation&lt;/a&gt; (in comic form!) of the basic principles behind how Pwnagotchi learns. (You can read more about how Pwnagotchi learns in the &lt;a href="https://www.pwnagotchi.ai/usage/#training-the-ai" rel="nofollow"&gt;Usage&lt;/a&gt; doc.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Keep in mind:&lt;/strong&gt; Unlike the usual RL simulations, Pwnagotchi learns over time. Time for a Pwnagotchi is measured in epochs; a single epoch can last from a few seconds to minutes, depending on how many access points and client stations are visible. Do not expect your Pwnagotchi to perform amazingly well at the very beginning, as it will be &lt;a href="https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752" rel="nofollow"&gt;exploring&lt;/a&gt; several combinations of &lt;a href="https://www.pwnagotchi.ai/usage/#training-the-ai" rel="nofollow"&gt;key parameters&lt;/a&gt; to determine ideal adjustments for pwning the particular environment you are exposing it to during its beginning epochs ... but ** listen to your Pwnagotchi when it tells you it's boring!** Bring it into novel WiFi environments with you and have it observe new networks and capture new handshakes—and you'll see. :)&lt;/p&gt;
&lt;p&gt;Multiple units within close physical proximity can "talk" to each other, advertising their presence to each other by broadcasting custom information elements using a parasite protocol I've built on top of the existing dot11 standard. Over time, two or more units trained together will learn to cooperate upon detecting each other's presence by dividing the available channels among them for optimal pwnage.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.pwnagotchi.ai" rel="nofollow"&gt;https://www.pwnagotchi.ai&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-links" class="anchor" aria-hidden="true" href="#links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Links&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt; &lt;/th&gt;
&lt;th&gt;Official Links&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Website&lt;/td&gt;
&lt;td&gt;&lt;a href="https://pwnagotchi.ai/" rel="nofollow"&gt;pwnagotchi.ai&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Forum&lt;/td&gt;
&lt;td&gt;&lt;a href="https://community.pwnagotchi.ai/" rel="nofollow"&gt;community.pwnagotchi.ai&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Slack&lt;/td&gt;
&lt;td&gt;&lt;a href="https://invite.pwnagotchi.ai/" rel="nofollow"&gt;pwnagotchi.slack.com&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Subreddit&lt;/td&gt;
&lt;td&gt;&lt;a href="https://www.reddit.com/r/pwnagotchi/" rel="nofollow"&gt;r/pwnagotchi&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Twitter&lt;/td&gt;
&lt;td&gt;&lt;a href="https://twitter.com/pwnagotchi" rel="nofollow"&gt;@pwnagotchi&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;pwnagotchi&lt;/code&gt; is made with ♥  by &lt;a href="https://twitter.com/evilsocket" rel="nofollow"&gt;@evilsocket&lt;/a&gt; and the &lt;a href="https://github.com/evilsocket/pwnagotchi/graphs/contributors"&gt;amazing dev team&lt;/a&gt;. It is released under the GPL3 license.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>evilsocket</author><guid isPermaLink="false">https://github.com/evilsocket/pwnagotchi</guid><pubDate>Mon, 11 Nov 2019 00:02:00 GMT</pubDate></item><item><title>streamlit/streamlit #3 in Python, This month</title><link>https://github.com/streamlit/streamlit</link><description>&lt;p&gt;&lt;i&gt;Streamlit — The fastest way to build custom ML tools&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-welcome-to-streamlit-wave" class="anchor" aria-hidden="true" href="#welcome-to-streamlit-wave"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Welcome to Streamlit &lt;g-emoji class="g-emoji" alias="wave" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png"&gt;👋&lt;/g-emoji&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The fastest way to build custom ML tools.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Streamlit lets you create apps for your machine learning projects with deceptively simple Python scripts. It supports hot-reloading, so your app updates live as you edit and save your file. No need to mess with HTTP requests, HTML, JavaScript, etc. All you need is your favorite editor and a browser. Take a look at Streamlit in action:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5ae1dcfd188be26bbb0648fb62e9d6d593dbb6f5/68747470733a2f2f617773312e646973636f757273652d63646e2e636f6d2f7374616e6461726431302f75706c6f6164732f73747265616d6c69742f6f726967696e616c2f31582f323932653938356637663735656637626566386332376235383939663731663736636435373765302e676966"&gt;&lt;img src="https://camo.githubusercontent.com/5ae1dcfd188be26bbb0648fb62e9d6d593dbb6f5/68747470733a2f2f617773312e646973636f757273652d63646e2e636f6d2f7374616e6461726431302f75706c6f6164732f73747265616d6c69742f6f726967696e616c2f31582f323932653938356637663735656637626566386332376235383939663731663736636435373765302e676966" alt="Example of live coding a dashboard in Streamlit|635x380" data-canonical-src="https://aws1.discourse-cdn.com/standard10/uploads/streamlit/original/1X/292e985f7f75ef7bef8c27b5899f71f76cd577e0.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Check out our &lt;a href="https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace" rel="nofollow"&gt;launch blog post&lt;/a&gt;!!&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install streamlit
streamlit hello&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-example" class="anchor" aria-hidden="true" href="#example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h2&gt;
&lt;p&gt;Streamlit lets you build interactive apps ridiculously easily:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; streamlit &lt;span class="pl-k"&gt;as&lt;/span&gt; st

x &lt;span class="pl-k"&gt;=&lt;/span&gt; st.slider(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Select a value&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
st.write(x, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;squared is&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, x &lt;span class="pl-k"&gt;*&lt;/span&gt; x)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1e18efff3f06946e9d1559712cea0cb76364f004/68747470733a2f2f73747265616d6c69742d64656d6f2d646174612e73332d75732d776573742d322e616d617a6f6e6177732e636f6d2f737175617265642d696d6167652d666f722d6769746875622d726561646d652d322e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/1e18efff3f06946e9d1559712cea0cb76364f004/68747470733a2f2f73747265616d6c69742d64656d6f2d646174612e73332d75732d776573742d322e616d617a6f6e6177732e636f6d2f737175617265642d696d6167652d666f722d6769746875622d726561646d652d322e706e67" width="490/" data-canonical-src="https://streamlit-demo-data.s3-us-west-2.amazonaws.com/squared-image-for-github-readme-2.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-a-bigger-example" class="anchor" aria-hidden="true" href="#a-bigger-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A Bigger Example&lt;/h2&gt;
&lt;p&gt;Despite its simplicity Streamlit lets you build incredibly rich and powerful tools. &lt;a href="https://github.com/streamlit/demo-self-driving"&gt;This demo project&lt;/a&gt; lets you browse the entire &lt;a href="https://github.com/udacity/self-driving-car"&gt;Udacity self-driving-car dataset&lt;/a&gt; and run inference in real time using the &lt;a href="https://pjreddie.com/darknet/yolo" rel="nofollow"&gt;YOLO object detection net&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/streamlit/demo-self-driving/master/av_final_optimized.gif"&gt;&lt;img src="https://raw.githubusercontent.com/streamlit/demo-self-driving/master/av_final_optimized.gif" alt="Making-of Animation" title="Making-of Animation" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The complete demo is implemented in less than 300 lines of Python. In fact, the app contains &lt;a href="https://github.com/streamlit/demo-self-driving/blob/master/app.py"&gt;only 23 Streamlit calls&lt;/a&gt; which illustrates all the major building blocks of Streamlit. You can try it right now with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install --upgrade streamlit opencv-python
streamlit run https://raw.githubusercontent.com/streamlit/demo-self-driving/master/app.py&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-more-information" class="anchor" aria-hidden="true" href="#more-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Our &lt;a href="https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace" rel="nofollow"&gt;launch post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Our lovely &lt;a href="https://discuss.streamlit.io/" rel="nofollow"&gt;community&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Streamlit &lt;a href="https://streamlit.io/docs" rel="nofollow"&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;More &lt;a href="https://github.com/streamlit/"&gt;demo projects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you would like to contribute, see &lt;a href="https://github.com/streamlit/streamlit/wiki/Contributing"&gt;instructions here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-streamlit-for-teams" class="anchor" aria-hidden="true" href="#streamlit-for-teams"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Streamlit for Teams&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://streamlit.io/forteams/" rel="nofollow"&gt;Streamlit for Teams&lt;/a&gt; is our enterprise edition, with single-click deploy, authentication, web editing, versioning, and more. Please contact us if you would like to learn more.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Streamlit is completely free and open source and licensed under the &lt;a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;Apache 2.0&lt;/a&gt; license.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>streamlit</author><guid isPermaLink="false">https://github.com/streamlit/streamlit</guid><pubDate>Mon, 11 Nov 2019 00:03:00 GMT</pubDate></item><item><title>openai/gpt-2 #4 in Python, This month</title><link>https://github.com/openai/gpt-2</link><description>&lt;p&gt;&lt;i&gt;Code for the paper "Language Models are Unsupervised Multitask Learners"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Archive (code is provided as-is, no updates expected)&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-gpt-2" class="anchor" aria-hidden="true" href="#gpt-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;gpt-2&lt;/h1&gt;
&lt;p&gt;Code and models from the paper &lt;a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" rel="nofollow"&gt;"Language Models are Unsupervised Multitask Learners"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can read about GPT-2 and its staged release in our &lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;original blog post&lt;/a&gt;, &lt;a href="https://openai.com/blog/gpt-2-6-month-follow-up/" rel="nofollow"&gt;6 month follow-up post&lt;/a&gt;, and &lt;a href="https://www.openai.com/blog/gpt-2-1-5b-release/" rel="nofollow"&gt;final post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We have also &lt;a href="https://github.com/openai/gpt-2-output-dataset"&gt;released a dataset&lt;/a&gt; for researchers to study their behaviors.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; &lt;em&gt;Note that our original parameter counts were wrong due to an error (in our previous blog posts and paper).  Thus you may have seen small referred to as 117M and medium referred to as 345M.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;This repository is meant to be a starting point for researchers and engineers to experiment with GPT-2.&lt;/p&gt;
&lt;p&gt;For basic information, see our &lt;a href="./model_card.md"&gt;model card&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-some-caveats" class="anchor" aria-hidden="true" href="#some-caveats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Some caveats&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GPT-2 models' robustness and worst case behaviors are not well-understood.  As with any machine-learned model, carefully evaluate GPT-2 for your use case, especially if used without fine-tuning or in safety-critical applications where reliability is important.&lt;/li&gt;
&lt;li&gt;The dataset our GPT-2 models were trained on contains many texts with &lt;a href="https://twitter.com/TomerUllman/status/1101485289720242177" rel="nofollow"&gt;biases&lt;/a&gt; and factual inaccuracies, and thus GPT-2 models are likely to be biased and inaccurate as well.&lt;/li&gt;
&lt;li&gt;To avoid having samples mistaken as human-written, we recommend clearly labeling samples as synthetic before wide dissemination.  Our models are often incoherent or inaccurate in subtle ways, which takes more than a quick read for a human to notice.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-work-with-us" class="anchor" aria-hidden="true" href="#work-with-us"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Work with us&lt;/h3&gt;
&lt;p&gt;Please &lt;a href="mailto:languagequestions@openai.com"&gt;let us know&lt;/a&gt; if you’re doing interesting research with or working on applications of GPT-2!  We’re especially interested in hearing from and potentially working with those who are studying&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Potential malicious use cases and defenses against them (e.g. the detectability of synthetic text)&lt;/li&gt;
&lt;li&gt;The extent of problematic content (e.g. bias) being baked into the models and effective mitigations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h2&gt;
&lt;p&gt;See &lt;a href="./DEVELOPERS.md"&gt;DEVELOPERS.md&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;See &lt;a href="./CONTRIBUTORS.md"&gt;CONTRIBUTORS.md&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;Please use the following bibtex entry:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-future-work" class="anchor" aria-hidden="true" href="#future-work"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future work&lt;/h2&gt;
&lt;p&gt;We may release code for evaluating the models on various benchmarks.&lt;/p&gt;
&lt;p&gt;We are still considering release of the larger models.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="./LICENSE"&gt;MIT&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>openai</author><guid isPermaLink="false">https://github.com/openai/gpt-2</guid><pubDate>Mon, 11 Nov 2019 00:04:00 GMT</pubDate></item><item><title>brightmart/albert_zh #5 in Python, This month</title><link>https://github.com/brightmart/albert_zh</link><description>&lt;p&gt;&lt;i&gt;A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS, 海量中文预训练ALBERT模型&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-albert_zh" class="anchor" aria-hidden="true" href="#albert_zh"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;albert_zh&lt;/h1&gt;
&lt;p&gt;An Implementation of &lt;a href="https://arxiv.org/pdf/1909.11942.pdf" rel="nofollow"&gt;A Lite Bert For Self-Supervised Learning Language Representations&lt;/a&gt; with TensorFlow&lt;/p&gt;
&lt;p&gt;ALBert is based on Bert, but with some improvements. It achieves state of the art performance on main benchmarks with 30% parameters less.&lt;/p&gt;
&lt;p&gt;For albert_base_zh it only has ten percentage parameters compare of original bert model, and main accuracy is retained.&lt;/p&gt;
&lt;p&gt;Different version of ALBERT pre-trained model for Chinese, including TensorFlow, PyTorch and Keras, is available now.&lt;/p&gt;
&lt;p&gt;海量中文语料上预训练ALBERT模型：参数更少，效果更好。预训练小模型也能拿下13项NLP任务，ALBERT三大改造登顶GLUE基准&lt;/p&gt;
&lt;p&gt;更多数据集、基线模型、不同任务上模型效果的详细对比，见&lt;a href="https://github.com/chineseGLUE/chineseGLUE"&gt;中文任务基准测评chineseGLUE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/brightmart/albert_zh/blob/master/resources/albert_tiny_compare_s.jpg"&gt;&lt;img src="https://github.com/brightmart/albert_zh/raw/master/resources/albert_tiny_compare_s.jpg" width="90%" height="70%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-模型下载-download-pre-trained-models-of-chinese" class="anchor" aria-hidden="true" href="#模型下载-download-pre-trained-models-of-chinese"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模型下载 Download Pre-trained Models of Chinese&lt;/h2&gt;
&lt;p&gt;1、&lt;a href="https://storage.googleapis.com/albert_zh/albert_tiny.zip" rel="nofollow"&gt;albert_tiny_zh&lt;/a&gt;, &lt;a href="https://storage.googleapis.com/albert_zh/albert_tiny_489k.zip" rel="nofollow"&gt;albert_tiny_zh(训练更久，累积学习20亿个样本)&lt;/a&gt;，文件大小16M、参数为4M&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;训练和推理预测速度提升约10倍，精度基本保留，模型大小为bert的1/25；语义相似度数据集LCQMC测试集上达到85.4%，相比bert_base仅下降1.5个点。

lcqmc训练使用如下参数： --max_seq_length=128 --train_batch_size=64   --learning_rate=1e-4   --num_train_epochs=5 

albert_tiny使用同样的大规模中文语料数据，层数仅为4层、hidden size等向量维度大幅减少; 尝试使用如下学习率来获得更好效果：{2e-5, 6e-5, 1e-4} 

【使用场景】任务相对比较简单一些或实时性要求高的任务，如语义相似度等句子对任务、分类任务；比较难的任务如阅读理解等，可以使用其他大模型。

 例如，可以使用[Tensorflow Lite](https://www.tensorflow.org/lite)在移动端进行部署，本文[随后](#use_tflite)针对这一点进行了介绍，包括如何把模型转换成Tensorflow Lite格式和对其进行性能测试等。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1.1、&lt;a href="https://storage.googleapis.com/albert_zh/albert_tiny_zh_google.zip" rel="nofollow"&gt;albert_tiny_google_zh(累积学习10亿个样本,google版本)&lt;/a&gt;，模型大小16M、性能与albert_tiny_zh一致&lt;/p&gt;
&lt;p&gt;1.2、&lt;a href="https://storage.googleapis.com/albert_zh/albert_small_zh_google.zip" rel="nofollow"&gt;albert_small_google_zh(累积学习10亿个样本,google版本)&lt;/a&gt;，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; 速度比bert_base快4倍；LCQMC测试集上比Bert下降仅0.9个点；去掉adam后模型大小18.5M；使用方法，见 #下游任务 Fine-tuning on Downstream Task     
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2、&lt;a href="https://storage.googleapis.com/albert_zh/albert_large_zh.zip" rel="nofollow"&gt;albert_large_zh&lt;/a&gt;,参数量，层数24，文件大小为64M&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;参数量和模型大小为bert_base的六分之一；在口语化描述相似性数据集LCQMC的测试集上相比bert_base上升0.2个点
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3、&lt;a href="https://storage.googleapis.com/albert_zh/albert_base_zh_additional_36k_steps.zip" rel="nofollow"&gt;albert_base_zh(额外训练了1.5亿个实例即 36k steps * batch_size 4096)&lt;/a&gt;; &lt;a href="https://storage.googleapis.com/albert_zh/albert_base_zh.zip" rel="nofollow"&gt; albert_base_zh(小模型体验版)&lt;/a&gt;, 参数量12M, 层数12，大小为40M&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;参数量为bert_base的十分之一，模型大小也十分之一；在口语化描述相似性数据集LCQMC的测试集上相比bert_base下降约0.6~1个点；
相比未预训练，albert_base提升14个点
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;4、&lt;a href="https://storage.googleapis.com/albert_zh/albert_xlarge_zh_177k.zip" rel="nofollow"&gt;albert_xlarge_zh_177k &lt;/a&gt;;
&lt;a href="https://storage.googleapis.com/albert_zh/albert_xlarge_zh_183k.zip" rel="nofollow"&gt;albert_xlarge_zh_183k(优先尝试)&lt;/a&gt;参数量，层数24，文件大小为230M&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;参数量和模型大小为bert_base的二分之一；需要一张大的显卡；完整测试对比将后续添加；batch_size不能太小，否则可能影响精度
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-updates" class="anchor" aria-hidden="true" href="#updates"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Updates&lt;/h2&gt;
&lt;p&gt;******* 2019-11-03: add google version of albert_small, albert_tiny;&lt;/p&gt;
&lt;p&gt;add method to deploy ablert_tiny to mobile devices with only 0.1 second inference time for sequence length 128, 60M memory *******&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** 2019-10-30: add a simple guide about converting the model to Tensorflow Lite for edge deployment *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** 2019-10-15: albert_tiny_zh, 10 times fast than bert base for training and inference, accuracy remains *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** 2019-10-07: more models of albert *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;add albert_xlarge_zh; albert_base_zh_additional_steps, training with more instances&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** 2019-10-04: PyTorch and Keras versions of albert were supported *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;a.Convert to PyTorch version and do your tasks through &lt;a href="https://github.com/lonePatient/albert_pytorch"&gt;albert_pytorch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;b.Load pre-trained model with keras using one line of codes through &lt;a href="https://github.com/bojone/bert4keras"&gt;bert4keras&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;c.Use albert with TensorFlow 2.0: Use or load pre-trained model with tf2.0 through &lt;a href="https://github.com/kpe/bert-for-tf2"&gt;bert-for-tf2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Releasing albert_xlarge on 6th Oct&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** 2019-10-02: albert_large_zh,albert_base_zh *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Relesed albert_base_zh with only 10% parameters of bert_base, a small model(40M) &amp;amp; training can be very fast.&lt;/p&gt;
&lt;p&gt;Relased albert_large_zh with only 16% parameters of bert_base(64M)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** 2019-09-28: codes and test functions *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Add codes and test functions for three main changes of albert from bert&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-albert模型介绍-introduction-of-albert" class="anchor" aria-hidden="true" href="#albert模型介绍-introduction-of-albert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ALBERT模型介绍 Introduction of ALBERT&lt;/h2&gt;
&lt;p&gt;ALBERT模型是BERT的改进版，与最近其他State of the art的模型不同的是，这次是预训练小模型，效果更好、参数更少。&lt;/p&gt;
&lt;p&gt;它对BERT进行了三个改造 Three main changes of ALBert from Bert：&lt;/p&gt;
&lt;p&gt;1）词嵌入向量参数的因式分解 Factorized embedding parameterization&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; O(V * H) to O(V * E + E * H)
 
 如以ALBert_xxlarge为例，V=30000, H=4096, E=128
   
 那么原先参数为V * H= 30000 * 4096 = 1.23亿个参数，现在则为V * E + E * H = 30000*128+128*4096 = 384万 + 52万 = 436万，
   
 词嵌入相关的参数变化前是变换后的28倍。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2）跨层参数共享 Cross-Layer Parameter Sharing&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; 参数共享能显著减少参数。共享可以分为全连接层、注意力层的参数共享；注意力层的参数对效果的减弱影响小一点。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3）段落连续性任务 Inter-sentence coherence loss.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; 使用段落连续性任务。正例，使用从一个文档中连续的两个文本段落；负例，使用从一个文档中连续的两个文本段落，但位置调换了。
 
 避免使用原有的NSP任务，原有的任务包含隐含了预测主题这类过于简单的任务。

  We maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss 
  based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic 
  prediction and instead focuses on modeling inter-sentence coherence. The SOP loss uses as positive examples the 
  same technique as BERT (two consecutive segments from the same document), and as negative examples the same two 
  consecutive segments but with their order swapped. This forces the model to learn finer-grained distinctions about
  discourse-level coherence properties. 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其他变化，还有 Other changes：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1）去掉了dropout  Remove dropout to enlarge capacity of model.
    最大的模型，训练了1百万步后，还是没有过拟合训练数据。说明模型的容量还可以更大，就移除了dropout
    （dropout可以认为是随机的去掉网络中的一部分，同时使网络变小一些）
    We also note that, even after training for 1M steps, our largest models still do not overfit to their training data. 
    As a result, we decide to remove dropout to further increase our model capacity.
    其他型号的模型，在我们的实现中我们还是会保留原始的dropout的比例，防止模型对训练数据的过拟合。
    
2）为加快训练速度，使用LAMB做为优化器 Use LAMB as optimizer, to train with big batch size
  使用了大的batch_size来训练(4096)。 LAMB优化器使得我们可以训练，特别大的批次batch_size，如高达6万。

3）使用n-gram(uni-gram,bi-gram, tri-gram）来做遮蔽语言模型 Use n-gram as make language model
   即以不同的概率使用n-gram,uni-gram的概率最大，bi-gram其次，tri-gram概率最小。
   本项目中目前使用的是在中文上做whole word mask，稍后会更新一下与n-gram mask的效果对比。n-gram从spanBERT中来。
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-训练语料训练配置-training-data--configuration" class="anchor" aria-hidden="true" href="#训练语料训练配置-training-data--configuration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;训练语料/训练配置 Training Data &amp;amp; Configuration&lt;/h2&gt;
&lt;p&gt;30g中文语料，超过100亿汉字，包括多个百科、新闻、互动社区。&lt;/p&gt;
&lt;p&gt;预训练序列长度sequence_length设置为512，批次batch_size为4096，训练产生了3.5亿个训练数据(instance)；每一个模型默认会训练125k步，albert_xxlarge将训练更久。&lt;/p&gt;
&lt;p&gt;作为比较，roberta_zh预训练产生了2.5亿个训练数据、序列长度为256。由于albert_zh预训练生成的训练数据更多、使用的序列长度更长，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;我们预计albert_zh会有比roberta_zh更好的性能表现，并且能更好处理较长的文本。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;训练使用TPU v3 Pod，我们使用的是v3-256，它包含32个v3-8。每个v3-8机器，含有128G的显存。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-模型性能与对比英文-performance-and-comparision" class="anchor" aria-hidden="true" href="#模型性能与对比英文-performance-and-comparision"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模型性能与对比(英文) Performance and Comparision&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/brightmart/albert_zh/blob/master/resources/state_of_the_art.jpg"&gt;&lt;img src="https://github.com/brightmart/albert_zh/raw/master/resources/state_of_the_art.jpg" width="80%" height="40%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/brightmart/albert_zh/blob/master/resources/albert_performance.jpg"&gt;&lt;img src="https://github.com/brightmart/albert_zh/raw/master/resources/albert_performance.jpg" width="80%" height="40%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/brightmart/albert_zh/blob/master/resources/add_data_removing_dropout.jpg"&gt;&lt;img src="https://github.com/brightmart/albert_zh/raw/master/resources/add_data_removing_dropout.jpg" width="80%" height="40%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-中文任务集上效果对比测试-performance-on-chinese-datasets" class="anchor" aria-hidden="true" href="#中文任务集上效果对比测试-performance-on-chinese-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;中文任务集上效果对比测试 Performance on Chinese datasets&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-问题匹配语任务lcqmcsentence-pair-matching" class="anchor" aria-hidden="true" href="#问题匹配语任务lcqmcsentence-pair-matching"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;问题匹配语任务：LCQMC(Sentence Pair Matching)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;模型&lt;/th&gt;
&lt;th align="center"&gt;开发集(Dev)&lt;/th&gt;
&lt;th align="center"&gt;测试集(Test)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;BERT&lt;/td&gt;
&lt;td align="center"&gt;89.4(88.4)&lt;/td&gt;
&lt;td align="center"&gt;86.9(86.4)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ERNIE&lt;/td&gt;
&lt;td align="center"&gt;89.8 (89.6)&lt;/td&gt;
&lt;td align="center"&gt;87.2 (87.0)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;BERT-wwm&lt;/td&gt;
&lt;td align="center"&gt;89.4 (89.2)&lt;/td&gt;
&lt;td align="center"&gt;87.0 (86.8)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;BERT-wwm-ext&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;RoBERTa-zh-base&lt;/td&gt;
&lt;td align="center"&gt;88.7&lt;/td&gt;
&lt;td align="center"&gt;87.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;RoBERTa-zh-Large&lt;/td&gt;
&lt;td align="center"&gt;&lt;em&gt;&lt;strong&gt;89.9(89.6)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td align="center"&gt;87.2(86.7)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;RoBERTa-zh-Large(20w_steps)&lt;/td&gt;
&lt;td align="center"&gt;89.7&lt;/td&gt;
&lt;td align="center"&gt;87.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ALBERT-zh-tiny&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;85.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ALBERT-zh-small&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;86.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ALBERT-zh-small(Pytorch)&lt;/td&gt;
&lt;td align="center"&gt;--&lt;/td&gt;
&lt;td align="center"&gt;86.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ALBERT-zh-base-additional-36k-steps&lt;/td&gt;
&lt;td align="center"&gt;87.8&lt;/td&gt;
&lt;td align="center"&gt;86.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ALBERT-zh-base&lt;/td&gt;
&lt;td align="center"&gt;87.2&lt;/td&gt;
&lt;td align="center"&gt;86.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ALBERT-large&lt;/td&gt;
&lt;td align="center"&gt;88.7&lt;/td&gt;
&lt;td align="center"&gt;87.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ALBERT-xlarge&lt;/td&gt;
&lt;td align="center"&gt;87.3&lt;/td&gt;
&lt;td align="center"&gt;&lt;em&gt;&lt;strong&gt;87.7&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;注：只跑了一次ALBERT-xlarge，效果还可能提升&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-自然语言推断xnli-of-chinese-version" class="anchor" aria-hidden="true" href="#自然语言推断xnli-of-chinese-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;自然语言推断：XNLI of Chinese Version&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;模型&lt;/th&gt;
&lt;th align="center"&gt;开发集&lt;/th&gt;
&lt;th align="center"&gt;测试集&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;BERT&lt;/td&gt;
&lt;td align="center"&gt;77.8 (77.4)&lt;/td&gt;
&lt;td align="center"&gt;77.8 (77.5)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ERNIE&lt;/td&gt;
&lt;td align="center"&gt;79.7 (79.4)&lt;/td&gt;
&lt;td align="center"&gt;78.6 (78.2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;BERT-wwm&lt;/td&gt;
&lt;td align="center"&gt;79.0 (78.4)&lt;/td&gt;
&lt;td align="center"&gt;78.2 (78.0)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;BERT-wwm-ext&lt;/td&gt;
&lt;td align="center"&gt;79.4 (78.6)&lt;/td&gt;
&lt;td align="center"&gt;78.7 (78.3)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;XLNet&lt;/td&gt;
&lt;td align="center"&gt;79.2&lt;/td&gt;
&lt;td align="center"&gt;78.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;RoBERTa-zh-base&lt;/td&gt;
&lt;td align="center"&gt;79.8&lt;/td&gt;
&lt;td align="center"&gt;78.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;RoBERTa-zh-Large&lt;/td&gt;
&lt;td align="center"&gt;80.2 (80.0)&lt;/td&gt;
&lt;td align="center"&gt;79.9 (79.5)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ALBERT-base&lt;/td&gt;
&lt;td align="center"&gt;77.0&lt;/td&gt;
&lt;td align="center"&gt;77.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ALBERT-large&lt;/td&gt;
&lt;td align="center"&gt;78.0&lt;/td&gt;
&lt;td align="center"&gt;77.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;ALBERT-xlarge&lt;/td&gt;
&lt;td align="center"&gt;?&lt;/td&gt;
&lt;td align="center"&gt;?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;注：BERT-wwm-ext来自于&lt;a href="https://github.com/ymcui/Chinese-BERT-wwm"&gt;这里&lt;/a&gt;；XLNet来自于&lt;a href="https://github.com/ymcui/Chinese-PreTrained-XLNet"&gt;这里&lt;/a&gt;; RoBERTa-zh-base，指12层RoBERTa中文模型&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-阅读理解任务crmc2018" class="anchor" aria-hidden="true" href="#阅读理解任务crmc2018"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;阅读理解任务：CRMC2018&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/brightmart/albert_zh/blob/master/resources/crmc2018_compare_s.jpg"&gt;&lt;img src="https://github.com/brightmart/albert_zh/raw/master/resources/crmc2018_compare_s.jpg" width="90%" height="70%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-语言模型文本段预测准确性训练时间-mask-language-model-accuarcy--training-time" class="anchor" aria-hidden="true" href="#语言模型文本段预测准确性训练时间-mask-language-model-accuarcy--training-time"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;语言模型、文本段预测准确性、训练时间 Mask Language Model Accuarcy &amp;amp; Training Time&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;MLM eval acc&lt;/th&gt;
&lt;th align="center"&gt;SOP eval acc&lt;/th&gt;
&lt;th align="center"&gt;Training(Hours)&lt;/th&gt;
&lt;th align="center"&gt;Loss eval&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;albert_zh_base&lt;/td&gt;
&lt;td align="center"&gt;79.1%&lt;/td&gt;
&lt;td align="center"&gt;99.0%&lt;/td&gt;
&lt;td align="center"&gt;6h&lt;/td&gt;
&lt;td align="center"&gt;1.01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;albert_zh_large&lt;/td&gt;
&lt;td align="center"&gt;80.9%&lt;/td&gt;
&lt;td align="center"&gt;98.6%&lt;/td&gt;
&lt;td align="center"&gt;22.5h&lt;/td&gt;
&lt;td align="center"&gt;0.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;albert_zh_xlarge&lt;/td&gt;
&lt;td align="center"&gt;?&lt;/td&gt;
&lt;td align="center"&gt;?&lt;/td&gt;
&lt;td align="center"&gt;53h(预估)&lt;/td&gt;
&lt;td align="center"&gt;?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;albert_zh_xxlarge&lt;/td&gt;
&lt;td align="center"&gt;?&lt;/td&gt;
&lt;td align="center"&gt;?&lt;/td&gt;
&lt;td align="center"&gt;106h(预估)&lt;/td&gt;
&lt;td align="center"&gt;?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;注：? 将很快替换&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-模型参数和配置-configuration-of-models" class="anchor" aria-hidden="true" href="#模型参数和配置-configuration-of-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;模型参数和配置 Configuration of Models&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/brightmart/albert_zh/blob/master/resources/albert_configuration.jpg"&gt;&lt;img src="https://github.com/brightmart/albert_zh/raw/master/resources/albert_configuration.jpg" width="80%" height="40%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-代码实现和测试-implementation-and-code-testing" class="anchor" aria-hidden="true" href="#代码实现和测试-implementation-and-code-testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;代码实现和测试 Implementation and Code Testing&lt;/h2&gt;
&lt;p&gt;通过运行以下命令测试主要的改进点，包括但不限于词嵌入向量参数的因式分解、跨层参数共享、段落连续性任务等。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python test_changes.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-预训练-pre-training" class="anchor" aria-hidden="true" href="#预训练-pre-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;预训练 Pre-training&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-生成特定格式的文件tfrecords-generate-tfrecords-files" class="anchor" aria-hidden="true" href="#生成特定格式的文件tfrecords-generate-tfrecords-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;生成特定格式的文件(tfrecords) Generate tfrecords Files&lt;/h4&gt;
&lt;p&gt;Run following command 运行以下命令即可。项目自动了一个示例的文本文件(data/news_zh_1.txt)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   bash create_pretrain_data.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果你有很多文本文件，可以通过传入参数的方式，生成多个特定格式的文件(tfrecords）&lt;/p&gt;
&lt;h6&gt;&lt;a id="user-content-support-english-and-other-non-chinese-language" class="anchor" aria-hidden="true" href="#support-english-and-other-non-chinese-language"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support English and Other Non-Chinese Language:&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;If you are doing pre-train for english or other language,which is not chinese, 
you should set hyperparameter of non_chinese to True on create_pretraining_data.py; 
otherwise, by default it is doing chinese pre-train using whole word mask of chinese.
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;a id="user-content-执行预训练-pre-training-on-gputpu-using-the-command" class="anchor" aria-hidden="true" href="#执行预训练-pre-training-on-gputpu-using-the-command"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;执行预训练 pre-training on GPU/TPU using the command&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;GPU(brightmart版, tiny模型):
export BERT_BASE_DIR=./albert_tiny_zh
nohup python3 run_pretraining.py --input_file=./data/tf*.tfrecord  \
--output_dir=./my_new_model_path --do_train=True --do_eval=True --bert_config_file=$BERT_BASE_DIR/albert_config_tiny.json \
--train_batch_size=4096 --max_seq_length=512 --max_predictions_per_seq=51 \
--num_train_steps=125000 --num_warmup_steps=12500 --learning_rate=0.00176    \
--save_checkpoints_steps=2000  --init_checkpoint=$BERT_BASE_DIR/albert_model.ckpt &amp;amp;

GPU(Google版本, small模型):
export BERT_BASE_DIR=./albert_small_zh_google
nohup python3 run_pretraining_google.py --input_file=./data/tf*.tfrecord --eval_batch_size=64 \
--output_dir=./my_new_model_path --do_train=True --do_eval=True --albert_config_file=$BERT_BASE_DIR/albert_config_small_google.json  --export_dir=./my_new_model_path_export \
--train_batch_size=4096 --max_seq_length=512 --max_predictions_per_seq=20 \
--num_train_steps=125000 --num_warmup_steps=12500 --learning_rate=0.00176   \
--save_checkpoints_steps=2000 --init_checkpoint=$BERT_BASE_DIR/albert_model.ckpt

TPU, add something like this:
    --use_tpu=True  --tpu_name=grpc://10.240.1.66:8470 --tpu_zone=us-central1-a
    
注：如果你重头开始训练，可以不指定init_checkpoint；
如果你从现有的模型基础上训练，指定一下BERT_BASE_DIR的路径，并确保bert_config_file和init_checkpoint两个参数的值能对应到相应的文件上；
领域上的预训练，根据数据的大小，可以不用训练特别久。
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-下游任务-fine-tuning-on-downstream-task" class="anchor" aria-hidden="true" href="#下游任务-fine-tuning-on-downstream-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下游任务 Fine-tuning on Downstream Task&lt;/h2&gt;
&lt;h5&gt;&lt;a id="user-content-使用tensorflow" class="anchor" aria-hidden="true" href="#使用tensorflow"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;使用TensorFlow:&lt;/h5&gt;
&lt;p&gt;以使用albert_base做LCQMC任务为例。LCQMC任务是在口语化描述的数据集上做文本的相似性预测。&lt;/p&gt;
&lt;p&gt;We will use LCQMC dataset for fine-tuning, it is oral language corpus, it is used to train and predict semantic similarity of a pair of sentences.&lt;/p&gt;
&lt;p&gt;下载&lt;a href="https://drive.google.com/open?id=1HXYMqsXjmA5uIfu_SFqP7r_vZZG-m_H0" rel="nofollow"&gt;LCQMC&lt;/a&gt;数据集，包含训练、验证和测试集，训练集包含24万口语化描述的中文句子对，标签为1或0。1为句子语义相似，0为语义不相似。&lt;/p&gt;
&lt;p&gt;通过运行下列命令做LCQMC数据集上的fine-tuning:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Clone this project:
      
      git clone https://github.com/brightmart/albert_zh.git
      
2. Fine-tuning by running the following command.
    brightmart版本的tiny模型
    export BERT_BASE_DIR=./albert_tiny_zh
    export TEXT_DIR=./lcqmc
    nohup python3 run_classifier.py   --task_name=lcqmc_pair   --do_train=true   --do_eval=true   --data_dir=$TEXT_DIR   --vocab_file=./albert_config/vocab.txt  \
    --bert_config_file=./albert_config/albert_config_tiny.json --max_seq_length=128 --train_batch_size=64   --learning_rate=1e-4  --num_train_epochs=5 \
    --output_dir=./albert_lcqmc_checkpoints --init_checkpoint=$BERT_BASE_DIR/albert_model.ckpt &amp;amp;
    
    google版本的small模型
    export BERT_BASE_DIR=./albert_small_zh
    export TEXT_DIR=./lcqmc
    nohup python3 run_classifier_sp_google.py --task_name=lcqmc_pair   --do_train=true   --do_eval=true   --data_dir=$TEXT_DIR   --vocab_file=./albert_config/vocab.txt  \
    --albert_config_file=./$BERT_BASE_DIR/albert_config_small_google.json --max_seq_length=128 --train_batch_size=64   --learning_rate=1e-4   --num_train_epochs=5 \
    --output_dir=./albert_lcqmc_checkpoints --init_checkpoint=$BERT_BASE_DIR/albert_model.ckpt &amp;amp;

Notice/注：
    1) you need to download pre-trained chinese albert model, and also download LCQMC dataset 
    你需要下载预训练的模型，并放入到项目当前项目，假设目录名称为albert_tiny_zh; 需要下载LCQMC数据集，并放入到当前项目，
    假设数据集目录名称为lcqmc

    2) for Fine-tuning, you can try to add small percentage of dropout(e.g. 0.1) by changing parameters of 
      attention_probs_dropout_prob &amp;amp; hidden_dropout_prob on albert_config_xxx.json. By default, we set dropout as zero. 
    
    3) you can try different learning rate {2e-5, 6e-5, 1e-4} for better performance 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;&lt;a id="user-content-使用tensorflow-litetflite在移动端进行部署" class="anchor" aria-hidden="true" href="#使用tensorflow-litetflite在移动端进行部署"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a name="user-content-use_tflite"&gt;&lt;/a&gt;使用TensorFlow Lite(TFLite)在移动端进行部署:&lt;/h5&gt;
&lt;p&gt;这里我们主要介绍TFLite模型格式转换和性能测试。转换成TFLite模型后，对于如何在移
动端使用该模型，可以参考TFLite提供的&lt;a href="https://www.tensorflow.org/lite/examples" rel="nofollow"&gt;Android/iOS应用完整开发案例教程页面&lt;/a&gt;。
该页面目前已经包含了&lt;a href="https://github.com/tensorflow/examples/blob/master/lite/examples/text_classification/android"&gt;文本分类&lt;/a&gt;，
&lt;a href="https://github.com/tensorflow/examples/blob/master/lite/examples/bert_qa/android"&gt;文本问答&lt;/a&gt;两个Android案例。&lt;/p&gt;
&lt;p&gt;下面以&lt;a href="https://storage.googleapis.com/albert_zh/albert_tiny.zip" rel="nofollow"&gt;albert_tiny_zh&lt;/a&gt;
为例来介绍TFLite模型格式转换和性能测试：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Freeze graph from the checkpoint&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Ensure to have &amp;gt;=1.14 1.x installed to use the freeze_graph tool as it is removed from 2.x distribution&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install tensorflow==1.15

freeze_graph --input_checkpoint=./albert_model.ckpt \
  --output_graph=/tmp/albert_tiny_zh.pb \
  --output_node_names=cls/predictions/truediv \
  --checkpoint_version=1 --input_meta_graph=./albert_model.ckpt.meta --input_binary=true
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Convert to TFLite format&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We are going to use the new experimental tf-&amp;gt;tflite converter that's distributed with the Tensorflow nightly build.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install tf-nightly

tflite_convert --graph_def_file=/tmp/albert_tiny_zh.pb \
  --input_arrays='input_ids,input_mask,segment_ids,masked_lm_positions,masked_lm_ids,masked_lm_weights' \
  --output_arrays='cls/predictions/truediv' \
  --input_shapes=1,128:1,128:128:1,128:1,128:1,128 \
  --output_file=/tmp/albert_tiny_zh.tflite \
  --enable_v1_converter --experimental_new_converter
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Benchmark the performance of the TFLite model&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;See &lt;a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark"&gt;here&lt;/a&gt;
for details about the performance benchmark tools in TFLite. For example: after
building the benchmark tool binary for an Android phone, do the following to
get an idea of how the TFLite model performs on the phone&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;adb push /tmp/albert_tiny_zh.tflite /data/local/tmp/
adb shell /data/local/tmp/benchmark_model_performance_options --graph=/data/local/tmp/albert_tiny_zh.tflite --perf_options_list=cpu
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On an Android phone w/ Qualcomm's SD845 SoC, via the above benchmark tool, as
of 2019/11/01, the inference latency is ~120ms w/ this converted TFLite model
using 4 threads on CPU, and the memory usage is ~60MB for the model during
inference. Note the performance will improve further with future TFLite
implementation optimizations.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-使用pytorch版本" class="anchor" aria-hidden="true" href="#使用pytorch版本"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;使用PyTorch版本:&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;download pre-trained model, and convert to PyTorch using:
 
  python convert_albert_tf_checkpoint_to_pytorch.py     
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;using &lt;a href="https://github.com/lonePatient/albert_pytorch"&gt;albert_pytorch&lt;/a&gt;&lt;/p&gt;&lt;a href="https://github.com/lonePatient/albert_pytorch"&gt;
&lt;h5&gt;&lt;a id="user-content-使用keras加载" class="anchor" aria-hidden="true" href="#使用keras加载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;使用Keras加载:&lt;/h5&gt;
&lt;/a&gt;&lt;p&gt;&lt;a href="https://github.com/lonePatient/albert_pytorch"&gt;&lt;/a&gt;&lt;a href="https://github.com/bojone/bert4keras"&gt;bert4keras&lt;/a&gt; 适配albert，能成功加载albert_zh的权重，只需要在load_pretrained_model函数里加上albert=True&lt;/p&gt;
&lt;p&gt;load pre-trained model with bert4keras&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-使用tf20加载" class="anchor" aria-hidden="true" href="#使用tf20加载"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;使用tf2.0加载:&lt;/h5&gt;
&lt;p&gt;&lt;a href="https://github.com/kpe/bert-for-tf2"&gt;bert-for-tf2&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-使用案例-基于用户输入预测文本相似性-use-case-text-similarity-based-on-user-input" class="anchor" aria-hidden="true" href="#使用案例-基于用户输入预测文本相似性-use-case-text-similarity-based-on-user-input"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;使用案例-基于用户输入预测文本相似性 Use Case-Text Similarity Based on User Input&lt;/h2&gt;
&lt;p&gt;功能说明：用户可以通过本例了解如何加载训训练集实现基于用户输入的短文本相似度判断。可以基于该代码将程序灵活地拓展为后台服务或增加文本分类等示例。&lt;/p&gt;
&lt;p&gt;涉及代码：similarity.py、args.py&lt;/p&gt;
&lt;p&gt;步骤：&lt;/p&gt;
&lt;p&gt;1、使用本模型进行文本相似性训练，保存模型文件至相应目录下&lt;/p&gt;
&lt;p&gt;2、根据实际情况，修改args.py中的参数，参数说明如下：&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;模型目录，存放ckpt文件&lt;/span&gt;
model_dir &lt;span class="pl-k"&gt;=&lt;/span&gt; os.path.join(file_path, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;albert_lcqmc_checkpoints/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;config文件，存放模型的json文件&lt;/span&gt;
config_name &lt;span class="pl-k"&gt;=&lt;/span&gt; os.path.join(file_path, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;albert_config/albert_config_tiny.json&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;ckpt文件名称&lt;/span&gt;
ckpt_name &lt;span class="pl-k"&gt;=&lt;/span&gt; os.path.join(model_dir, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;model.ckpt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;输出文件目录，训练时的模型输出目录&lt;/span&gt;
output_dir &lt;span class="pl-k"&gt;=&lt;/span&gt; os.path.join(file_path, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;albert_lcqmc_checkpoints/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;vocab文件目录&lt;/span&gt;
vocab_file &lt;span class="pl-k"&gt;=&lt;/span&gt; os.path.join(file_path, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;albert_config/vocab.txt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;数据目录，训练使用的数据集存放目录&lt;/span&gt;
data_dir &lt;span class="pl-k"&gt;=&lt;/span&gt; os.path.join(file_path, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;data/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;本例中的文件结构为：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;|__args.py

|__similarity.py

|__data

|__albert_config

|__albert_lcqmc_checkpoints

|__lcqmc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3、修改用户输入单词&lt;/p&gt;
&lt;p&gt;打开similarity.py，最底部如下代码：&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;__name__&lt;/span&gt; &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;__main__&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:
    sim &lt;span class="pl-k"&gt;=&lt;/span&gt; BertSim()
    sim.start_model()
    sim.predict_sentences([(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;我喜欢妈妈做的汤&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;妈妈做的汤我很喜欢喝&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)])&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;其中sim.start_model()表示加载模型，sim.predict_sentences的输入为一个元组数组，元组中包含两个元素分别为需要判定相似的句子。&lt;/p&gt;
&lt;p&gt;4、运行python文件：similarity.py&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-支持的序列长度与批次大小的关系12g显存-trade-off-between-batch-size-and-sequence-length" class="anchor" aria-hidden="true" href="#支持的序列长度与批次大小的关系12g显存-trade-off-between-batch-size-and-sequence-length"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;支持的序列长度与批次大小的关系,12G显存 Trade off between batch Size and sequence length&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Seq Length&lt;/th&gt;
&lt;th&gt;Max Batch Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;albert-base&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;albert-large&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;albert-xlarge&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-学习曲线-training-loss-of-xlarge-of-albert_zh" class="anchor" aria-hidden="true" href="#学习曲线-training-loss-of-xlarge-of-albert_zh"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;学习曲线 Training Loss of xlarge of albert_zh&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/brightmart/albert_zh/blob/master/resources/xlarge_loss.jpg"&gt;&lt;img src="https://github.com/brightmart/albert_zh/raw/master/resources/xlarge_loss.jpg" width="80%" height="40%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-所有的参数-parameters-of-albert_xlarge" class="anchor" aria-hidden="true" href="#所有的参数-parameters-of-albert_xlarge"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;所有的参数 Parameters of albert_xlarge&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/brightmart/albert_zh/blob/master/resources/albert_large_zh_parameters.jpg"&gt;&lt;img src="https://github.com/brightmart/albert_zh/raw/master/resources/albert_large_zh_parameters.jpg" width="80%" height="40%" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-技术交流与问题讨论qq群-836811304-join-us-on-qq-group" class="anchor" aria-hidden="true" href="#技术交流与问题讨论qq群-836811304-join-us-on-qq-group"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;技术交流与问题讨论QQ群: 836811304 Join us on QQ group&lt;/h4&gt;
&lt;p&gt;If you have any question, you can raise an issue, or send me an email: &lt;a href="mailto:brightmart@hotmail.com"&gt;brightmart@hotmail.com&lt;/a&gt;;&lt;/p&gt;
&lt;p&gt;Currently how to use PyTorch version of albert is not clear yet, if you know how to do that, just email us or open an issue.&lt;/p&gt;
&lt;p&gt;You can also send pull request to report you performance on your task or add methods on how to load models for PyTorch and so on.&lt;/p&gt;
&lt;p&gt;If you have ideas for generate best performance pre-training Chinese model, please also let me know.&lt;/p&gt;
&lt;h5&gt;&lt;a id="user-content-research-supported-with-cloud-tpus-from-googles-tensorflow-research-cloud-tfrc" class="anchor" aria-hidden="true" href="#research-supported-with-cloud-tpus-from-googles-tensorflow-research-cloud-tfrc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Research supported with Cloud TPUs from Google's TensorFlow Research Cloud (TFRC)&lt;/h5&gt;
&lt;h2&gt;&lt;a id="user-content-cite-us" class="anchor" aria-hidden="true" href="#cite-us"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cite Us&lt;/h2&gt;
&lt;p&gt;Bright Liang Xu, albert_zh, (2019), GitHub repository, &lt;a href="https://github.com/brightmart/albert_zh"&gt;https://github.com/brightmart/albert_zh&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-reference" class="anchor" aria-hidden="true" href="#reference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Reference&lt;/h2&gt;
&lt;p&gt;1、&lt;a href="https://arxiv.org/pdf/1909.11942.pdf" rel="nofollow"&gt;ALBERT: A Lite BERT For Self-Supervised Learning Of Language Representations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2、&lt;a href="https://arxiv.org/pdf/1810.04805.pdf" rel="nofollow"&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3、&lt;a href="https://arxiv.org/abs/1907.10529" rel="nofollow"&gt;SpanBERT: Improving Pre-training by Representing and Predicting Spans&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;4、&lt;a href="https://arxiv.org/pdf/1907.11692.pdf" rel="nofollow"&gt;RoBERTa: A Robustly Optimized BERT Pretraining Approach&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;5、&lt;a href="https://arxiv.org/pdf/1904.00962.pdf" rel="nofollow"&gt;Large Batch Optimization for Deep Learning: Training BERT in 76 minutes(LAMB)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;6、&lt;a href="https://github.com/ymcui/LAMB_Optimizer_TF"&gt;LAMB Optimizer,TensorFlow version&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;7、&lt;a href="http://baijiahao.baidu.com/s?id=1645712785366950083&amp;amp;wfr=spider&amp;amp;for=pc" rel="nofollow"&gt;预训练小模型也能拿下13项NLP任务，ALBERT三大改造登顶GLUE基准&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;8、 &lt;a href="https://github.com/lonePatient/albert_pytorch"&gt;albert_pytorch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;9、&lt;a href="https://github.com/bojone/bert4keras"&gt;load albert with keras&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;10、&lt;a href="https://github.com/kpe/bert-for-tf2"&gt;load albert with tf2.0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;11、&lt;a href="https://github.com/google-research/google-research/tree/master/albert"&gt;repo of albert from google&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;12、&lt;a href="https://github.com/chineseGLUE/chineseGLUE"&gt;chineseGLUE-中文任务基准测评：公开可用多个任务、基线模型、广泛测评与效果对比&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>brightmart</author><guid isPermaLink="false">https://github.com/brightmart/albert_zh</guid><pubDate>Mon, 11 Nov 2019 00:05:00 GMT</pubDate></item><item><title>swisskyrepo/PayloadsAllTheThings #6 in Python, This month</title><link>https://github.com/swisskyrepo/PayloadsAllTheThings</link><description>&lt;p&gt;&lt;i&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-payloads-all-the-things" class="anchor" aria-hidden="true" href="#payloads-all-the-things"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Payloads All The Things&lt;/h1&gt;
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques !
I &lt;g-emoji class="g-emoji" alias="heart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2764.png"&gt;❤️&lt;/g-emoji&gt; pull requests :)&lt;/p&gt;
&lt;p&gt;You can also contribute with a &lt;g-emoji class="g-emoji" alias="beers" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f37b.png"&gt;🍻&lt;/g-emoji&gt; IRL&lt;/p&gt;
&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;README.md - vulnerability description and how to exploit it&lt;/li&gt;
&lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt;
&lt;li&gt;Images - pictures for the README.md&lt;/li&gt;
&lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You might also like the &lt;code&gt;Methodology and Resources&lt;/code&gt; folder :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/"&gt;Methodology and Resources&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Active%20Directory%20Attack.md"&gt;Active Directory Attack.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Persistence.md"&gt;Linux - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Privilege%20Escalation.md"&gt;Linux - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Metasploit%20-%20Cheatsheet.md"&gt;Metasploit - Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Methodology%20and%20enumeration.md"&gt;Methodology and enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Pivoting%20Techniques.md"&gt;Network Pivoting Techniques.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Discovery.md"&gt;Network Discovery.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Reverse%20Shell%20Cheatsheet.md"&gt;Reverse Shell Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Subdomains%20Enumeration.md"&gt;Subdomains Enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Download%20and%20Execute.md"&gt;Windows - Download and Execute.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Mimikatz.md"&gt;Windows - Mimikatz.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Persistence.md"&gt;Windows - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Post%20Exploitation%20Koadic.md"&gt;Windows - Post Exploitation Koadic.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Privilege%20Escalation.md"&gt;Windows - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Using%20credentials.md"&gt;Windows - Using credentials.md&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CVE%20Exploits"&gt;CVE Exploits&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apache Struts 2 CVE-2013-2251 CVE-2017-5638 CVE-2018-11776_.py&lt;/li&gt;
&lt;li&gt;Apache Struts 2 CVE-2017-9805.py&lt;/li&gt;
&lt;li&gt;Apache Struts 2 CVE-2018-11776.py&lt;/li&gt;
&lt;li&gt;Docker API RCE.py&lt;/li&gt;
&lt;li&gt;Drupalgeddon2 CVE-2018-7600.rb&lt;/li&gt;
&lt;li&gt;Heartbleed CVE-2014-0160.py&lt;/li&gt;
&lt;li&gt;JBoss CVE-2015-7501.py&lt;/li&gt;
&lt;li&gt;Jenkins CVE-2015-8103.py&lt;/li&gt;
&lt;li&gt;Jenkins CVE-2016-0792.py&lt;/li&gt;
&lt;li&gt;Rails CVE-2019-5420.rb&lt;/li&gt;
&lt;li&gt;Shellshock CVE-2014-6271.py&lt;/li&gt;
&lt;li&gt;Tomcat CVE-2017-12617.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2016-3510.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2017-10271.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2018-2894.py&lt;/li&gt;
&lt;li&gt;WebSphere CVE-2015-7450.py&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You want more ? Check the &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/BOOKS.md"&gt;Books&lt;/a&gt; and &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/YOUTUBE.md"&gt;Youtube videos&lt;/a&gt; selections.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>swisskyrepo</author><guid isPermaLink="false">https://github.com/swisskyrepo/PayloadsAllTheThings</guid><pubDate>Mon, 11 Nov 2019 00:06:00 GMT</pubDate></item><item><title>google-research/bert #7 in Python, This month</title><link>https://github.com/google-research/bert</link><description>&lt;p&gt;&lt;i&gt;TensorFlow code and pre-trained models for BERT&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-bert" class="anchor" aria-hidden="true" href="#bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BERT&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;***** New May 31st, 2019: Whole Word Masking Models *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a release of several new models which were the result of an improvement
the pre-processing code.&lt;/p&gt;
&lt;p&gt;In the original pre-processing code, we randomly select WordPiece tokens to
mask. For example:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Input Text: the man jumped up , put his basket on phil ##am ##mon ' s head&lt;/code&gt;
&lt;code&gt;Original Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The new technique is called Whole Word Masking. In this case, we always mask
&lt;em&gt;all&lt;/em&gt; of the the tokens corresponding to a word at once. The overall masking
rate remains the same.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The training is identical -- we still predict each masked WordPiece token
independently. The improvement comes from the fact that the original prediction
task was too 'easy' for words that had been split into multiple WordPieces.&lt;/p&gt;
&lt;p&gt;This can be enabled during data generation by passing the flag
&lt;code&gt;--do_whole_word_mask=True&lt;/code&gt; to &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Pre-trained models with Whole Word Masking are linked below. The data and
training were otherwise identical, and the models have identical structure and
vocab to the original models. We only include BERT-Large models. When using
these models, please make it clear in the paper that you are using the Whole
Word Masking variant of BERT-Large.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;SQUAD 1.1 F1/EM&lt;/th&gt;
&lt;th align="center"&gt;Multi NLI Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.0/84.3&lt;/td&gt;
&lt;td align="center"&gt;86.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.8/86.7&lt;/td&gt;
&lt;td align="center"&gt;87.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.5/84.8&lt;/td&gt;
&lt;td align="center"&gt;86.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.9/86.7&lt;/td&gt;
&lt;td align="center"&gt;86.46&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;***** New February 7th, 2019: TfHub Module *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;BERT has been uploaded to &lt;a href="https://tfhub.dev" rel="nofollow"&gt;TensorFlow Hub&lt;/a&gt;. See
&lt;code&gt;run_classifier_with_tfhub.py&lt;/code&gt; for an example of how to use the TF Hub module,
or run an example in the browser on
&lt;a href="https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb" rel="nofollow"&gt;Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 23rd, 2018: Un-normalized multilingual model + Thai +
Mongolian *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We uploaded a new multilingual model which does &lt;em&gt;not&lt;/em&gt; perform any normalization
on the input (no lower casing, accent stripping, or Unicode normalization), and
additionally inclues Thai and Mongolian.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It is recommended to use this version for developing multilingual models,
especially on languages with non-Latin alphabets.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This does not require any code changes, and can be downloaded here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;***** New November 15th, 2018: SOTA SQuAD 2.0 System *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which is
currently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of the
README for details.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 5th, 2018: Third-party PyTorch and Chainer versions of
BERT available *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NLP researchers from HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. Sosuke Kobayashi also made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
(Thanks!) We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 3rd, 2018: Multilingual and Chinese models available
*****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have made two new BERT models available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use character-based tokenization for Chinese, and WordPiece tokenization for
all other languages. Both models should work out-of-the-box without any code
changes. We did update the implementation of &lt;code&gt;BasicTokenizer&lt;/code&gt; in
&lt;code&gt;tokenization.py&lt;/code&gt; to support Chinese character tokenization, so please update if
you forked it. However, we did not change the tokenization API.&lt;/p&gt;
&lt;p&gt;For more, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** End new information *****&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BERT&lt;/strong&gt;, or &lt;strong&gt;B&lt;/strong&gt;idirectional &lt;strong&gt;E&lt;/strong&gt;ncoder &lt;strong&gt;R&lt;/strong&gt;epresentations from
&lt;strong&gt;T&lt;/strong&gt;ransformers, is a new method of pre-training language representations which
obtains state-of-the-art results on a wide array of Natural Language Processing
(NLP) tasks.&lt;/p&gt;
&lt;p&gt;Our academic paper which describes BERT in detail and provides full results on a
number of tasks can be found here:
&lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To give a few numbers, here are the results on the
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD v1.1&lt;/a&gt; question answering
task:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SQuAD v1.1 Leaderboard (Oct 8th 2018)&lt;/th&gt;
&lt;th align="center"&gt;Test EM&lt;/th&gt;
&lt;th align="center"&gt;Test F1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Ensemble - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;87.4&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;93.2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Ensemble - nlnet&lt;/td&gt;
&lt;td align="center"&gt;86.0&lt;/td&gt;
&lt;td align="center"&gt;91.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Single Model - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;85.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.8&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Single Model - nlnet&lt;/td&gt;
&lt;td align="center"&gt;83.5&lt;/td&gt;
&lt;td align="center"&gt;90.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And several natural language inference tasks:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th align="center"&gt;MultiNLI&lt;/th&gt;
&lt;th align="center"&gt;Question NLI&lt;/th&gt;
&lt;th align="center"&gt;SWAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.7&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.3&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenAI GPT (Prev. SOTA)&lt;/td&gt;
&lt;td align="center"&gt;82.2&lt;/td&gt;
&lt;td align="center"&gt;88.1&lt;/td&gt;
&lt;td align="center"&gt;75.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Plus many other tasks.&lt;/p&gt;
&lt;p&gt;Moreover, these results were all obtained with almost no task-specific neural
network architecture design.&lt;/p&gt;
&lt;p&gt;If you already know what BERT is and you just want to get started, you can
&lt;a href="#pre-trained-models"&gt;download the pre-trained models&lt;/a&gt; and
&lt;a href="#fine-tuning-with-bert"&gt;run a state-of-the-art fine-tuning&lt;/a&gt; in only a few
minutes.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-bert" class="anchor" aria-hidden="true" href="#what-is-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is BERT?&lt;/h2&gt;
&lt;p&gt;BERT is a method of pre-training language representations, meaning that we train
a general-purpose "language understanding" model on a large text corpus (like
Wikipedia), and then use that model for downstream NLP tasks that we care about
(like question answering). BERT outperforms previous methods because it is the
first &lt;em&gt;unsupervised&lt;/em&gt;, &lt;em&gt;deeply bidirectional&lt;/em&gt; system for pre-training NLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unsupervised&lt;/em&gt; means that BERT was trained using only a plain text corpus, which
is important because an enormous amount of plain text data is publicly available
on the web in many languages.&lt;/p&gt;
&lt;p&gt;Pre-trained representations can also either be &lt;em&gt;context-free&lt;/em&gt; or &lt;em&gt;contextual&lt;/em&gt;,
and contextual representations can further be &lt;em&gt;unidirectional&lt;/em&gt; or
&lt;em&gt;bidirectional&lt;/em&gt;. Context-free models such as
&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec" rel="nofollow"&gt;word2vec&lt;/a&gt; or
&lt;a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow"&gt;GloVe&lt;/a&gt; generate a single "word
embedding" representation for each word in the vocabulary, so &lt;code&gt;bank&lt;/code&gt; would have
the same representation in &lt;code&gt;bank deposit&lt;/code&gt; and &lt;code&gt;river bank&lt;/code&gt;. Contextual models
instead generate a representation of each word that is based on the other words
in the sentence.&lt;/p&gt;
&lt;p&gt;BERT was built upon recent work in pre-training contextual representations —
including &lt;a href="https://arxiv.org/abs/1511.01432" rel="nofollow"&gt;Semi-supervised Sequence Learning&lt;/a&gt;,
&lt;a href="https://blog.openai.com/language-unsupervised/" rel="nofollow"&gt;Generative Pre-Training&lt;/a&gt;,
&lt;a href="https://allennlp.org/elmo" rel="nofollow"&gt;ELMo&lt;/a&gt;, and
&lt;a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html" rel="nofollow"&gt;ULMFit&lt;/a&gt;
— but crucially these models are all &lt;em&gt;unidirectional&lt;/em&gt; or &lt;em&gt;shallowly
bidirectional&lt;/em&gt;. This means that each word is only contextualized using the words
to its left (or right). For example, in the sentence &lt;code&gt;I made a bank deposit&lt;/code&gt; the
unidirectional representation of &lt;code&gt;bank&lt;/code&gt; is only based on &lt;code&gt;I made a&lt;/code&gt; but not
&lt;code&gt;deposit&lt;/code&gt;. Some previous work does combine the representations from separate
left-context and right-context models, but only in a "shallow" manner. BERT
represents "bank" using both its left and right context — &lt;code&gt;I made a ... deposit&lt;/code&gt;
— starting from the very bottom of a deep neural network, so it is &lt;em&gt;deeply
bidirectional&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;BERT uses a simple approach for this: We mask out 15% of the words in the input,
run the entire sequence through a deep bidirectional
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; encoder, and then predict only
the masked words. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: the man went to the [MASK1] . he bought a [MASK2] of milk.
Labels: [MASK1] = store; [MASK2] = gallon
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to learn relationships between sentences, we also train on a simple
task which can be generated from any monolingual corpus: Given two sentences &lt;code&gt;A&lt;/code&gt;
and &lt;code&gt;B&lt;/code&gt;, is &lt;code&gt;B&lt;/code&gt; the actual next sentence that comes after &lt;code&gt;A&lt;/code&gt;, or just a random
sentence from the corpus?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: he bought a gallon of milk .
Label: IsNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: penguins are flightless .
Label: NotNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then train a large model (12-layer to 24-layer Transformer) on a large corpus
(Wikipedia + &lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt;) for a long time (1M
update steps), and that's BERT.&lt;/p&gt;
&lt;p&gt;Using BERT has two stages: &lt;em&gt;Pre-training&lt;/em&gt; and &lt;em&gt;fine-tuning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pre-training&lt;/strong&gt; is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a
one-time procedure for each language (current models are English-only, but
multilingual models will be released in the near future). We are releasing a
number of pre-trained models from the paper which were pre-trained at Google.
Most NLP researchers will never need to pre-train their own model from scratch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt; is inexpensive. All of the results in the paper can be
replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU,
starting from the exact same pre-trained model. SQuAD, for example, can be
trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of
91.0%, which is the single system state-of-the-art.&lt;/p&gt;
&lt;p&gt;The other important aspect of BERT is that it can be adapted to many types of
NLP tasks very easily. In the paper, we demonstrate state-of-the-art results on
sentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level
(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific
modifications.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-has-been-released-in-this-repository" class="anchor" aria-hidden="true" href="#what-has-been-released-in-this-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What has been released in this repository?&lt;/h2&gt;
&lt;p&gt;We are releasing the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow code for the BERT model architecture (which is mostly a standard
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; architecture).&lt;/li&gt;
&lt;li&gt;Pre-trained checkpoints for both the lowercase and cased version of
&lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; from the paper.&lt;/li&gt;
&lt;li&gt;TensorFlow code for push-button replication of the most important
fine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of the code in this repository works out-of-the-box with CPU, GPU, and Cloud
TPU.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-trained-models" class="anchor" aria-hidden="true" href="#pre-trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-trained models&lt;/h2&gt;
&lt;p&gt;We are releasing the &lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; models from the paper.
&lt;code&gt;Uncased&lt;/code&gt; means that the text has been lowercased before WordPiece tokenization,
e.g., &lt;code&gt;John Smith&lt;/code&gt; becomes &lt;code&gt;john smith&lt;/code&gt;. The &lt;code&gt;Uncased&lt;/code&gt; model also strips out any
accent markers. &lt;code&gt;Cased&lt;/code&gt; means that the true case and accent markers are
preserved. Typically, the &lt;code&gt;Uncased&lt;/code&gt; model is better unless you know that case
information is important for your task (e.g., Named Entity Recognition or
Part-of-Speech tagging).&lt;/p&gt;
&lt;p&gt;These models are all released under the same license as the source code (Apache
2.0).&lt;/p&gt;
&lt;p&gt;For information about the Multilingual and Chinese model, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When using a cased model, make sure to pass &lt;code&gt;--do_lower=False&lt;/code&gt; to the training
scripts. (Or pass &lt;code&gt;do_lower_case=False&lt;/code&gt; directly to &lt;code&gt;FullTokenizer&lt;/code&gt; if you're
using your own script.)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The links to the models are here (right-click, 'Save link as...' on the name):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads , 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased (New, recommended)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Uncased (Orig, not recommended)&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each .zip file contains three items:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A TensorFlow checkpoint (&lt;code&gt;bert_model.ckpt&lt;/code&gt;) containing the pre-trained
weights (which is actually 3 files).&lt;/li&gt;
&lt;li&gt;A vocab file (&lt;code&gt;vocab.txt&lt;/code&gt;) to map WordPiece to word id.&lt;/li&gt;
&lt;li&gt;A config file (&lt;code&gt;bert_config.json&lt;/code&gt;) which specifies the hyperparameters of
the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-fine-tuning-with-bert" class="anchor" aria-hidden="true" href="#fine-tuning-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with BERT&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: All results on the paper were fine-tuned on a single Cloud TPU,
which has 64GB of RAM. It is currently not possible to re-produce most of the
&lt;code&gt;BERT-Large&lt;/code&gt; results on the paper using a GPU with 12GB - 16GB of RAM, because
the maximum batch size that can fit in memory is too small. We are working on
adding code to this repository which allows for much larger effective batch size
on the GPU. See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for
more details.&lt;/p&gt;
&lt;p&gt;This code was tested with TensorFlow 1.11.0. It was tested with Python2 and
Python3 (but more thoroughly with Python2, since this is what's used internally
in Google).&lt;/p&gt;
&lt;p&gt;The fine-tuning examples which use &lt;code&gt;BERT-Base&lt;/code&gt; should be able to run on a GPU
that has at least 12GB of RAM using the hyperparameters given.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-fine-tuning-with-cloud-tpus" class="anchor" aria-hidden="true" href="#fine-tuning-with-cloud-tpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with Cloud TPUs&lt;/h3&gt;
&lt;p&gt;Most of the examples below assumes that you will be running training/evaluation
on your local machine, using a GPU like a Titan X or GTX 1080.&lt;/p&gt;
&lt;p&gt;However, if you have access to a Cloud TPU that you want to train on, just add
the following flags to &lt;code&gt;run_classifier.py&lt;/code&gt; or &lt;code&gt;run_squad.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --use_tpu=True \
  --tpu_name=$TPU_NAME
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please see the
&lt;a href="https://cloud.google.com/tpu/docs/tutorials/mnist" rel="nofollow"&gt;Google Cloud TPU tutorial&lt;/a&gt;
for how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;On Cloud TPUs, the pretrained model and the output directory will need to be on
Google Cloud Storage. For example, if you have a bucket named &lt;code&gt;some_bucket&lt;/code&gt;, you
might use the following flags instead:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --output_dir=gs://some_bucket/my_output_dir/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The unzipped pre-trained model files can also be found in the Google Cloud
Storage folder &lt;code&gt;gs://bert_models/2018_10_18&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export BERT_BASE_DIR=gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-sentence-and-sentence-pair-classification-tasks" class="anchor" aria-hidden="true" href="#sentence-and-sentence-pair-classification-tasks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sentence (and sentence-pair) classification tasks&lt;/h3&gt;
&lt;p&gt;Before running this example you must download the
&lt;a href="https://gluebenchmark.com/tasks" rel="nofollow"&gt;GLUE data&lt;/a&gt; by running
&lt;a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"&gt;this script&lt;/a&gt;
and unpack it to some directory &lt;code&gt;$GLUE_DIR&lt;/code&gt;. Next, download the &lt;code&gt;BERT-Base&lt;/code&gt;
checkpoint and unzip it to some directory &lt;code&gt;$BERT_BASE_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This example code fine-tunes &lt;code&gt;BERT-Base&lt;/code&gt; on the Microsoft Research Paraphrase
Corpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in a
few minutes on most GPUs.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue

python run_classifier.py \
  --task_name=MRPC \
  --do_train=true \
  --do_eval=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  eval_accuracy = 0.845588
  eval_loss = 0.505248
  global_step = 343
  loss = 0.505248
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that the Dev set accuracy was 84.55%. Small sets like MRPC have a
high variance in the Dev set accuracy, even when starting from the same
pre-training checkpoint. If you re-run multiple times (making sure to point to
different &lt;code&gt;output_dir&lt;/code&gt;), you should see results between 84% and 88%.&lt;/p&gt;
&lt;p&gt;A few other pre-trained models are implemented off-the-shelf in
&lt;code&gt;run_classifier.py&lt;/code&gt;, so it should be straightforward to follow those examples to
use BERT for any single-sentence or sentence-pair classification task.&lt;/p&gt;
&lt;p&gt;Note: You might see a message &lt;code&gt;Running train on CPU&lt;/code&gt;. This really just means
that it's running on something other than a Cloud TPU, which includes a GPU.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-prediction-from-classifier" class="anchor" aria-hidden="true" href="#prediction-from-classifier"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prediction from classifier&lt;/h4&gt;
&lt;p&gt;Once you have trained your classifier you can use it in inference mode by using
the --do_predict=true command. You need to have a file named test.tsv in the
input folder. Output will be created in file called test_results.tsv in the
output folder. Each line will contain output for each sample, columns are the
class probabilities.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue
&lt;span class="pl-k"&gt;export&lt;/span&gt; TRAINED_CLASSIFIER=/path/to/fine/tuned/classifier

python run_classifier.py \
  --task_name=MRPC \
  --do_predict=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$TRAINED_CLASSIFIER&lt;/span&gt; \
  --max_seq_length=128 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-squad-11" class="anchor" aria-hidden="true" href="#squad-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 1.1&lt;/h3&gt;
&lt;p&gt;The Stanford Question Answering Dataset (SQuAD) is a popular question answering
benchmark dataset. BERT (at the time of the release) obtains state-of-the-art
results on SQuAD with almost no task-specific network architecture modifications
or data augmentation. However, it does require semi-complex data pre-processing
and post-processing to deal with (a) the variable-length nature of SQuAD context
paragraphs, and (b) the character-level answer annotations which are used for
SQuAD training. This processing is implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD, you will first need to download the dataset. The
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD website&lt;/a&gt; does not seem to
link to the v1.1 datasets any longer, but the necessary files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json" rel="nofollow"&gt;train-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json" rel="nofollow"&gt;dev-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py"&gt;evaluate-v1.1.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The state-of-the-art SQuAD results from the paper currently cannot be reproduced
on a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 does
not seem to fit on a 12GB GPU using &lt;code&gt;BERT-Large&lt;/code&gt;). However, a reasonably strong
&lt;code&gt;BERT-Base&lt;/code&gt; model can be trained on the GPU with these hyperparameters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=12 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=/tmp/squad_base/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The dev set predictions will be saved into a file called &lt;code&gt;predictions.json&lt;/code&gt; in
the &lt;code&gt;output_dir&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/evaluate-v1.1.py &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json ./squad/predictions.json&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Which should produce an output like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 88.41249612335034, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 81.2488174077578}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see a result similar to the 88.5% reported in the paper for
&lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you have access to a Cloud TPU, you can train with &lt;code&gt;BERT-Large&lt;/code&gt;. Here is a
set of hyperparameters (slightly different than the paper) which consistently
obtain around 90.5%-91.0% F1 single-system trained only on SQuAD:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For example, one random run with these parameters produces the following Dev
scores:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 90.87081895814865, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 84.38978240302744}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you fine-tune for one epoch on
&lt;a href="http://nlp.cs.washington.edu/triviaqa/" rel="nofollow"&gt;TriviaQA&lt;/a&gt; before this the results will
be even better, but you will need to convert TriviaQA into the SQuAD json
format.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-squad-20" class="anchor" aria-hidden="true" href="#squad-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 2.0&lt;/h3&gt;
&lt;p&gt;This model is also implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD 2.0, you will first need to download the dataset. The necessary
files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json" rel="nofollow"&gt;train-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json" rel="nofollow"&gt;dev-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/" rel="nofollow"&gt;evaluate-v2.0.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On Cloud TPU you can run with BERT-Large as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We assume you have copied everything from the output directory to a local
directory called ./squad/. The initial dev set predictions will be at
./squad/predictions.json and the differences between the score of no answer ("")
and the best non-null answer for each question will be in the file
./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Run this script to tune a threshold for predicting null versus non-null answers:&lt;/p&gt;
&lt;p&gt;python $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json
./squad/predictions.json --na-prob-file ./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Assume the script outputs "best_f1_thresh" THRESH. (Typical values are between
-1.0 and -5.0). You can now re-run the model to generate predictions with the
derived threshold or alternatively you can extract the appropriate answers from
./squad/nbest_predictions.json.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=False \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True \
  --null_score_diff_threshold=&lt;span class="pl-smi"&gt;$THRESH&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-out-of-memory-issues" class="anchor" aria-hidden="true" href="#out-of-memory-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Out-of-memory issues&lt;/h3&gt;
&lt;p&gt;All experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB of
device RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely
to encounter out-of-memory issues if you use the same hyperparameters described
in the paper.&lt;/p&gt;
&lt;p&gt;The factors that affect memory usage are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;max_seq_length&lt;/code&gt;&lt;/strong&gt;: The released models were trained with sequence lengths
up to 512, but you can fine-tune with a shorter max sequence length to save
substantial memory. This is controlled by the &lt;code&gt;max_seq_length&lt;/code&gt; flag in our
example code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;train_batch_size&lt;/code&gt;&lt;/strong&gt;: The memory usage is also directly proportional to
the batch size.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model type, &lt;code&gt;BERT-Base&lt;/code&gt; vs. &lt;code&gt;BERT-Large&lt;/code&gt;&lt;/strong&gt;: The &lt;code&gt;BERT-Large&lt;/code&gt; model
requires significantly more memory than &lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimizer&lt;/strong&gt;: The default optimizer for BERT is Adam, which requires a lot
of extra memory to store the &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;v&lt;/code&gt; vectors. Switching to a more memory
efficient optimizer can reduce memory usage, but can also affect the
results. We have not experimented with other optimizers for fine-tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the default training scripts (&lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;run_squad.py&lt;/code&gt;), we
benchmarked the maximum batch size on single Titan X GPU (12GB RAM) with
TensorFlow 1.11.0:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Seq Length&lt;/th&gt;
&lt;th&gt;Max Batch Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Base&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Large&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unfortunately, these max batch sizes for &lt;code&gt;BERT-Large&lt;/code&gt; are so small that they
will actually harm the model accuracy, regardless of the learning rate used. We
are working on adding code to this repository which will allow much larger
effective batch sizes to be used on the GPU. The code will be based on one (or
both) of the following techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gradient accumulation&lt;/strong&gt;: The samples in a minibatch are typically
independent with respect to gradient computation (excluding batch
normalization, which is not used here). This means that the gradients of
multiple smaller minibatches can be accumulated before performing the weight
update, and this will be exactly equivalent to a single larger update.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/openai/gradient-checkpointing"&gt;&lt;strong&gt;Gradient checkpointing&lt;/strong&gt;&lt;/a&gt;:
The major use of GPU/TPU memory during DNN training is caching the
intermediate activations in the forward pass that are necessary for
efficient computation in the backward pass. "Gradient checkpointing" trades
memory for compute time by re-computing the activations in an intelligent
way.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;However, this is not implemented in the current release.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-to-extract-fixed-feature-vectors-like-elmo" class="anchor" aria-hidden="true" href="#using-bert-to-extract-fixed-feature-vectors-like-elmo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT to extract fixed feature vectors (like ELMo)&lt;/h2&gt;
&lt;p&gt;In certain cases, rather than fine-tuning the entire pre-trained model
end-to-end, it can be beneficial to obtained &lt;em&gt;pre-trained contextual
embeddings&lt;/em&gt;, which are fixed contextual representations of each input token
generated from the hidden layers of the pre-trained model. This should also
mitigate most of the out-of-memory issues.&lt;/p&gt;
&lt;p&gt;As an example, we include the script &lt;code&gt;extract_features.py&lt;/code&gt; which can be used
like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Sentence A and Sentence B are separated by the ||| delimiter for sentence&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pair tasks like question answering and entailment.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; For single sentence inputs, put one sentence per line and DON'T use the&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; delimiter.&lt;/span&gt;
&lt;span class="pl-c1"&gt;echo&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Who was Jim Henson ? ||| Jim Henson was a puppeteer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; /tmp/input.txt

python extract_features.py \
  --input_file=/tmp/input.txt \
  --output_file=/tmp/output.jsonl \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --layers=-1,-2,-3,-4 \
  --max_seq_length=128 \
  --batch_size=8&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will create a JSON file (one line per line of input) containing the BERT
activations from each Transformer layer specified by &lt;code&gt;layers&lt;/code&gt; (-1 is the final
hidden layer of the Transformer, etc.)&lt;/p&gt;
&lt;p&gt;Note that this script will produce very large output files (by default, around
15kb for every input token).&lt;/p&gt;
&lt;p&gt;If you need to maintain alignment between the original and tokenized words (for
projecting training labels), see the &lt;a href="#tokenization"&gt;Tokenization&lt;/a&gt; section
below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You may see a message like &lt;code&gt;Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict.&lt;/code&gt; This message is expected, it
just means that we are using the &lt;code&gt;init_from_checkpoint()&lt;/code&gt; API rather than the
saved model API. If you don't specify a checkpoint or specify an invalid
checkpoint, this script will complain.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tokenization" class="anchor" aria-hidden="true" href="#tokenization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;For sentence-level tasks (or sentence-pair) tasks, tokenization is very simple.
Just follow the example code in &lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;extract_features.py&lt;/code&gt;.
The basic procedure for sentence-level tasks is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Instantiate an instance of &lt;code&gt;tokenizer = tokenization.FullTokenizer&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tokenize the raw text with &lt;code&gt;tokens = tokenizer.tokenize(raw_text)&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Truncate to the maximum sequence length. (You can use up to 512, but you
probably want to use shorter if possible for memory and speed reasons.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[SEP]&lt;/code&gt; tokens in the right place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Word-level and span-level tasks (e.g., SQuAD and NER) are more complex, since
you need to maintain alignment between your input text and output text so that
you can project your training labels. SQuAD is a particularly complex example
because the input labels are &lt;em&gt;character&lt;/em&gt;-based, and SQuAD paragraphs are often
longer than our maximum sequence length. See the code in &lt;code&gt;run_squad.py&lt;/code&gt; to show
how we handle this.&lt;/p&gt;
&lt;p&gt;Before we describe the general recipe for handling word-level tasks, it's
important to understand what exactly our tokenizer is doing. It has three main
steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Text normalization&lt;/strong&gt;: Convert all whitespace characters to spaces, and
(for the &lt;code&gt;Uncased&lt;/code&gt; model) lowercase the input and strip out accent markers.
E.g., &lt;code&gt;John Johanson's, → john johanson's,&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Punctuation splitting&lt;/strong&gt;: Split &lt;em&gt;all&lt;/em&gt; punctuation characters on both sides
(i.e., add whitespace around all punctuation characters). Punctuation
characters are defined as (a) Anything with a &lt;code&gt;P*&lt;/code&gt; Unicode class, (b) any
non-letter/number/space ASCII character (e.g., characters like &lt;code&gt;$&lt;/code&gt; which are
technically not punctuation). E.g., &lt;code&gt;john johanson's, → john johanson ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;WordPiece tokenization&lt;/strong&gt;: Apply whitespace tokenization to the output of
the above procedure, and apply
&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py"&gt;WordPiece&lt;/a&gt;
tokenization to each token separately. (Our implementation is directly based
on the one from &lt;code&gt;tensor2tensor&lt;/code&gt;, which is linked). E.g., &lt;code&gt;john johanson ' s , → john johan ##son ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The advantage of this scheme is that it is "compatible" with most existing
English tokenizers. For example, imagine that you have a part-of-speech tagging
task which looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input:  John Johanson 's   house
Labels: NNP  NNP      POS NN
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tokenized output will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Tokens: john johan ##son ' s house
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Crucially, this would be the same output as if the raw text were &lt;code&gt;John Johanson's house&lt;/code&gt; (with no space before the &lt;code&gt;'s&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you have a pre-tokenized representation with word-level annotations, you can
simply tokenize each input word independently, and deterministically maintain an
original-to-tokenized alignment:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Input&lt;/span&gt;
orig_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;John&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Johanson&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;'s&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;house&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
labels      &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;POS&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Output&lt;/span&gt;
bert_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; []

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Token map will be an int -&amp;gt; int mapping between the `orig_tokens` index and&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; the `bert_tokens` index.&lt;/span&gt;
orig_to_tok_map &lt;span class="pl-k"&gt;=&lt;/span&gt; []

tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenization.FullTokenizer(
    &lt;span class="pl-v"&gt;vocab_file&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;vocab_file, &lt;span class="pl-v"&gt;do_lower_case&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[CLS]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;for&lt;/span&gt; orig_token &lt;span class="pl-k"&gt;in&lt;/span&gt; orig_tokens:
  orig_to_tok_map.append(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(bert_tokens))
  bert_tokens.extend(tokenizer.tokenize(orig_token))
bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[SEP]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; bert_tokens == ["[CLS]", "john", "johan", "##son", "'", "s", "house", "[SEP]"]&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; orig_to_tok_map == [1, 2, 4, 6]&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now &lt;code&gt;orig_to_tok_map&lt;/code&gt; can be used to project &lt;code&gt;labels&lt;/code&gt; to the tokenized
representation.&lt;/p&gt;
&lt;p&gt;There are common English tokenization schemes which will cause a slight mismatch
between how BERT was pre-trained. For example, if your input tokenization splits
off contractions like &lt;code&gt;do n't&lt;/code&gt;, this will cause a mismatch. If it is possible to
do so, you should pre-process your data to convert these back to raw-looking
text, but if it's not possible, this mismatch is likely not a big deal.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-training-with-bert" class="anchor" aria-hidden="true" href="#pre-training-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training with BERT&lt;/h2&gt;
&lt;p&gt;We are releasing code to do "masked LM" and "next sentence prediction" on an
arbitrary text corpus. Note that this is &lt;em&gt;not&lt;/em&gt; the exact code that was used for
the paper (the original code was written in C++, and had some additional
complexity), but this code does generate pre-training data as described in the
paper.&lt;/p&gt;
&lt;p&gt;Here's how to run the data generation. The input is a plain text file, with one
sentence per line. (It is important that these be actual sentences for the "next
sentence prediction" task). Documents are delimited by empty lines. The output
is a set of &lt;code&gt;tf.train.Example&lt;/code&gt;s serialized into &lt;code&gt;TFRecord&lt;/code&gt; file format.&lt;/p&gt;
&lt;p&gt;You can perform sentence segmentation with an off-the-shelf NLP toolkit such as
&lt;a href="https://spacy.io/" rel="nofollow"&gt;spaCy&lt;/a&gt;. The &lt;code&gt;create_pretraining_data.py&lt;/code&gt; script will
concatenate segments until they reach the maximum sequence length to minimize
computational waste from padding (see the script for more details). However, you
may want to intentionally add a slight amount of noise to your input data (e.g.,
randomly truncate 2% of input segments) to make it more robust to non-sentential
input during fine-tuning.&lt;/p&gt;
&lt;p&gt;This script stores all of the examples for the entire input file in memory, so
for large data files you should shard the input file and call the script
multiple times. (You can pass in a file glob to &lt;code&gt;run_pretraining.py&lt;/code&gt;, e.g.,
&lt;code&gt;tf_examples.tf_record*&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;max_predictions_per_seq&lt;/code&gt; is the maximum number of masked LM predictions per
sequence. You should set this to around &lt;code&gt;max_seq_length&lt;/code&gt; * &lt;code&gt;masked_lm_prob&lt;/code&gt; (the
script doesn't do that automatically because the exact value needs to be passed
to both scripts).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python create_pretraining_data.py \
  --input_file=./sample_text.txt \
  --output_file=/tmp/tf_examples.tfrecord \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's how to run the pre-training. Do not include &lt;code&gt;init_checkpoint&lt;/code&gt; if you are
pre-training from scratch. The model configuration (including vocab size) is
specified in &lt;code&gt;bert_config_file&lt;/code&gt;. This demo code only pre-trains for a small
number of steps (20), but in practice you will probably want to set
&lt;code&gt;num_train_steps&lt;/code&gt; to 10000 steps or more. The &lt;code&gt;max_seq_length&lt;/code&gt; and
&lt;code&gt;max_predictions_per_seq&lt;/code&gt; parameters passed to &lt;code&gt;run_pretraining.py&lt;/code&gt; must be the
same as &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_pretraining.py \
  --input_file=/tmp/tf_examples.tfrecord \
  --output_dir=/tmp/pretraining_output \
  --do_train=True \
  --do_eval=True \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --train_batch_size=32 \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --num_train_steps=20 \
  --num_warmup_steps=10 \
  --learning_rate=2e-5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will produce an output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  global_step = 20
  loss = 0.0979674
  masked_lm_accuracy = 0.985479
  masked_lm_loss = 0.0979328
  next_sentence_accuracy = 1.0
  next_sentence_loss = 3.45724e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that since our &lt;code&gt;sample_text.txt&lt;/code&gt; file is very small, this example training
will overfit that data in only a few steps and produce unrealistically high
accuracy numbers.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-tips-and-caveats" class="anchor" aria-hidden="true" href="#pre-training-tips-and-caveats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training tips and caveats&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;If using your own vocabulary, make sure to change &lt;code&gt;vocab_size&lt;/code&gt; in
&lt;code&gt;bert_config.json&lt;/code&gt;. If you use a larger vocabulary without changing this,
you will likely get NaNs when training on GPU or TPU due to unchecked
out-of-bounds access.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;If your task has a large domain-specific corpus available (e.g., "movie
reviews" or "scientific papers"), it will likely be beneficial to run
additional steps of pre-training on your corpus, starting from the BERT
checkpoint.&lt;/li&gt;
&lt;li&gt;The learning rate we used in the paper was 1e-4. However, if you are doing
additional steps of pre-training starting from an existing BERT checkpoint,
you should use a smaller learning rate (e.g., 2e-5).&lt;/li&gt;
&lt;li&gt;Current BERT models are English-only, but we do plan to release a
multilingual model which has been pre-trained on a lot of languages in the
near future (hopefully by the end of November 2018).&lt;/li&gt;
&lt;li&gt;Longer sequences are disproportionately expensive because attention is
quadratic to the sequence length. In other words, a batch of 64 sequences of
length 512 is much more expensive than a batch of 256 sequences of
length 128. The fully-connected/convolutional cost is the same, but the
attention cost is far greater for the 512-length sequences. Therefore, one
good recipe is to pre-train for, say, 90,000 steps with a sequence length of
128 and then for 10,000 additional steps with a sequence length of 512. The
very long sequences are mostly needed to learn positional embeddings, which
can be learned fairly quickly. Note that this does require generating the
data twice with different values of &lt;code&gt;max_seq_length&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you are pre-training from scratch, be prepared that pre-training is
computationally expensive, especially on GPUs. If you are pre-training from
scratch, our recommended recipe is to pre-train a &lt;code&gt;BERT-Base&lt;/code&gt; on a single
&lt;a href="https://cloud.google.com/tpu/docs/pricing" rel="nofollow"&gt;preemptible Cloud TPU v2&lt;/a&gt;, which
takes about 2 weeks at a cost of about $500 USD (based on the pricing in
October 2018). You will have to scale down the batch size when only training
on a single Cloud TPU, compared to what was used in the paper. It is
recommended to use the largest batch size that fits into TPU memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-data" class="anchor" aria-hidden="true" href="#pre-training-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training data&lt;/h3&gt;
&lt;p&gt;We will &lt;strong&gt;not&lt;/strong&gt; be able to release the pre-processed datasets used in the paper.
For Wikipedia, the recommended pre-processing is to download
&lt;a href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2" rel="nofollow"&gt;the latest dump&lt;/a&gt;,
extract the text with
&lt;a href="https://github.com/attardi/wikiextractor"&gt;&lt;code&gt;WikiExtractor.py&lt;/code&gt;&lt;/a&gt;, and then apply
any necessary cleanup to convert it into plain text.&lt;/p&gt;
&lt;p&gt;Unfortunately the researchers who collected the
&lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt; no longer have it available for
public download. The
&lt;a href="https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html" rel="nofollow"&gt;Project Guttenberg Dataset&lt;/a&gt;
is a somewhat smaller (200M word) collection of older books that are public
domain.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://commoncrawl.org/" rel="nofollow"&gt;Common Crawl&lt;/a&gt; is another very large collection of
text, but you will likely have to do substantial pre-processing and cleanup to
extract a usable corpus for pre-training BERT.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-learning-a-new-wordpiece-vocabulary" class="anchor" aria-hidden="true" href="#learning-a-new-wordpiece-vocabulary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learning a new WordPiece vocabulary&lt;/h3&gt;
&lt;p&gt;This repository does not include code for &lt;em&gt;learning&lt;/em&gt; a new WordPiece vocabulary.
The reason is that the code used in the paper was implemented in C++ with
dependencies on Google's internal libraries. For English, it is almost always
better to just start with our vocabulary and pre-trained models. For learning
vocabularies of other languages, there are a number of open source options
available. However, keep in mind that these are not compatible with our
&lt;code&gt;tokenization.py&lt;/code&gt; library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;Google's SentencePiece library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py"&gt;tensor2tensor's WordPiece generation script&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rsennrich/subword-nmt"&gt;Rico Sennrich's Byte Pair Encoding library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-in-colab" class="anchor" aria-hidden="true" href="#using-bert-in-colab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT in Colab&lt;/h2&gt;
&lt;p&gt;If you want to use BERT with &lt;a href="https://colab.research.google.com" rel="nofollow"&gt;Colab&lt;/a&gt;, you can
get started with the notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".
&lt;strong&gt;At the time of this writing (October 31st, 2018), Colab users can access a
Cloud TPU completely for free.&lt;/strong&gt; Note: One per user, availability limited,
requires a Google Cloud Platform account with storage (although storage may be
purchased with free credit for signing up with GCP), and this capability may not
longer be available in the future. Click on the BERT Colab that was just linked
for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-is-this-code-compatible-with-cloud-tpus-what-about-gpus" class="anchor" aria-hidden="true" href="#is-this-code-compatible-with-cloud-tpus-what-about-gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is this code compatible with Cloud TPUs? What about GPUs?&lt;/h4&gt;
&lt;p&gt;Yes, all of the code in this repository works out-of-the-box with CPU, GPU, and
Cloud TPU. However, GPU training is single-GPU only.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-i-am-getting-out-of-memory-errors-what-is-wrong" class="anchor" aria-hidden="true" href="#i-am-getting-out-of-memory-errors-what-is-wrong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I am getting out-of-memory errors, what is wrong?&lt;/h4&gt;
&lt;p&gt;See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for more
information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-pytorch-version-available" class="anchor" aria-hidden="true" href="#is-there-a-pytorch-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a PyTorch version available?&lt;/h4&gt;
&lt;p&gt;There is no official PyTorch implementation. However, NLP researchers from
HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-chainer-version-available" class="anchor" aria-hidden="true" href="#is-there-a-chainer-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a Chainer version available?&lt;/h4&gt;
&lt;p&gt;There is no official Chainer implementation. However, Sosuke Kobayashi made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the Chainer
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-in-other-languages-be-released" class="anchor" aria-hidden="true" href="#will-models-in-other-languages-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models in other languages be released?&lt;/h4&gt;
&lt;p&gt;Yes, we plan to release a multi-lingual BERT model in the near future. We cannot
make promises about exactly which languages will be included, but it will likely
be a single model which includes &lt;em&gt;most&lt;/em&gt; of the languages which have a
significantly-sized Wikipedia.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-larger-than-bert-large-be-released" class="anchor" aria-hidden="true" href="#will-models-larger-than-bert-large-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models larger than &lt;code&gt;BERT-Large&lt;/code&gt; be released?&lt;/h4&gt;
&lt;p&gt;So far we have not attempted to train anything larger than &lt;code&gt;BERT-Large&lt;/code&gt;. It is
possible that we will release larger models if we are able to obtain significant
improvements.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-what-license-is-this-library-released-under" class="anchor" aria-hidden="true" href="#what-license-is-this-library-released-under"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What license is this library released under?&lt;/h4&gt;
&lt;p&gt;All code &lt;em&gt;and&lt;/em&gt; models are released under the Apache 2.0 license. See the
&lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-how-do-i-cite-bert" class="anchor" aria-hidden="true" href="#how-do-i-cite-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I cite BERT?&lt;/h4&gt;
&lt;p&gt;For now, cite &lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;the Arxiv paper&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we submit the paper to a conference or journal, we will update the BibTeX.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;This is not an official Google product.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact-information" class="anchor" aria-hidden="true" href="#contact-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact information&lt;/h2&gt;
&lt;p&gt;For help or issues using BERT, please submit a GitHub issue.&lt;/p&gt;
&lt;p&gt;For personal communication related to BERT, please contact Jacob Devlin
(&lt;code&gt;jacobdevlin@google.com&lt;/code&gt;), Ming-Wei Chang (&lt;code&gt;mingweichang@google.com&lt;/code&gt;), or
Kenton Lee (&lt;code&gt;kentonl@google.com&lt;/code&gt;).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/bert</guid><pubDate>Mon, 11 Nov 2019 00:07:00 GMT</pubDate></item><item><title>pandas-dev/pandas #8 in Python, This month</title><link>https://github.com/pandas-dev/pandas</link><description>&lt;p&gt;&lt;i&gt;Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5cb734f6fc37f645dc900e35559c60d91cc6b550/68747470733a2f2f6465762e70616e6461732e696f2f7374617469632f696d672f70616e6461732e737667"&gt;&lt;img src="https://camo.githubusercontent.com/5cb734f6fc37f645dc900e35559c60d91cc6b550/68747470733a2f2f6465762e70616e6461732e696f2f7374617469632f696d672f70616e6461732e737667" data-canonical-src="https://dev.pandas.io/static/img/pandas.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h1&gt;&lt;a id="user-content-pandas-powerful-python-data-analysis-toolkit" class="anchor" aria-hidden="true" href="#pandas-powerful-python-data-analysis-toolkit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pandas: powerful Python data analysis toolkit&lt;/h1&gt;

  &lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
  &lt;td&gt;Latest Release&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://pypi.org/project/pandas/" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/0478449c23a65b3a5c72bdcda14a957b473f32d7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f70616e6461732e737667" alt="latest release" data-canonical-src="https://img.shields.io/pypi/v/pandas.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://anaconda.org/anaconda/pandas/" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/648d53d019be087dec2c296ede54ce5076386215/68747470733a2f2f616e61636f6e64612e6f72672f636f6e64612d666f7267652f70616e6461732f6261646765732f76657273696f6e2e737667" alt="latest release" data-canonical-src="https://anaconda.org/conda-forge/pandas/badges/version.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Package Status&lt;/td&gt;
  &lt;td&gt;
		&lt;a href="https://pypi.org/project/pandas/" rel="nofollow"&gt;
		&lt;img src="https://camo.githubusercontent.com/cd31982c75831c063fb3b9a3977994cabac40ec1/68747470733a2f2f696d672e736869656c64732e696f2f707970692f7374617475732f70616e6461732e737667" alt="status" data-canonical-src="https://img.shields.io/pypi/status/pandas.svg" style="max-width:100%;"&gt;
		&lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;License&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://github.com/pandas-dev/pandas/blob/master/LICENSE"&gt;
    &lt;img src="https://camo.githubusercontent.com/102808689f0625350eb8a5dcd0bca9b843d42064/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f70616e6461732e737667" alt="license" data-canonical-src="https://img.shields.io/pypi/l/pandas.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Build Status&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://travis-ci.org/pandas-dev/pandas" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/1a5ad213967c3966f2cdf4bb7abdeb257d4589d9/68747470733a2f2f7472617669732d63692e6f72672f70616e6461732d6465762f70616e6461732e7376673f6272616e63683d6d6173746572" alt="travis build status" data-canonical-src="https://travis-ci.org/pandas-dev/pandas.svg?branch=master" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://dev.azure.com/pandas-dev/pandas/_build/latest?definitionId=1&amp;amp;branch=master" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/521b8d04b7e476e5d78e7497527d8b4593649782/68747470733a2f2f6465762e617a7572652e636f6d2f70616e6461732d6465762f70616e6461732f5f617069732f6275696c642f7374617475732f70616e6461732d6465762e70616e6461733f6272616e63683d6d6173746572" alt="Azure Pipelines build status" data-canonical-src="https://dev.azure.com/pandas-dev/pandas/_apis/build/status/pandas-dev.pandas?branch=master" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Coverage&lt;/td&gt;&lt;td&gt;
    &lt;a href="https://codecov.io/gh/pandas-dev/pandas" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/141595bb5eadcc9952ad8593c54b3a4aa62327a7/68747470733a2f2f636f6465636f762e696f2f6769746875622f70616e6461732d6465762f70616e6461732f636f7665726167652e7376673f6272616e63683d6d6173746572" alt="coverage" data-canonical-src="https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=master" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;Downloads&lt;/td&gt;
  &lt;td&gt;
    &lt;a href="https://pandas.pydata.org" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/9c9bd93488cea17ae2ddb225cc8244a4175d26f8/68747470733a2f2f616e61636f6e64612e6f72672f636f6e64612d666f7267652f70616e6461732f6261646765732f646f776e6c6f6164732e737667" alt="conda-forge downloads" data-canonical-src="https://anaconda.org/conda-forge/pandas/badges/downloads.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
	&lt;td&gt;Gitter&lt;/td&gt;
	&lt;td&gt;
		&lt;a href="https://gitter.im/pydata/pandas" rel="nofollow"&gt;
		&lt;img src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;
		&lt;/a&gt;
	&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-it" class="anchor" aria-hidden="true" href="#what-is-it"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is it?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;pandas&lt;/strong&gt; is a Python package providing fast, flexible, and expressive data
structures designed to make working with "relational" or "labeled" data both
easy and intuitive. It aims to be the fundamental high-level building block for
doing practical, &lt;strong&gt;real world&lt;/strong&gt; data analysis in Python. Additionally, it has
the broader goal of becoming &lt;strong&gt;the most powerful and flexible open source data
analysis / manipulation tool available in any language&lt;/strong&gt;. It is already well on
its way towards this goal.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-main-features" class="anchor" aria-hidden="true" href="#main-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Features&lt;/h2&gt;
&lt;p&gt;Here are just a few of the things that pandas does well:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easy handling of &lt;a href="https://pandas.pydata.org/pandas-docs/stable/missing_data.html#working-with-missing-data" rel="nofollow"&gt;&lt;strong&gt;missing data&lt;/strong&gt;&lt;/a&gt; (represented as
&lt;code&gt;NaN&lt;/code&gt;) in floating point as well as non-floating point data&lt;/li&gt;
&lt;li&gt;Size mutability: columns can be &lt;a href="https://pandas.pydata.org/pandas-docs/stable/dsintro.html#column-selection-addition-deletion" rel="nofollow"&gt;&lt;strong&gt;inserted and
deleted&lt;/strong&gt;&lt;/a&gt; from DataFrame and higher dimensional
objects&lt;/li&gt;
&lt;li&gt;Automatic and explicit &lt;a href="https://pandas.pydata.org/pandas-docs/stable/dsintro.html?highlight=alignment#intro-to-data-structures" rel="nofollow"&gt;&lt;strong&gt;data alignment&lt;/strong&gt;&lt;/a&gt;: objects can
be explicitly aligned to a set of labels, or the user can simply
ignore the labels and let &lt;code&gt;Series&lt;/code&gt;, &lt;code&gt;DataFrame&lt;/code&gt;, etc. automatically
align the data for you in computations&lt;/li&gt;
&lt;li&gt;Powerful, flexible &lt;a href="https://pandas.pydata.org/pandas-docs/stable/groupby.html#group-by-split-apply-combine" rel="nofollow"&gt;&lt;strong&gt;group by&lt;/strong&gt;&lt;/a&gt; functionality to perform
split-apply-combine operations on data sets, for both aggregating
and transforming data&lt;/li&gt;
&lt;li&gt;Make it &lt;a href="https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe" rel="nofollow"&gt;&lt;strong&gt;easy to convert&lt;/strong&gt;&lt;/a&gt; ragged,
differently-indexed data in other Python and NumPy data structures
into DataFrame objects&lt;/li&gt;
&lt;li&gt;Intelligent label-based &lt;a href="https://pandas.pydata.org/pandas-docs/stable/indexing.html#slicing-ranges" rel="nofollow"&gt;&lt;strong&gt;slicing&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://pandas.pydata.org/pandas-docs/stable/indexing.html#advanced-indexing-with-ix" rel="nofollow"&gt;&lt;strong&gt;fancy
indexing&lt;/strong&gt;&lt;/a&gt;, and &lt;a href="https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing" rel="nofollow"&gt;&lt;strong&gt;subsetting&lt;/strong&gt;&lt;/a&gt; of
large data sets&lt;/li&gt;
&lt;li&gt;Intuitive &lt;a href="https://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging" rel="nofollow"&gt;&lt;strong&gt;merging&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://pandas.pydata.org/pandas-docs/stable/merging.html#joining-on-index" rel="nofollow"&gt;&lt;strong&gt;joining&lt;/strong&gt;&lt;/a&gt; data
sets&lt;/li&gt;
&lt;li&gt;Flexible &lt;a href="https://pandas.pydata.org/pandas-docs/stable/reshaping.html#reshaping-and-pivot-tables" rel="nofollow"&gt;&lt;strong&gt;reshaping&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://pandas.pydata.org/pandas-docs/stable/reshaping.html#pivot-tables-and-cross-tabulations" rel="nofollow"&gt;&lt;strong&gt;pivoting&lt;/strong&gt;&lt;/a&gt; of
data sets&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/indexing.html#hierarchical-indexing-multiindex" rel="nofollow"&gt;&lt;strong&gt;Hierarchical&lt;/strong&gt;&lt;/a&gt; labeling of axes (possible to have multiple
labels per tick)&lt;/li&gt;
&lt;li&gt;Robust IO tools for loading data from &lt;a href="https://pandas.pydata.org/pandas-docs/stable/io.html#csv-text-files" rel="nofollow"&gt;&lt;strong&gt;flat files&lt;/strong&gt;&lt;/a&gt;
(CSV and delimited), &lt;a href="https://pandas.pydata.org/pandas-docs/stable/io.html#excel-files" rel="nofollow"&gt;&lt;strong&gt;Excel files&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://pandas.pydata.org/pandas-docs/stable/io.html#sql-queries" rel="nofollow"&gt;&lt;strong&gt;databases&lt;/strong&gt;&lt;/a&gt;,
and saving/loading data from the ultrafast &lt;a href="https://pandas.pydata.org/pandas-docs/stable/io.html#hdf5-pytables" rel="nofollow"&gt;&lt;strong&gt;HDF5 format&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pandas.pydata.org/pandas-docs/stable/timeseries.html#time-series-date-functionality" rel="nofollow"&gt;&lt;strong&gt;Time series&lt;/strong&gt;&lt;/a&gt;-specific functionality: date range
generation and frequency conversion, moving window statistics,
moving window linear regressions, date shifting and lagging, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-where-to-get-it" class="anchor" aria-hidden="true" href="#where-to-get-it"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Where to get it&lt;/h2&gt;
&lt;p&gt;The source code is currently hosted on GitHub at:
&lt;a href="https://github.com/pandas-dev/pandas"&gt;https://github.com/pandas-dev/pandas&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Binary installers for the latest released version are available at the &lt;a href="https://pypi.org/project/pandas" rel="nofollow"&gt;Python
package index&lt;/a&gt; and on conda.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; conda&lt;/span&gt;
conda install pandas&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; or PyPI&lt;/span&gt;
pip install pandas&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-dependencies" class="anchor" aria-hidden="true" href="#dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dependencies&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.numpy.org" rel="nofollow"&gt;NumPy&lt;/a&gt;: 1.13.3 or higher&lt;/li&gt;
&lt;li&gt;&lt;a href="https://labix.org/python-dateutil" rel="nofollow"&gt;python-dateutil&lt;/a&gt;: 2.5.0 or higher&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pythonhosted.org/pytz" rel="nofollow"&gt;pytz&lt;/a&gt;: 2015.4 or higher&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See the &lt;a href="https://pandas.pydata.org/pandas-docs/stable/install.html#dependencies" rel="nofollow"&gt;full installation instructions&lt;/a&gt;
for recommended and optional dependencies.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation-from-sources" class="anchor" aria-hidden="true" href="#installation-from-sources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation from sources&lt;/h2&gt;
&lt;p&gt;To install pandas from source you need Cython in addition to the normal
dependencies above. Cython can be installed from pypi:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install cython&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the &lt;code&gt;pandas&lt;/code&gt; directory (same one where you found this file after
cloning the git repo), execute:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python setup.py install&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or for installing in &lt;a href="https://pip.pypa.io/en/latest/reference/pip_install.html#editable-installs" rel="nofollow"&gt;development mode&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m pip install -e &lt;span class="pl-c1"&gt;.&lt;/span&gt; --no-build-isolation --no-use-pep517&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you have &lt;code&gt;make&lt;/code&gt;, you can also use &lt;code&gt;make develop&lt;/code&gt; to run the same command.&lt;/p&gt;
&lt;p&gt;or alternatively&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python setup.py develop&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;See the full instructions for &lt;a href="https://pandas.pydata.org/pandas-docs/stable/install.html#installing-from-source" rel="nofollow"&gt;installing from source&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="LICENSE"&gt;BSD 3&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;The official documentation is hosted on PyData.org: &lt;a href="https://pandas.pydata.org/pandas-docs/stable" rel="nofollow"&gt;https://pandas.pydata.org/pandas-docs/stable&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-background" class="anchor" aria-hidden="true" href="#background"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Background&lt;/h2&gt;
&lt;p&gt;Work on &lt;code&gt;pandas&lt;/code&gt; started at AQR (a quantitative hedge fund) in 2008 and
has been under active development since then.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-help" class="anchor" aria-hidden="true" href="#getting-help"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting Help&lt;/h2&gt;
&lt;p&gt;For usage questions, the best place to go to is &lt;a href="https://stackoverflow.com/questions/tagged/pandas" rel="nofollow"&gt;StackOverflow&lt;/a&gt;.
Further, general questions and discussions can also take place on the &lt;a href="https://groups.google.com/forum/?fromgroups#!forum/pydata" rel="nofollow"&gt;pydata mailing list&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-discussion-and-development" class="anchor" aria-hidden="true" href="#discussion-and-development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Discussion and Development&lt;/h2&gt;
&lt;p&gt;Most development discussion is taking place on github in this repo. Further, the &lt;a href="https://mail.python.org/mailman/listinfo/pandas-dev" rel="nofollow"&gt;pandas-dev mailing list&lt;/a&gt; can also be used for specialized discussions or design issues, and a &lt;a href="https://gitter.im/pydata/pandas" rel="nofollow"&gt;Gitter channel&lt;/a&gt; is available for quick development related questions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing-to-pandas-" class="anchor" aria-hidden="true" href="#contributing-to-pandas-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing to pandas &lt;a href="https://www.codetriage.com/pandas-dev/pandas" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6332f3d9633f4d7fac6d388358c971829b9c8aff/68747470733a2f2f7777772e636f64657472696167652e636f6d2f70616e6461732d6465762f70616e6461732f6261646765732f75736572732e737667" alt="Open Source Helpers" data-canonical-src="https://www.codetriage.com/pandas-dev/pandas/badges/users.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;All contributions, bug reports, bug fixes, documentation improvements, enhancements and ideas are welcome.&lt;/p&gt;
&lt;p&gt;A detailed overview on how to contribute can be found in the &lt;strong&gt;&lt;a href="https://dev.pandas.io/docs/contributing.html" rel="nofollow"&gt;contributing guide&lt;/a&gt;&lt;/strong&gt;. There is also an &lt;a href=".github/CONTRIBUTING.md"&gt;overview&lt;/a&gt; on GitHub.&lt;/p&gt;
&lt;p&gt;If you are simply looking to start working with the pandas codebase, navigate to the &lt;a href="https://github.com/pandas-dev/pandas/issues"&gt;GitHub "issues" tab&lt;/a&gt; and start looking through interesting issues. There are a number of issues listed under &lt;a href="https://github.com/pandas-dev/pandas/issues?labels=Docs&amp;amp;sort=updated&amp;amp;state=open"&gt;Docs&lt;/a&gt; and &lt;a href="https://github.com/pandas-dev/pandas/issues?labels=good+first+issue&amp;amp;sort=updated&amp;amp;state=open"&gt;good first issue&lt;/a&gt; where you could start out.&lt;/p&gt;
&lt;p&gt;You can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to &lt;a href="https://www.codetriage.com/pandas-dev/pandas" rel="nofollow"&gt;subscribe to pandas on CodeTriage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Or maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking ‘this can be improved’...you can do something about it!&lt;/p&gt;
&lt;p&gt;Feel free to ask questions on the &lt;a href="https://groups.google.com/forum/?fromgroups#!forum/pydata" rel="nofollow"&gt;mailing list&lt;/a&gt; or on &lt;a href="https://gitter.im/pydata/pandas" rel="nofollow"&gt;Gitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. More information can be found at: &lt;a href="https://github.com/pandas-dev/pandas/blob/master/.github/CODE_OF_CONDUCT.md"&gt;Contributor Code of Conduct&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pandas-dev</author><guid isPermaLink="false">https://github.com/pandas-dev/pandas</guid><pubDate>Mon, 11 Nov 2019 00:08:00 GMT</pubDate></item><item><title>geekcomputers/Python #9 in Python, This month</title><link>https://github.com/geekcomputers/Python</link><description>&lt;p&gt;&lt;i&gt;My Python Examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-my-python-examples" class="anchor" aria-hidden="true" href="#my-python-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;My Python Examples&lt;/h1&gt;
&lt;p&gt;I do not consider myself a programmer. I create these little programs as experiments to play with Python, or to solve problems for myself. I would gladly accept pointers from others to improve, simplify, or make the code more efficient. If you would like to make any comments then please feel free to email me at &lt;a href="mailto:craig@geekcomputers.co.uk"&gt;craig@geekcomputers.co.uk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These scripts contain important functions which help reduce human workload.
Code documentation is aligned correctly when the files are viewed in &lt;a href="https://notepad-plus-plus.org/" rel="nofollow"&gt;Notepad++&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/batch_file_rename.py"&gt;batch_file_rename.py&lt;/a&gt; - This batch renames a group of files in a given directory, once you pass the current and the new extensions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/create_dir_if_not_there.py"&gt;create_dir_if_not_there.py&lt;/a&gt; - Checks to see if a directory exists in the users home directory. If a directory does not exist, then one will be created.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/youtube-downloader%20fast.py"&gt;Fast Youtube Downloader&lt;/a&gt; - Downloads YouTube videos quickly with parallel threads using aria2c.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/tree/master/Google_Image_Downloader"&gt;Google Image Downloader&lt;/a&gt; - Query a given term and retrieve images from the Google Image database.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/dir_test.py"&gt;dir_test.py&lt;/a&gt; - Tests to see if the directory &lt;code&gt;testdir&lt;/code&gt; exists, if not it will create the directory for you.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/env_check.py"&gt;env_check.py&lt;/a&gt; - This script will check to see if all of the environment variables required are set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/Ratna04priya/Python/blob/master/BlackJack_game/blackjack.py"&gt;blackjack.py&lt;/a&gt; - This script contains the Casino BlackJack-21 Game in Python.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/fileinfo.py"&gt;fileinfo.py&lt;/a&gt; - Shows file information for a given file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/folder_size.py"&gt;folder_size.py&lt;/a&gt; - Scans the current directory and all subdirectories and displays the size.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/logs.py"&gt;logs.py&lt;/a&gt; - This script will search for all &lt;code&gt;*.log&lt;/code&gt; files in the given directory, zip them using the program you specify, and then date stamp them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/move_files_over_x_days.py"&gt;move_files_over_x_days.py&lt;/a&gt; - Moves all files over a specified age (in days) from the source directory to the destination directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/nslookup_check.py"&gt;nslookup_check.py&lt;/a&gt; - This simple script opens the file &lt;code&gt;server_list.txt&lt;/code&gt; and then does an nslookup for each one to check the DNS entry.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/osinfo.py"&gt;osinfo.py&lt;/a&gt; - Displays some information about the OS on which you are running this script.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/ping_servers.py"&gt;ping_servers.py&lt;/a&gt; - This script, depending on the arguments supplied, will ping the servers associated with that application group.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/ping_subnet.py"&gt;ping_subnet.py&lt;/a&gt; - After supplying the first 3 octets this file scans the final range for available addresses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/powerdown_startup.py"&gt;powerdown_startup.py&lt;/a&gt; - This file goes through the server list and pings the machine, if it is up it will load the putty session, if it is not it will notify you.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/puttylogs.py"&gt;puttylogs.py&lt;/a&gt; -  This file zips up all the logs in the given directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/script_count.py"&gt;script_count.py&lt;/a&gt; - This file scans the scripts directory and gives a count of the different types of scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[get_youtube_view.py] - This is a simple python script used to get more views on your youtube videos. This script may also be used to repeat songs on Youtube.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/script_listing.py"&gt;script_listing.py&lt;/a&gt; - This file will list all the files in the given directory, and go through all the subdirectories as well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/testlines.py"&gt;testlines.py&lt;/a&gt; - This simple script opens a file and prints out 100 lines of whatever is the set for the line variable.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/tweeter.py"&gt;tweeter.py&lt;/a&gt; - Allows you to tweet text or a picture from the terminal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/serial_scanner.py"&gt;serial_scanner.py&lt;/a&gt; contains a method called ListAvailablePorts which returns a list with the names of the serial ports that are in use in the computer. This method works only on Linux and Windows (can be extended for mac osx). If no port is found, an empty list is returned.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/get_youtube_view.py"&gt;get_youtube_view.py&lt;/a&gt; - A simple python script to get more views for your YouTube videos. Useful for repeating songs on YouTube.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/CountMillionCharacter.py"&gt;CountMillionCharacter.py&lt;/a&gt; And &lt;a href="https://github.com/geekcomputers/Python/blob/master/CountMillionCharacters-2.0.py"&gt;CountMillionCharacter2.0&lt;/a&gt;.py - Gets character count of a text file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/xkcd_downloader.py"&gt;xkcd_downloader.py&lt;/a&gt; - Downloads the latest XKCD comic and places them in a new folder called "comics".&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/timymodule.py"&gt;timymodule.py&lt;/a&gt; - A great alternative to Pythons 'timeit' module and easier to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/calculator.py"&gt;calculator.py&lt;/a&gt; - Uses Python's eval() function to implement a calculator.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/Google_News.py"&gt;Google_News.py&lt;/a&gt; - Uses BeautifulSoup to provide Latest news headline along with news link.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/Cricket_score.py"&gt;cricket_live_score&lt;/a&gt; - Uses BeautifulSoup to provide live cricket score.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/youtube.py"&gt;youtube.py&lt;/a&gt; - Takes a song name as input and fetches the YouTube URL of the best matching song and plays it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/site_health.py"&gt;site_health.py&lt;/a&gt; - Checks the health of a remote server&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/SimpleStopWatch.py"&gt;SimpleStopWatch.py&lt;/a&gt; - Simple Stop Watch implementation using Python's time module.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/changemac.py"&gt;Changemac.py&lt;/a&gt; - This script change your MAC address , generate random MAC address or enter input as new MAC address in your linux(Successfully Tested in Ubuntu 18.04).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/geekcomputers/Python/blob/master/whatsapp-monitor.py"&gt;whatsapp-monitor.py&lt;/a&gt; - Uses Selenium to give online status about your contacts when your contacts become online in whatsapp you will get an update about it on terminal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/subahanii/whatsapp-Chat-Analyzer"&gt;whatsapp-chat-analyzer.py&lt;/a&gt; - This is whatsapp group/individual chat analyzer .
This script is able to analyse all activity happened in whatsapp group and visualize all thing through matplotlib library(In Graph form).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>geekcomputers</author><guid isPermaLink="false">https://github.com/geekcomputers/Python</guid><pubDate>Mon, 11 Nov 2019 00:09:00 GMT</pubDate></item><item><title>DrDonk/unlocker #10 in Python, This month</title><link>https://github.com/DrDonk/unlocker</link><description>&lt;p&gt;&lt;i&gt;VMware Workstation macOS &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body txt" data-path="readme.txt"&gt;&lt;div class="plain"&gt;&lt;pre style="white-space: pre-wrap"&gt;macOS Unlocker V3.0 for VMware Workstation
==========================================

+-----------------------------------------------------------------------------+
| IMPORTANT:                                                                  |
| ==========                                                                  |
|                                                                             |
| Always uninstall the previous version of the Unlocker before using a new    |
| version. Failure to do this could render VMware unusable.                   |
|                                                                             |
+-----------------------------------------------------------------------------+

1. Introduction
---------------

Unlocker 3 is designed for VMware Workstation 11-15 and Player 7-15.

If you are using an earlier product please continue using Unlocker 1.

Version 3 has been tested against:

* Workstation 11/12/14/15 on Windows and Linux
* Workstation Player 7/12/14/15 on Windows and Linux

The patch code carries out the following modifications dependent on the product
being patched:

* Fix vmware-vmx and derivatives to allow macOS to boot
* Fix vmwarebase .dll or .so to allow Apple to be selected during VM creation
* Download a copy of the latest VMware Tools for macOS

Note that not all products recognise the darwin.iso via install tools menu item.
You will have to manually mount the darwin.iso for example on Workstation 11 and Player 7.

In all cases make sure VMware is not running, and any background guests have
been shutdown.

The code is written in Python.

2. Prerequisites
----------------

The code requires Python 2.7 to work. Most Linux distros ship with a compatible
Python interpreter and should work without requiring any additional software.

Windows Unlocker has a packaged version of the Python script using PyInstaller, 
and so does not require Python to be installed.

3. Limitations
--------------

If you are using VMware Player or Workstation on Windows you may get a core dump.

Latest Linux products are OK and do not show this problem.

+-----------------------------------------------------------------------------+
| IMPORTANT:                                                                  |
| ==========                                                                  |
|                                                                             |
| If you create a new VM VMware may stop and create a core dump.              |
| There are two options to work around this issue:                            |
|                                                                             |
| 1. Change the VM to be HW 10 - this does not affect performance.            |
| 2. Edit the VMX file and add:                                               |
|    smc.version = "0"                                                        |
|                                                                             |
+-----------------------------------------------------------------------------+

4. Windows
----------
On Windows you will need to either run cmd.exe as Administrator or using
Explorer right click on the command file and select "Run as administrator".

win-install.cmd   - patches VMware
win-uninstall.cmd - restores VMware
win-update-tools.cmd - retrieves latest macOS guest tools

5. Linux
---------
On Linux you will need to be either root or use sudo to run the scripts.

You may need to ensure the Linux scripts have execute permissions
by running chmod +x against the 2 files.

lnx-install.sh   - patches VMware
lnx-uninstall.sh - restores VMware
lnx-update-tools.sh - retrieves latest macOS guest tools
   
6. Thanks
---------

Thanks to Zenith432 for originally building the C++ unlocker and Mac Son of Knife
(MSoK) for all the testing and support.

Thanks also to Sam B for finding the solution for ESXi 6 and helping me with
debugging expertise. Sam also wrote the code for patching ESXi ELF files and
modified the unlocker code to run on Python 3 in the ESXi 6.5 environment.


History
-------
27/09/18 3.0.0 - First release
02/10/18 3.0.1 - Fixed gettools.py to work with Python 3 and correctly download darwinPre15.iso
10/10/18 3.0.2 - Fixed false positives from anti-virus software with Windows executables
               - Allow Python 2 and 3 to run the Python code from Bash scripts


(c) 2011-2018 Dave Parsons&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</description><author>DrDonk</author><guid isPermaLink="false">https://github.com/DrDonk/unlocker</guid><pubDate>Mon, 11 Nov 2019 00:10:00 GMT</pubDate></item><item><title>apachecn/AiLearning #11 in Python, This month</title><link>https://github.com/apachecn/AiLearning</link><description>&lt;p&gt;&lt;i&gt;AiLearning: 机器学习 - MachineLearning - ML、深度学习 - DeepLearning - DL、自然语言处理 NLP&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;a href="https://www.apachecn.org" rel="nofollow"&gt;
        &lt;img width="200" src="https://camo.githubusercontent.com/9d35c24a9d070c56093b5598ef22afae12a2f45b/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f6c6f676f2e6a7067" data-canonical-src="http://data.apachecn.org/img/logo.jpg" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;br&gt;
    &lt;a href="https://www.apachecn.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/63fa00e49cf2df161cf6022c501b145d225bc335/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2533452d484f4d452d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/%3E-HOME-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="http://home.apachecn.org/about/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/971440c5a29c84e984688b2ebe165dc27aa8cf61/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2533452d41424f55542d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/%3E-ABOUT-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="mailto:apache@163.com"&gt;&lt;img src="https://camo.githubusercontent.com/f1fab6e562c98b86b95a7c44eae041e04022ec3e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2533452d456d61696c2d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/%3E-Email-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h1 align="center"&gt;&lt;a id="user-content-ai-learning" class="anchor" aria-hidden="true" href="#ai-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/apachecn/AiLearning"&gt;AI learning&lt;/a&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-组织介绍" class="anchor" aria-hidden="true" href="#组织介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;组织介绍&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;合作or侵权，请联系: &lt;code&gt;apachecn@163.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;我们不是 Apache 的官方组织/机构/团体，只是 Apache 技术栈（以及 AI）的爱好者！&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApacheCN - 学习机器学习群【629470233】&lt;a href="//shang.qq.com/wpa/qunwpa?idkey=30e5f1123a79867570f665aa3a483ca404b1c3f77737bc01ec520ed5f078ddef" rel="nofollow"&gt;&lt;img border="0" src="https://camo.githubusercontent.com/6ef3c468024fa0a3ac83d0084d8d0847c6f57769/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f6c6f676f2f417061636865434e2d67726f75702e706e67" alt="ApacheCN - 学习机器学习群[629470233]" title="ApacheCN - 学习机器学习群[629470233]" data-canonical-src="http://data.apachecn.org/img/logo/ApacheCN-group.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;欢迎任何人参与和完善：一个人可以走的很快，但是一群人却可以走的更远&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-路线图" class="anchor" aria-hidden="true" href="#路线图"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;路线图&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;入门只看: 步骤 1 =&amp;gt; 2 =&amp;gt; 3，你可以当大牛！&lt;/li&gt;
&lt;li&gt;中级补充 - 资料库: &lt;a href="https://github.com/apachecn/ai-roadmap"&gt;https://github.com/apachecn/ai-roadmap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-1机器学习---基础" class="anchor" aria-hidden="true" href="#1机器学习---基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.机器学习 - 基础&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-基本介绍" class="anchor" aria-hidden="true" href="#基本介绍"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;基本介绍&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;资料来源: Machine Learning in Action(机器学习实战-个人笔记)&lt;/li&gt;
&lt;li&gt;统一数据地址: &lt;a href="https://github.com/apachecn/data"&gt;https://github.com/apachecn/data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;书籍下载地址: &lt;a href="https://github.com/apachecn/data/tree/master/book"&gt;https://github.com/apachecn/data/tree/master/book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;机器学习下载地址: &lt;a href="https://github.com/apachecn/data/tree/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"&gt;https://github.com/apachecn/data/tree/master/机器学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;深度学习数据地址: &lt;a href="https://github.com/apachecn/data/tree/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"&gt;https://github.com/apachecn/data/tree/master/深度学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;推荐系统数据地址: &lt;a href="https://github.com/apachecn/data/tree/master/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"&gt;https://github.com/apachecn/data/tree/master/推荐系统&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;视频网站：优酷 ／bilibili / Acfun / 网易云课堂，可直接在线播放。（最下方有相应链接）&lt;/li&gt;
&lt;li&gt;-- 推荐 &lt;a href="https://github.com/RedstoneWill"&gt;红色石头&lt;/a&gt;: &lt;a href="https://github.com/apachecn/ntu-hsuantienlin-ml"&gt;台湾大学林轩田机器学习笔记&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;-- 推荐 &lt;a href="https://feisky.xyz/machine-learning" rel="nofollow"&gt;机器学习笔记&lt;/a&gt;: &lt;a href="https://feisky.xyz/machine-learning" rel="nofollow"&gt;https://feisky.xyz/machine-learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-学习文档" class="anchor" aria-hidden="true" href="#学习文档"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;学习文档&lt;/h3&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;th&gt;模块&lt;/th&gt;
    &lt;th&gt;章节&lt;/th&gt;
    &lt;th&gt;类型&lt;/th&gt;
    &lt;th&gt;负责人(GitHub)&lt;/th&gt;
    &lt;th&gt;QQ&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/1.机器学习基础.md"&gt; 第 1 章: 机器学习基础&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;介绍&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/ElmaDavies"&gt;@毛红动&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;1306014226&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/2.k-近邻算法.md"&gt;第 2 章: KNN 近邻算法&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;分类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/youyj521"&gt;@尤永江&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;279393323&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/3.决策树.md"&gt;第 3 章: 决策树&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;分类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/jingwangfei"&gt;@景涛&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;844300439&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/4.朴素贝叶斯.md"&gt;第 4 章: 朴素贝叶斯&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;分类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/wnma3mz"&gt;@wnma3mz&lt;/a&gt;&lt;br&gt;&lt;a href="https://github.com/kailian"&gt;@分析&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;1003324213&lt;br&gt;244970749&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/5.Logistic回归.md"&gt;第 5 章: Logistic回归&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;分类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/DataMonk2017"&gt;@微光同尘&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;529925688&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/6.支持向量机.md"&gt;第 6 章: SVM 支持向量机&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;分类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/VPrincekin"&gt;@王德红&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;934969547&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;网上组合内容&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/7.集成方法-随机森林和AdaBoost.md"&gt;第 7 章: 集成方法（随机森林和 AdaBoost）&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;分类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/jiangzhonglian"&gt;@片刻&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;529815144&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/8.回归.md"&gt;第 8 章: 回归&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;回归&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/DataMonk2017"&gt;@微光同尘&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;529925688&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/9.树回归.md"&gt;第 9 章: 树回归&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;回归&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/DataMonk2017"&gt;@微光同尘&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;529925688&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/10.k-means聚类.md"&gt;第 10 章: K-Means 聚类&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;聚类&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/xuzhaoqing"&gt;@徐昭清&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;827106588&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/11.使用Apriori算法进行关联分析.md"&gt;第 11 章: 利用 Apriori 算法进行关联分析&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;频繁项集&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/WindZQ"&gt;@刘海飞&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;1049498972&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/12.使用FP-growth算法来高效发现频繁项集.md"&gt;第 12 章: FP-growth 高效发现频繁项集&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;频繁项集&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/mikechengwei"&gt;@程威&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;842725815&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/13.利用PCA来简化数据.md"&gt;第 13 章: 利用 PCA 来简化数据&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;工具&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/lljuan330"&gt;@廖立娟&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;835670618&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/14.利用SVD简化数据.md"&gt;第 14 章: 利用 SVD 来简化数据&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;工具&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/marsjhao"&gt;@张俊皓&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;714974242&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;机器学习实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/15.大数据与MapReduce.md"&gt;第 15 章: 大数据与 MapReduce&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;工具&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/wnma3mz"&gt;@wnma3mz&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;1003324213&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Ml项目实战&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/16.推荐系统.md"&gt;第 16 章: 推荐系统（已迁移）&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;项目&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/apachecn/RecommenderSystems"&gt;推荐系统（迁移后地址）&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;第一期的总结&lt;/td&gt;
    &lt;td&gt;&lt;a href="report/2017-04-08_第一期的总结.md"&gt;2017-04-08: 第一期的总结&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;总结&lt;/td&gt;
    &lt;td&gt;总结&lt;/td&gt;
    &lt;td&gt;529815144&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-网站视频" class="anchor" aria-hidden="true" href="#网站视频"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;网站视频&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/question/20691338/answer/248678328" rel="nofollow"&gt;知乎问答-爆炸啦-机器学习该怎么入门？&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;当然我知道，第一句就会被吐槽，因为科班出身的人，不屑的吐了一口唾沫，说傻X，还评论 Andrew Ng 的视频。。&lt;/p&gt;
&lt;p&gt;我还知道还有一部分人，看 Andrew Ng 的视频就是看不懂，那神秘的数学推导，那迷之微笑的英文版的教学，我何尝又不是这样走过来的？？ 我的心可能比你们都痛，因为我在网上收藏过上10部《机器学习》相关视频，外加国内本土风格的教程：7月+小象 等等，我都很难去听懂，直到有一天，被一个百度的高级算法分析师推荐说：《机器学习实战》还不错，通俗易懂，你去试试？？&lt;/p&gt;
&lt;p&gt;我试了试，还好我的Python基础和调试能力还不错，基本上代码都调试过一遍，很多高大上的 "理论+推导"，在我眼中变成了几个 "加减乘除+循环"，我想这不就是像我这样的程序员想要的入门教程么？&lt;/p&gt;
&lt;p&gt;很多程序员说机器学习 TM 太难学了，是的，真 TM 难学，我想最难的是：没有一本像《机器学习实战》那样的作者愿意以程序员 Coding 角度去给大家讲解！！&lt;/p&gt;
&lt;p&gt;最近几天，GitHub 涨了 300颗 star，加群的200人， 现在还在不断的增加++，我想大家可能都是感同身受吧！&lt;/p&gt;
&lt;p&gt;很多想入门新手就是被忽悠着收藏收藏再收藏，但是最后还是什么都没有学到，也就是"资源收藏家"，也许新手要的就是 &lt;a href="https://docs.apachecn.org/map" rel="nofollow"&gt;MachineLearning(机器学习) 学习路线图&lt;/a&gt;。没错，我可以给你们的一份，因为我们还通过视频记录下来我们的学习过程。水平当然也有限，不过对于新手入门，绝对没问题，如果你还不会，那算我输！！&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;视频怎么看？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/4aeb51811bbc73e4715ced3ebc64ab26670bdc67/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434e2d4d4c2d62696c6962696c692d636f6d706172652e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/4aeb51811bbc73e4715ced3ebc64ab26670bdc67/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434e2d4d4c2d62696c6962696c692d636f6d706172652e6a7067" alt="" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCN-ML-bilibili-compare.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;理论科班出身-建议去学习 Andrew Ng 的视频（Ng 的视频绝对是权威，这个毋庸置疑）&lt;/li&gt;
&lt;li&gt;编码能力强 - 建议看我们的&lt;a href="https://space.bilibili.com/97678687/#!/channel/detail?cid=22486" rel="nofollow"&gt;《机器学习实战-教学版》&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;编码能力弱 - 建议看我们的&lt;a href="https://space.bilibili.com/97678687/#!/channel/detail?cid=13045" rel="nofollow"&gt;《机器学习实战-讨论版》&lt;/a&gt;，不过在看理论的时候，看 教学版-理论部分；讨论版的废话太多，不过在讲解代码的时候是一行一行讲解的；所以，根据自己的需求，自由的组合。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;【免费】数学教学视频 - 可汗学院 入门篇&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;@于振梓&lt;/a&gt; 推荐: 可汗学院-网易公开课&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;概率&lt;/th&gt;
&lt;th&gt;统计&lt;/th&gt;
&lt;th&gt;线性代数&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://open.163.com/special/Khan/probability.html" rel="nofollow"&gt;可汗学院(概率)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://open.163.com/special/Khan/khstatistics.html" rel="nofollow"&gt;可汗学院(统计学)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://open.163.com/special/Khan/linearalgebra.html" rel="nofollow"&gt;可汗学院(线性代数)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;机器学习视频 - ApacheCN 教学版&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;AcFun&lt;/td&gt;
&lt;td&gt;B站&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a title="AcFun（机器学习视频）" href="http://www.acfun.cn/u/12540256.aspx#page=1" rel="nofollow"&gt;&lt;img width="290" src="https://camo.githubusercontent.com/122aa82278121b78486f6cc20b3813851832d1b0/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434e2d4d4c2d416346756e2e6a7067" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCN-ML-AcFun.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a title="bilibili（机器学习视频）" href="https://space.bilibili.com/97678687/#!/channel/index" rel="nofollow"&gt;&lt;img width="290" src="https://camo.githubusercontent.com/c04107f0478a7e3e15b5103004d534af8ec36afd/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434e2d4d4c2d62696c6962696c692e6a7067" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCN-ML-bilibili.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;优酷&lt;/td&gt;
&lt;td&gt;网易云课堂&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a title="YouKu（机器学习视频）" href="http://i.youku.com/apachecn" rel="nofollow"&gt;&lt;img width="290" src="https://camo.githubusercontent.com/ff306cb8a0eac3f6a9a59f01e73b538b0be8588e/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434d2d4d4c2d796f756b752e6a7067" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCM-ML-youku.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a title="WangYiYunKeTang（机器学习视频）" href="http://study.163.com/course/courseMain.htm?courseId=1004582003" rel="nofollow"&gt;&lt;img width="290" src="https://camo.githubusercontent.com/615d94638b0fefc25253ca5a4147a5153e35379f/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434d2d4d4c2d57616e67596959756e4b6554616e672e706e67" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCM-ML-WangYiYunKeTang.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;【免费】机器/深度学习视频 - 吴恩达&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;机器学习&lt;/th&gt;
&lt;th&gt;深度学习&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://study.163.com/course/courseMain.htm?courseId=1004570029" rel="nofollow"&gt;吴恩达机器学习&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://mooc.study.163.com/course/2001281002?tid=2001392029" rel="nofollow"&gt;神经网络和深度学习&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-2深度学习" class="anchor" aria-hidden="true" href="#2深度学习"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.深度学习&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-入门基础" class="anchor" aria-hidden="true" href="#入门基础"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;入门基础&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="/docs/dl/%E5%8F%8D%E5%90%91%E4%BC%A0%E9%80%92.md"&gt;反向传递&lt;/a&gt;: &lt;a href="https://www.cnblogs.com/charlotte77/p/5629865.html" rel="nofollow"&gt;https://www.cnblogs.com/charlotte77/p/5629865.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/docs/dl/CNN%E5%8E%9F%E7%90%86.md"&gt;CNN原理&lt;/a&gt;: &lt;a href="http://www.cnblogs.com/charlotte77/p/7759802.html" rel="nofollow"&gt;http://www.cnblogs.com/charlotte77/p/7759802.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/docs/dl/RNN%E5%8E%9F%E7%90%86.md"&gt;RNN原理&lt;/a&gt;: &lt;a href="https://blog.csdn.net/qq_39422642/article/details/78676567" rel="nofollow"&gt;https://blog.csdn.net/qq_39422642/article/details/78676567&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/docs/dl/LSTM%E5%8E%9F%E7%90%86.md"&gt;LSTM原理&lt;/a&gt;: &lt;a href="https://blog.csdn.net/weixin_42111770/article/details/80900575" rel="nofollow"&gt;https://blog.csdn.net/weixin_42111770/article/details/80900575&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-pytorch---教程" class="anchor" aria-hidden="true" href="#pytorch---教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pytorch - 教程&lt;/h3&gt;
&lt;p&gt;-- 待更新&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-tensorflow-20---教程" class="anchor" aria-hidden="true" href="#tensorflow-20---教程"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 2.0 - 教程&lt;/h3&gt;
&lt;p&gt;-- 待更新&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;目录结构:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97.md"&gt;安装指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/Keras%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.md"&gt;Kears 快速入门&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE_1_%E7%94%B5%E5%BD%B1%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB.md"&gt;实战项目 1 电影情感分类&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE_2_%E6%B1%BD%E8%BD%A6%E7%87%83%E6%B2%B9%E6%95%88%E7%8E%87.md"&gt;实战项目 2 汽车燃油效率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE%E4%BC%98%E5%8C%96_%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88.md"&gt;实战项目 优化: 过拟合，欠拟合&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-3自然语言处理" class="anchor" aria-hidden="true" href="#3自然语言处理"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3.自然语言处理&lt;/h2&gt;
&lt;p&gt;学习过程中-内心复杂的变化！！！&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;自从学习&lt;span class="pl-c1"&gt;NLP&lt;/span&gt;以后，才发现国内与国外的典型区别:
&lt;span class="pl-c1"&gt;1&lt;/span&gt;. 对资源的态度是完全相反的:
  &lt;span class="pl-c1"&gt;1&lt;/span&gt;) 国内：就好像为了名气，举办工作装逼的会议，就是没有干货，全部都是象征性的&lt;span class="pl-c1"&gt;PPT&lt;/span&gt;介绍，不是针对在做的各位
  &lt;span class="pl-c1"&gt;2&lt;/span&gt;）国外：就好像是为了推动nlp进步一样，分享者各种干货资料和具体的实现。（特别是: python自然语言处理）
&lt;span class="pl-c1"&gt;2&lt;/span&gt;. 论文的实现：
  &lt;span class="pl-c1"&gt;1&lt;/span&gt;) 各种高大上的论文实现，却还是没看到一个像样的GitHub项目！（可能我的搜索能力差了点，一直没找到）
  &lt;span class="pl-c1"&gt;2&lt;/span&gt;）国外就不举例了，我看不懂！
&lt;span class="pl-c1"&gt;3&lt;/span&gt;. 开源的框架
  &lt;span class="pl-c1"&gt;1&lt;/span&gt;）国外的开源框架： tensorflow&lt;span class="pl-k"&gt;/&lt;/span&gt;pytorch 文档&lt;span class="pl-k"&gt;+&lt;/span&gt;教程&lt;span class="pl-k"&gt;+&lt;/span&gt;视频（官方提供）
  &lt;span class="pl-c1"&gt;2&lt;/span&gt;) 国内的开源框架: 额额，还真举例不出来！但是牛逼吹得不比国外差！（MXNet虽然有众多国人参与开发，但不能算是国内开源框架。基于MXNet的动手学深度学习(http:&lt;span class="pl-k"&gt;//&lt;/span&gt;zh.d2l.ai &lt;span class="pl-k"&gt;&amp;amp;&lt;/span&gt; https:&lt;span class="pl-k"&gt;//&lt;/span&gt;discuss.gluon.ai&lt;span class="pl-k"&gt;/&lt;/span&gt;t&lt;span class="pl-k"&gt;/&lt;/span&gt;topic&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;753&lt;/span&gt;)中文教程,已经由沐神(李沐)以及阿斯顿·张讲授录制，公开发布(文档&lt;span class="pl-k"&gt;+&lt;/span&gt;第一季教程&lt;span class="pl-k"&gt;+&lt;/span&gt;视频）。)
每一次深入都要去翻墙，每一次深入都要Google，每一次看着国内的说：哈工大、讯飞、中科大、百度、阿里多牛逼，但是资料还是得国外去找！
有时候真的挺恨的！真的有点瞧不起自己国内的技术环境！

当然谢谢国内很多博客大佬，特别是一些入门的Demo和基本概念。【深入的水平有限，没看懂】&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/6535fd936f9a2b1db5392b626320ce725854f675/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f6e6c702f46393435383146363443323141313039344134373333393744464134324639432e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/6535fd936f9a2b1db5392b626320ce725854f675/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f6e6c702f46393435383146363443323141313039344134373333393744464134324639432e6a7067" alt="" data-canonical-src="http://data.apachecn.org/img/AiLearning/nlp/F94581F64C21A1094A473397DFA42F9C.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;【入门须知】必须了解&lt;/strong&gt;: &lt;a href="https://github.com/apachecn/AiLearning/tree/master/docs/nlp"&gt;https://github.com/apachecn/AiLearning/tree/master/docs/nlp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;【入门教程】强烈推荐: PyTorch 自然语言处理&lt;/strong&gt;: &lt;a href="https://github.com/apachecn/NLP-with-PyTorch"&gt;https://github.com/apachecn/NLP-with-PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Python 自然语言处理 第二版: &lt;a href="https://usyiyi.github.io/nlp-py-2e-zh" rel="nofollow"&gt;https://usyiyi.github.io/nlp-py-2e-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;推荐一个&lt;a href="https://github.com/liuhuanyong"&gt;liuhuanyong大佬&lt;/a&gt;整理的nlp全面知识体系: &lt;a href="https://liuhuanyong.github.io" rel="nofollow"&gt;https://liuhuanyong.github.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;开源 - 词向量库集合:
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Embedding/Chinese-Word-Vectors"&gt;https://github.com/Embedding/Chinese-Word-Vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/brightmart/nlp_chinese_corpus"&gt;https://github.com/brightmart/nlp_chinese_corpus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/codemayq/chinese_chatbot_corpus"&gt;https://github.com/codemayq/chinese_chatbot_corpus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/candlewill/Dialog_Corpus"&gt;https://github.com/candlewill/Dialog_Corpus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-1使用场景-百度公开课" class="anchor" aria-hidden="true" href="#1使用场景-百度公开课"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.使用场景 （百度公开课）&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;第一部分 入门介绍&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;1.) &lt;a href="/docs/nlp/1.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D.md"&gt;自然语言处理入门介绍&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;第二部分 机器翻译&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;2.) &lt;a href="/docs/nlp/2.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91.md"&gt;机器翻译&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;第三部分 篇章分析&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;3.1.) &lt;a href="/docs/nlp/3.1.%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90-%E5%86%85%E5%AE%B9%E6%A6%82%E8%BF%B0.md"&gt;篇章分析-内容概述&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.2.) &lt;a href="/docs/nlp/3.2.%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90-%E5%86%85%E5%AE%B9%E6%A0%87%E7%AD%BE.md"&gt;篇章分析-内容标签&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.3.) &lt;a href="/docs/nlp/3.3.%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.md"&gt;篇章分析-情感分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.4.) &lt;a href="/docs/nlp/3.4.%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90-%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81.md"&gt;篇章分析-自动摘要&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;第四部分 UNIT-语言理解与交互技术&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;4.) &lt;a href="/docs/nlp/4.UNIT-%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E4%BA%A4%E4%BA%92%E6%8A%80%E6%9C%AF.md"&gt;UNIT-语言理解与交互技术&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-应用领域" class="anchor" aria-hidden="true" href="#应用领域"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;应用领域&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-中文分词" class="anchor" aria-hidden="true" href="#中文分词"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;中文分词：&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;构建DAG图&lt;/li&gt;
&lt;li&gt;动态规划查找，综合正反向（正向加权反向输出）求得DAG最大概率路径&lt;/li&gt;
&lt;li&gt;使用了SBME语料训练了一套 HMM + Viterbi 模型，解决未登录词问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-1文本分类text-classification" class="anchor" aria-hidden="true" href="#1文本分类text-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.文本分类（Text Classification）&lt;/h4&gt;
&lt;p&gt;文本分类是指标记句子或文档，例如电子邮件垃圾邮件分类和情感分析。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者文本分类数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html" rel="nofollow"&gt;路透社Newswire主题分类&lt;/a&gt;（路透社-21578）。1987年路透社出现的一系列新闻文件，按类别编制索引。&lt;a href="http://trec.nist.gov/data/reuters/reuters.html" rel="nofollow"&gt;另见RCV1，RCV2和TRC2&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ai.stanford.edu/~amaas/data/sentiment" rel="nofollow"&gt;IMDB电影评论情感分类（斯坦福）&lt;/a&gt;。来自网站imdb.com的一系列电影评论及其积极或消极的情绪。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/" rel="nofollow"&gt;新闻组电影评论情感分类（康奈尔）&lt;/a&gt;。来自网站imdb.com的一系列电影评论及其积极或消极的情绪。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;有关更多信息，请参阅帖子：
&lt;a href="http://ana.cachopo.org/datasets-for-single-label-text-categorization" rel="nofollow"&gt;单标签文本分类的数据集&lt;/a&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;情感分析&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;比赛地址: &lt;a href="https://www.kaggle.com/c/word2vec-nlp-tutorial" rel="nofollow"&gt;https://www.kaggle.com/c/word2vec-nlp-tutorial&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方案一(0.86)：WordCount + 朴素 Bayes&lt;/li&gt;
&lt;li&gt;方案二(0.94)：LDA + 分类模型（knn/决策树/逻辑回归/svm/xgboost/随机森林）
&lt;ul&gt;
&lt;li&gt;a) 决策树效果不是很好，这种连续特征不太适合的&lt;/li&gt;
&lt;li&gt;b) 通过参数调整 200 个topic，信息量保存效果较优（计算主题）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;方案三(0.72)：word2vec + CNN
&lt;ul&gt;
&lt;li&gt;说实话：没有一个好的机器，是调不出来一个好的结果 (: 逃&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;通过AUC 来评估模型的效果&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-2语言模型language-modeling" class="anchor" aria-hidden="true" href="#2语言模型language-modeling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.语言模型（Language Modeling）&lt;/h4&gt;
&lt;p&gt;语言建模涉及开发一种统计模型，用于预测句子中的下一个单词或一个单词中的下一个单词。它是语音识别和机器翻译等任务中的前置任务。&lt;/p&gt;
&lt;p&gt;它是语音识别和机器翻译等任务中的前置任务。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者语言建模数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.gutenberg.org/" rel="nofollow"&gt;古腾堡项目&lt;/a&gt;，一系列免费书籍，可以用纯文本检索各种语言。&lt;/li&gt;
&lt;li&gt;还有更多正式的语料库得到了很好的研究; 例如：
&lt;a href="https://en.wikipedia.org/wiki/Brown_Corpus" rel="nofollow"&gt;布朗大学现代美国英语标准语料库&lt;/a&gt;。大量英语单词样本。
&lt;a href="https://github.com/ciprian-chelba/1-billion-word-language-modeling-benchmark"&gt;谷歌10亿字语料库&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;新词发现&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;中文分词新词发现&lt;/li&gt;
&lt;li&gt;python3利用互信息和左右信息熵的中文分词新词发现&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanzecheng/Chinese_segment_augment"&gt;https://github.com/zhanzecheng/Chinese_segment_augment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;句子相似度识别&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;项目地址: &lt;a href="https://www.kaggle.com/c/quora-question-pairs" rel="nofollow"&gt;https://www.kaggle.com/c/quora-question-pairs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;解决方案: word2vec + Bi-GRU&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;文本纠错&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;bi-gram + levenshtein&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-3图像字幕image-captioning" class="anchor" aria-hidden="true" href="#3图像字幕image-captioning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3.图像字幕（Image Captioning）&lt;/h4&gt;
&lt;p&gt;mage字幕是为给定图像生成文本描述的任务。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者图像字幕数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://mscoco.org/dataset/#overview" rel="nofollow"&gt;上下文中的公共对象（COCO）&lt;/a&gt;。包含超过12万张带描述的图像的集合&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html" rel="nofollow"&gt;Flickr 8K&lt;/a&gt;。从flickr.com获取的8千个描述图像的集合。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://shannon.cs.illinois.edu/DenotationGraph/" rel="nofollow"&gt;Flickr 30K&lt;/a&gt;。从flickr.com获取的3万个描述图像的集合。
欲了解更多，请看帖子：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="http://sidgan.me/technical/2016/01/09/Exploring-Datasets" rel="nofollow"&gt;探索图像字幕数据集，2016年&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-4机器翻译machine-translation" class="anchor" aria-hidden="true" href="#4机器翻译machine-translation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.机器翻译（Machine Translation）&lt;/h4&gt;
&lt;p&gt;机器翻译是将文本从一种语言翻译成另一种语言的任务。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者机器翻译数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.isi.edu/natural-language/download/hansard/" rel="nofollow"&gt;加拿大第36届议会的协调国会议员&lt;/a&gt;。成对的英语和法语句子。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.statmt.org/europarl/" rel="nofollow"&gt;欧洲议会诉讼平行语料库1996-2011&lt;/a&gt;。句子对一套欧洲语言。
有大量标准数据集用于年度机器翻译挑战; 看到：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="http://www.statmt.org/" rel="nofollow"&gt;统计机器翻译&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;机器翻译&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Encoder + Decoder(Attention)&lt;/li&gt;
&lt;li&gt;参考案例: &lt;a href="http://pytorch.apachecn.org/cn/tutorials/intermediate/seq2seq_translation_tutorial.html" rel="nofollow"&gt;http://pytorch.apachecn.org/cn/tutorials/intermediate/seq2seq_translation_tutorial.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-5问答系统question-answering" class="anchor" aria-hidden="true" href="#5问答系统question-answering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5.问答系统（Question Answering）&lt;/h4&gt;
&lt;p&gt;问答是一项任务，其中提供了一个句子或文本样本，从中提出问题并且必须回答问题。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者问题回答数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;斯坦福问题回答数据集（SQuAD）&lt;/a&gt;。回答有关维基百科文章的问题。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/deepmind/rc-data"&gt;Deepmind问题回答语料库&lt;/a&gt;。从每日邮报回答有关新闻文章的问题。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmcauley.ucsd.edu/data/amazon/qa/" rel="nofollow"&gt;亚马逊问答数据&lt;/a&gt;。回答有关亚马逊产品的问题。
有关更多信息，请参阅帖子：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="https://www.quora.com/Datasets-How-can-I-get-corpus-of-a-question-answering-website-like-Quora-or-Yahoo-Answers-or-Stack-Overflow-for-analyzing-answer-quality" rel="nofollow"&gt;数据集：我如何获得问答网站的语料库，如Quora或Yahoo Answers或Stack Overflow来分析答案质量？&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-6语音识别speech-recognition" class="anchor" aria-hidden="true" href="#6语音识别speech-recognition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;6.语音识别（Speech Recognition）&lt;/h4&gt;
&lt;p&gt;语音识别是将口语的音频转换为人类可读文本的任务。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者语音识别数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://catalog.ldc.upenn.edu/LDC93S1" rel="nofollow"&gt;TIMIT声学 - 语音连续语音语料库&lt;/a&gt;。不是免费的，但因其广泛使用而上市。口语美国英语和相关的转录。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://voxforge.org/" rel="nofollow"&gt;VoxForge&lt;/a&gt;。用于构建用于语音识别的开源数据库的项目。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.openslr.org/12/" rel="nofollow"&gt;LibriSpeech ASR语料库&lt;/a&gt;。从LibriVox收集的大量英语有声读物。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-7自动文摘document-summarization" class="anchor" aria-hidden="true" href="#7自动文摘document-summarization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;7.自动文摘（Document Summarization）&lt;/h4&gt;
&lt;p&gt;文档摘要是创建较大文档的简短有意义描述的任务。&lt;/p&gt;
&lt;p&gt;下面是一些很好的初学者文档摘要数据集。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports" rel="nofollow"&gt;法律案例报告数据集&lt;/a&gt;。收集了4000份法律案件及其摘要。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www-nlpir.nist.gov/related_projects/tipster_summac/cmp_lg.html" rel="nofollow"&gt;TIPSTER文本摘要评估会议语料库&lt;/a&gt;。收集了近200份文件及其摘要。&lt;/li&gt;
&lt;li&gt;&lt;a href="https://catalog.ldc.upenn.edu/LDC2002T31" rel="nofollow"&gt;英语新闻文本的AQUAINT语料库&lt;/a&gt;。不是免费的，而是广泛使用的。新闻文章的语料库。
欲了解更多信息：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="http://www-nlpir.nist.gov/projects/duc/data.html" rel="nofollow"&gt;文档理解会议（DUC）任务&lt;/a&gt;。
&lt;a href="https://www.quora.com/Where-can-I-find-good-data-sets-for-text-summarization" rel="nofollow"&gt;在哪里可以找到用于文本摘要的良好数据集？&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;命名实体识别&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Bi-LSTM CRF&lt;/li&gt;
&lt;li&gt;参考案例: &lt;a href="http://pytorch.apachecn.org/cn/tutorials/beginner/nlp/advanced_tutorial.html" rel="nofollow"&gt;http://pytorch.apachecn.org/cn/tutorials/beginner/nlp/advanced_tutorial.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CRF推荐文档: &lt;a href="https://www.jianshu.com/p/55755fc649b1" rel="nofollow"&gt;https://www.jianshu.com/p/55755fc649b1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;文本摘要&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;抽取式&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;word2vec + textrank&lt;/li&gt;
&lt;li&gt;word2vec推荐文档: &lt;a href="https://www.zhihu.com/question/44832436/answer/266068967" rel="nofollow"&gt;https://www.zhihu.com/question/44832436/answer/266068967&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;textrank推荐文档: &lt;a href="https://blog.csdn.net/BaiHuaXiu123/article/details/77847232" rel="nofollow"&gt;https://blog.csdn.net/BaiHuaXiu123/article/details/77847232&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-graph图计算慢慢更新" class="anchor" aria-hidden="true" href="#graph图计算慢慢更新"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Graph图计算【慢慢更新】&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;数据集: &lt;a href="data/nlp/graph"&gt;data/nlp/graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;学习资料: spark graphX实战.pdf 【文件太大不方便提供，自己百度】&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-知识图谱" class="anchor" aria-hidden="true" href="#知识图谱"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;知识图谱&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;知识图谱，我只认 &lt;a href="https://www.zhihu.com/people/simmerchan" rel="nofollow"&gt;SimmerChan&lt;/a&gt;: &lt;a href="https://zhuanlan.zhihu.com/knowledgegraph" rel="nofollow"&gt;【知识图谱-给AI装个大脑】&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;说实话，我是看这博主老哥写的博客长大的，写的真的是深入浅出。我很喜欢，所以就分享给大家，希望你们也喜欢。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-进一步阅读" class="anchor" aria-hidden="true" href="#进一步阅读"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;进一步阅读&lt;/h3&gt;
&lt;p&gt;如果您希望更深入，本节提供了其他数据集列表。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#Text_data" rel="nofollow"&gt;维基百科研究中使用的文本数据集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/Datasets-What-are-the-major-text-corpora-used-by-computational-linguists-and-natural-language-processing-researchers-and-what-are-the-characteristics-biases-of-each-corpus" rel="nofollow"&gt;数据集：计算语言学家和自然语言处理研究人员使用的主要文本语料库是什么？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nlp.stanford.edu/links/statnlp.html#Corpora" rel="nofollow"&gt;斯坦福统计自然语言处理语料库&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/niderhoff/nlp-datasets"&gt;按字母顺序排列的NLP数据集列表&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nltk.org/nltk_data/" rel="nofollow"&gt;该机构NLTK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://deeplearning4j.org/opendata" rel="nofollow"&gt;在DL4J上打开深度学习数据&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/caesar0301/awesome-public-datasets#natural-language"&gt;NLP数据集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;国内开放数据集: &lt;a href="https://bosonnlp.com/dev/resource" rel="nofollow"&gt;https://bosonnlp.com/dev/resource&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-项目负责人" class="anchor" aria-hidden="true" href="#项目负责人"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目负责人&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第一期 (2017-02-27)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jiangzhonglian"&gt;@片刻&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangyangting"&gt;@那伊抹微笑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chenyyx"&gt;@瑶妹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/report/2017-04-08_%E7%AC%AC%E4%B8%80%E6%9C%9F%E7%9A%84%E6%80%BB%E7%BB%93.md"&gt;2017-04-08_第一期的总结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第二期 (2017-08-14)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jiangzhonglian"&gt;@片刻&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangyangting"&gt;@那伊抹微笑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chenyyx"&gt;@瑶妹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mikechengwei"&gt;@Mike&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第三期 (2018-04-16)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-项目贡献者" class="anchor" aria-hidden="true" href="#项目贡献者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;项目贡献者&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第一期 (2017-02-27)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/geekidentity"&gt;@侯法超&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hello19883"&gt;@hello19883&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sheepmen"&gt;@徐鑫&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/highfei2011"&gt;@ibe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第二期 (2017-08-14)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/LeeMoonCh"&gt;@Arithmetic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/caopeirui"&gt;@Veyron C&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Cugtyt"&gt;@Cugtyt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hey-bruce"&gt;@BBruceyuan&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第三期 (2018-04-16)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-群管理员换届" class="anchor" aria-hidden="true" href="#群管理员换届"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;群管理员换届&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/chenyyx"&gt;@瑶妹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wizardforcel"&gt;@飞龙&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jiangzhonglian"&gt;@片刻&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Watermelon233"&gt;@伪文艺.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangyangting"&gt;@那伊抹微笑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@LAMDA-健忘症&lt;/a&gt; 永久留任-非常感谢对群的贡献&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第一届 (2017-09-01)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;@易漠&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mikechengwei"&gt;@Mike&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@Books&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@李孟禹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@张假飞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@Glassy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@红色石头&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@微光同尘&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第二届 (2018-07-04)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;@张假飞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@李孟禹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@小明教主&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@平淡的天&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@凌少skierゞ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@じ☆νЁ坐看云起&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;古柳-DesertsX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;woodchuck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;自由精灵&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;楚盟&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;99杆清台&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;时空守望者@&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;只想发论文的渣渣&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;目标: ml劝退专家&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第三届 (2019-01-01)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;只会喊666的存在&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;codefun007.xyz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;荼靡&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;大鱼&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;青鸟&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;古柳-DesertsX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Edge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Alluka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;不发篇paper不改名片&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;FontTian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Bigjing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;仁 礼 智 爱&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;可啪的小乖受&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;老古董&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;时空守望者&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;我好菜啊&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Messi 19&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;萌Jay小公举&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml 第四届 (2019-06-01)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;佛学爱好者&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;楚盟&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;codefun007.xyz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;大鱼-群花-声优&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;大海&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Edge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;if only&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;李孟禹&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;平静&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;任务做不完&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;仁礼智爱&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;园时空守望者@&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;坐看云起&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;阿花君霸占路人&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;烦焖鸡&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;古柳-DesertsX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;青鸟(服务员)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;小明教主&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;zhiqing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;SrL.z&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;欢迎贡献者不断的追加&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-免责声明---只供学习参考" class="anchor" aria-hidden="true" href="#免责声明---只供学习参考"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;免责声明 - 【只供学习参考】&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ApacheCN 纯粹出于学习目的与个人兴趣翻译本书&lt;/li&gt;
&lt;li&gt;ApacheCN 保留对此版本译文的署名权及其它相关权利&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-协议" class="anchor" aria-hidden="true" href="#协议"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;协议&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;以各项目协议为准。&lt;/li&gt;
&lt;li&gt;ApacheCN 账号下没有协议的项目，一律视为 &lt;a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="nofollow"&gt;CC BY-NC-SA 4.0&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-资料来源" class="anchor" aria-hidden="true" href="#资料来源"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;资料来源:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;【比赛收集平台】: &lt;a href="https://github.com/iphysresearch/DataSciComp"&gt;https://github.com/iphysresearch/DataSciComp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pbharrin/machinelearninginaction"&gt;https://github.com/pbharrin/machinelearninginaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/datasets-natural-language-processing" rel="nofollow"&gt;https://machinelearningmastery.com/datasets-natural-language-processing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-感谢信" class="anchor" aria-hidden="true" href="#感谢信"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;感谢信&lt;/h2&gt;
&lt;p&gt;最近无意收到群友推送的链接，发现得到大佬高度的认可，并在热心的推广&lt;/p&gt;
&lt;p&gt;在此感谢:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.zhihu.com/org/liang-zi-wei-48" rel="nofollow"&gt;量子位&lt;/a&gt;: &lt;a href="https://www.zhihu.com/question/20472776/answer/691646493" rel="nofollow"&gt;https://www.zhihu.com/question/20472776/answer/691646493&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;人工智能前沿讲习: &lt;a href="https://mp.weixin.qq.com/s/f2dqulxOPkt7k5hqPsydyQ" rel="nofollow"&gt;https://mp.weixin.qq.com/s/f2dqulxOPkt7k5hqPsydyQ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-赞助我们" class="anchor" aria-hidden="true" href="#赞助我们"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;赞助我们&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2814efae28977e977f631af3a30acfe4e9089dd9/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f61626f75742f646f6e6174652e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/2814efae28977e977f631af3a30acfe4e9089dd9/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f61626f75742f646f6e6174652e6a7067" alt="微信&amp;amp;支付宝" data-canonical-src="http://data.apachecn.org/img/about/donate.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;特别赞助商(欢迎“私聊”赞助)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td align="center" valign="middle"&gt;
            &lt;a href="https://coding.net/?utm_source=ApacheCN&amp;amp;utm_medium=banner&amp;amp;utm_campaign=march2019" rel="nofollow"&gt;
              &lt;img width="1080" src="https://camo.githubusercontent.com/8d33a9d36a6822434ce78147cdb7cb41aba56a02/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f5370656369616c53706f6e736f72732f436f64696e674e65742e706e67" data-canonical-src="http://data.apachecn.org/img/SpecialSponsors/CodingNet.png" style="max-width:100%;"&gt;
            &lt;/a&gt;
          &lt;/td&gt;
      &lt;/tr&gt;&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>apachecn</author><guid isPermaLink="false">https://github.com/apachecn/AiLearning</guid><pubDate>Mon, 11 Nov 2019 00:11:00 GMT</pubDate></item><item><title>isocpp/CppCoreGuidelines #12 in Python, This month</title><link>https://github.com/isocpp/CppCoreGuidelines</link><description>&lt;p&gt;&lt;i&gt;The C++ Core Guidelines are a set of tried-and-true guidelines, rules, and best practices about coding in C++&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines" rel="nofollow"&gt;&lt;img src="cpp_core_guidelines_logo_text.png" alt="C++ Core Guidelines" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Within C++ is a smaller, simpler, safer language struggling to get out."
-- Bjarne Stroustrup&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;a href="CppCoreGuidelines.md"&gt;C++ Core Guidelines&lt;/a&gt; are a collaborative effort led by Bjarne Stroustrup, much like the C++ language itself. They are the result of many
person-years of discussion and design across a number of organizations. Their design encourages general applicability and broad adoption but
they can be freely copied and modified to meet your organization's needs.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-getting-started" class="anchor" aria-hidden="true" href="#getting-started"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Getting started&lt;/h2&gt;
&lt;p&gt;The guidelines themselves are found at &lt;a href="CppCoreGuidelines.md"&gt;CppCoreGuidelines&lt;/a&gt;. The document is in &lt;a href="https://github.github.com/gfm/"&gt;GH-flavored MarkDown&lt;/a&gt;. It is intentionally kept simple, mostly in ASCII, to allow automatic post-processing such as language translation and reformatting. The editors maintain one
&lt;a href="http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines" rel="nofollow"&gt;version formatted for browsing&lt;/a&gt;. Note that it is manually integrated and can be slightly older than the version in the master branch.&lt;/p&gt;
&lt;p&gt;The Guidelines are a constantly evolving document without a strict "release" cadence. Bjarne Stroustrup periodically reviews the document and increments the version number in the introduction. &lt;a href="https://github.com/isocpp/CppCoreGuidelines/releases"&gt;Checkins that increment the version number&lt;/a&gt; are tagged in git.&lt;/p&gt;
&lt;p&gt;Many of the guidelines make use of the header-only Guidelines Support Library. One implementation is available at &lt;a href="https://github.com/Microsoft/GSL"&gt;GSL: Guidelines Support Library&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-background-and-scope" class="anchor" aria-hidden="true" href="#background-and-scope"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Background and scope&lt;/h2&gt;
&lt;p&gt;The aim of the guidelines is to help people to use modern C++ effectively. By "modern C++" we mean C++11, C++14, and C++17. In other
words, what would you like your code to look like in 5 years' time, given that you can start now? In 10 years' time?&lt;/p&gt;
&lt;p&gt;The guidelines are focused on relatively higher-level issues, such as interfaces, resource management, memory management, and concurrency. Such
rules affect application architecture and library design. Following the rules will lead to code that is statically type-safe, has no resource
leaks, and catches many more programming logic errors than is common in code today. And it will run fast -- you can afford to do things right.&lt;/p&gt;
&lt;p&gt;We are less concerned with low-level issues, such as naming conventions and indentation style. However, no topic that can help a programmer is
out of bounds.&lt;/p&gt;
&lt;p&gt;Our initial set of rules emphasizes safety (of various forms) and simplicity. They may very well be too strict. We expect to have to introduce
more exceptions to better accommodate real-world needs. We also need more rules.&lt;/p&gt;
&lt;p&gt;You will find some of the rules contrary to your expectations or even contrary to your experience. If we haven't suggested that you change your
coding style in any way, we have failed! Please try to verify or disprove rules! In particular, we'd really like to have some of our rules
backed up with measurements or better examples.&lt;/p&gt;
&lt;p&gt;You will find some of the rules obvious or even trivial. Please remember that one purpose of a guideline is to help someone who is less
experienced or coming from a different background or language to get up to speed.&lt;/p&gt;
&lt;p&gt;The rules are designed to be supported by an analysis tool. Violations of rules will be flagged with references (or links) to the relevant rule.
We do not expect you to memorize all the rules before trying to write code.&lt;/p&gt;
&lt;p&gt;The rules are meant for gradual introduction into a code base. We plan to build tools for that and hope others will too.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributions-and-license" class="anchor" aria-hidden="true" href="#contributions-and-license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions and LICENSE&lt;/h2&gt;
&lt;p&gt;Comments and suggestions for improvements are most welcome. We plan to modify and extend this document as our understanding improves and the
language and the set of available libraries improve. More details are found at &lt;a href="./CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; and &lt;a href="./LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://www.digitalocean.com/?refcode=32f291566cf7&amp;amp;utm_campaign=Referral_Invite&amp;amp;utm_medium=Referral_Program&amp;amp;utm_source=CopyPaste" rel="nofollow"&gt;DigitalOcean&lt;/a&gt; for hosting the Standard C++ Foundation website.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>isocpp</author><guid isPermaLink="false">https://github.com/isocpp/CppCoreGuidelines</guid><pubDate>Mon, 11 Nov 2019 00:12:00 GMT</pubDate></item><item><title>home-assistant/home-assistant #13 in Python, This month</title><link>https://github.com/home-assistant/home-assistant</link><description>&lt;p&gt;&lt;i&gt;:house_with_garden: Open source home automation that puts local control and privacy first&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-home-assistant-" class="anchor" aria-hidden="true" href="#home-assistant-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Home Assistant &lt;a href="https://discord.gg/c5DvZ4e" rel="nofollow"&gt;&lt;img alt="Chat Status" src="https://camo.githubusercontent.com/599662b08725231a9f847723b9bc4f6dc4757d0c/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3333303934343233383931303936333731342e737667" data-canonical-src="https://img.shields.io/discord/330944238910963714.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Home Assistant is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control.&lt;/p&gt;
&lt;p&gt;To get started:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 -m pip install homeassistant
hass --open-ui&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Check out &lt;a href="https://home-assistant.io" rel="nofollow"&gt;home-assistant.io&lt;/a&gt; for &lt;a href="https://home-assistant.io/demo/" rel="nofollow"&gt;a
demo&lt;/a&gt;, &lt;a href="https://home-assistant.io/getting-started/" rel="nofollow"&gt;installation instructions&lt;/a&gt;,
&lt;a href="https://home-assistant.io/getting-started/automation-2/" rel="nofollow"&gt;tutorials&lt;/a&gt; and &lt;a href="https://home-assistant.io/docs/" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://home-assistant.io/demo/" rel="nofollow"&gt;&lt;img alt="screenshot-states" src="https://camo.githubusercontent.com/99578d7bca06d9c2973c2564e06f1ca444a4cce1/68747470733a2f2f7261772e6769746875622e636f6d2f686f6d652d617373697374616e742f686f6d652d617373697374616e742f6d61737465722f646f63732f73637265656e73686f74732e706e67" data-canonical-src="https://raw.github.com/home-assistant/home-assistant/master/docs/screenshots.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-featured-integrations"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-featured-integrations" class="anchor" aria-hidden="true" href="#featured-integrations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Featured integrations&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://home-assistant.io/integrations/" rel="nofollow"&gt;&lt;img alt="screenshot-components" src="https://camo.githubusercontent.com/ba21a6029ccb81d4a26b1ad9c198e61d01a07e7a/68747470733a2f2f7261772e6769746875622e636f6d2f686f6d652d617373697374616e742f686f6d652d617373697374616e742f6465762f646f63732f73637265656e73686f742d636f6d706f6e656e74732e706e67" data-canonical-src="https://raw.github.com/home-assistant/home-assistant/dev/docs/screenshot-components.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The system is built using a modular approach so support for other devices or actions can be implemented easily. See also the &lt;a href="https://developers.home-assistant.io/docs/en/architecture_index.html" rel="nofollow"&gt;section on architecture&lt;/a&gt; and the &lt;a href="https://developers.home-assistant.io/docs/en/creating_component_index.html" rel="nofollow"&gt;section on creating your own
components&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you run into issues while using Home Assistant or during development
of a component, check the &lt;a href="https://home-assistant.io/help/" rel="nofollow"&gt;Home Assistant help section&lt;/a&gt; of our website for further help and information.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>home-assistant</author><guid isPermaLink="false">https://github.com/home-assistant/home-assistant</guid><pubDate>Mon, 11 Nov 2019 00:13:00 GMT</pubDate></item><item><title>wangzheng0822/algo #14 in Python, This month</title><link>https://github.com/wangzheng0822/algo</link><description>&lt;p&gt;&lt;i&gt;数据结构和算法必知必会的50个代码实现&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-数据结构和算法必知必会的50个代码实现" class="anchor" aria-hidden="true" href="#数据结构和算法必知必会的50个代码实现"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数据结构和算法必知必会的50个代码实现&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-微信搜索我的公众号小争哥或者微信扫描下面二维码-获取更多压箱底的干货分享" class="anchor" aria-hidden="true" href="#微信搜索我的公众号小争哥或者微信扫描下面二维码-获取更多压箱底的干货分享"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;微信搜索我的公众号“小争哥”，或者微信扫描下面二维码, 获取更多压箱底的干货分享&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-前google工程师5万人跟着学的数据结构和算法之美专栏作者" class="anchor" aria-hidden="true" href="#前google工程师5万人跟着学的数据结构和算法之美专栏作者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;前Google工程师，5万+人跟着学的《数据结构和算法之美》专栏作者&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/wangzheng0822/markdownphotos/blob/master/pics/qrcode_for_gh_9b0e7afdff20_258.jpg"&gt;&lt;img src="https://github.com/wangzheng0822/markdownphotos/raw/master/pics/qrcode_for_gh_9b0e7afdff20_258.jpg" alt="t2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-数组" class="anchor" aria-hidden="true" href="#数组"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;数组&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现一个支持动态扩容的数组&lt;/li&gt;
&lt;li&gt;实现一个大小固定的有序数组，支持动态增删改操作&lt;/li&gt;
&lt;li&gt;实现两个有序数组合并为一个有序数组&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-链表" class="anchor" aria-hidden="true" href="#链表"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;链表&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现单链表、循环链表、双向链表，支持增删操作&lt;/li&gt;
&lt;li&gt;实现单链表反转&lt;/li&gt;
&lt;li&gt;实现两个有序的链表合并为一个有序链表&lt;/li&gt;
&lt;li&gt;实现求链表的中间结点&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-栈" class="anchor" aria-hidden="true" href="#栈"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;栈&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;用数组实现一个顺序栈&lt;/li&gt;
&lt;li&gt;用链表实现一个链式栈&lt;/li&gt;
&lt;li&gt;编程模拟实现一个浏览器的前进、后退功能&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-队列" class="anchor" aria-hidden="true" href="#队列"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;队列&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;用数组实现一个顺序队列&lt;/li&gt;
&lt;li&gt;用链表实现一个链式队列&lt;/li&gt;
&lt;li&gt;实现一个循环队列&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-递归" class="anchor" aria-hidden="true" href="#递归"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;递归&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;编程实现斐波那契数列求值f(n)=f(n-1)+f(n-2)&lt;/li&gt;
&lt;li&gt;编程实现求阶乘n!&lt;/li&gt;
&lt;li&gt;编程实现一组数据集合的全排列&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-排序" class="anchor" aria-hidden="true" href="#排序"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;排序&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现归并排序、快速排序、插入排序、冒泡排序、选择排序&lt;/li&gt;
&lt;li&gt;编程实现O(n)时间复杂度内找到一组数据的第K大元素&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-二分查找" class="anchor" aria-hidden="true" href="#二分查找"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;二分查找&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现一个有序数组的二分查找算法&lt;/li&gt;
&lt;li&gt;实现模糊二分查找算法（比如大于等于给定值的第一个元素）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-散列表" class="anchor" aria-hidden="true" href="#散列表"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;散列表&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现一个基于链表法解决冲突问题的散列表&lt;/li&gt;
&lt;li&gt;实现一个LRU缓存淘汰算法&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-字符串" class="anchor" aria-hidden="true" href="#字符串"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;字符串&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现一个字符集，只包含a～z这26个英文字母的Trie树&lt;/li&gt;
&lt;li&gt;实现朴素的字符串匹配算法&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-二叉树" class="anchor" aria-hidden="true" href="#二叉树"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;二叉树&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现一个二叉查找树，并且支持插入、删除、查找操作&lt;/li&gt;
&lt;li&gt;实现查找二叉查找树中某个节点的后继、前驱节点&lt;/li&gt;
&lt;li&gt;实现二叉树前、中、后序以及按层遍历&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-堆" class="anchor" aria-hidden="true" href="#堆"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;堆&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现一个小顶堆、大顶堆、优先级队列&lt;/li&gt;
&lt;li&gt;实现堆排序&lt;/li&gt;
&lt;li&gt;利用优先级队列合并K个有序数组&lt;/li&gt;
&lt;li&gt;求一组动态数据集合的最大Top K&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-图" class="anchor" aria-hidden="true" href="#图"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;图&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实现有向图、无向图、有权图、无权图的邻接矩阵和邻接表表示方法&lt;/li&gt;
&lt;li&gt;实现图的深度优先搜索、广度优先搜索&lt;/li&gt;
&lt;li&gt;实现Dijkstra算法、A*算法&lt;/li&gt;
&lt;li&gt;实现拓扑排序的Kahn算法、DFS算法&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-回溯" class="anchor" aria-hidden="true" href="#回溯"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;回溯&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;利用回溯算法求解八皇后问题&lt;/li&gt;
&lt;li&gt;利用回溯算法求解0-1背包问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-分治" class="anchor" aria-hidden="true" href="#分治"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;分治&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;利用分治算法求一组数据的逆序对个数&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-动态规划" class="anchor" aria-hidden="true" href="#动态规划"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;动态规划&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;0-1背包问题&lt;/li&gt;
&lt;li&gt;最小路径和&lt;/li&gt;
&lt;li&gt;编程实现莱文斯坦最短编辑距离&lt;/li&gt;
&lt;li&gt;编程实现查找两个字符串的最长公共子序列&lt;/li&gt;
&lt;li&gt;编程实现一个数据序列的最长递增子序列&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>wangzheng0822</author><guid isPermaLink="false">https://github.com/wangzheng0822/algo</guid><pubDate>Mon, 11 Nov 2019 00:14:00 GMT</pubDate></item><item><title>521xueweihan/HelloGitHub #15 in Python, This month</title><link>https://github.com/521xueweihan/HelloGitHub</link><description>&lt;p&gt;&lt;i&gt;:octocat: Find pearls on open-source seashore 分享 GitHub 上有趣、入门级的开源项目&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/readme.gif"&gt;&lt;img src="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/readme.gif" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;br&gt;中文 | &lt;a href="README_en.md"&gt;English&lt;/a&gt;
  &lt;br&gt;&lt;strong&gt;HelloGitHub&lt;/strong&gt; 一个分享 GitHub 上有趣、入门级的开源项目。&lt;br&gt;兴趣是最好的老师，这里能够帮你找到编程的兴趣！
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://hellogithub.com/weixin.png" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/61343b85520a4714ddb37eb300f8268cc881ae7e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f54616c6b2d2545352542452541452545342542462541312545372542452541342d627269676874677265656e2e7376673f7374796c653d706f706f75742d737175617265" alt="WeiXin" data-canonical-src="https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://github.com/521xueweihan/HelloGitHub/stargazers"&gt;&lt;img src="https://camo.githubusercontent.com/0aec7fa1a5647255bbe8af37a82a007be69d8739/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f35323178756577656968616e2f48656c6c6f4769744875622e7376673f7374796c653d706f706f75742d737175617265" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://github.com/521xueweihan/HelloGitHub/issues"&gt;&lt;img src="https://camo.githubusercontent.com/a8367e38e94eccf7e469023edfec05db15132454/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f35323178756577656968616e2f48656c6c6f4769744875622e7376673f7374796c653d706f706f75742d737175617265" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://weibo.com/hellogithub" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4627590b5d81a690c6c83abaf47f678d70d26e6b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2545362539362542302545362542352541412d576569626f2d7265642e7376673f7374796c653d706f706f75742d737175617265" alt="Sina Weibo" data-canonical-src="https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-简介" class="anchor" aria-hidden="true" href="#简介"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;简介&lt;/h2&gt;
&lt;p&gt;这是一个面向编程新手、热爱编程、对开源社区感兴趣人群的项目，内容&lt;strong&gt;每月 28 号&lt;/strong&gt;以月刊的形式更新发布。内容包括：&lt;strong&gt;流行项目&lt;/strong&gt;、&lt;strong&gt;入门级项目&lt;/strong&gt;、&lt;strong&gt;让生活变得更美好的工具&lt;/strong&gt;、&lt;strong&gt;书籍&lt;/strong&gt;、&lt;strong&gt;学习心得笔记&lt;/strong&gt;、&lt;strong&gt;企业级项目&lt;/strong&gt;等，这些开源项目大多都是非常容易上手、很 Cool，能够让你用很短时间感受到编程的魅力和便捷。从而让大家感受到编程的乐趣，动手开始编程。&lt;/p&gt;
&lt;p&gt;希望通过本项目能够有更多人加入到开源社区、回馈社区。&lt;strong&gt;让有趣、有价值的项目被更多人发现和加入&lt;/strong&gt;。在参与这些项目的过程中，你将得到：&lt;strong&gt;热爱编程的小伙伴&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="man_dancing" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f57a.png"&gt;🕺&lt;/g-emoji&gt; 、&lt;strong&gt;更多编程知识&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png"&gt;📚&lt;/g-emoji&gt; 、&lt;strong&gt;优秀的编程技巧&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;💻&lt;/g-emoji&gt; 、&lt;strong&gt;找到编程的乐趣&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="video_game" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ae.png"&gt;🎮&lt;/g-emoji&gt; 。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;『每日精选』&lt;/strong&gt; 关注我们的&lt;a href="https://weibo.com/hellogithub" rel="nofollow"&gt;最惨官微&lt;/a&gt;获取最新项目推荐。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;『讲解开源项目』&lt;/strong&gt; 欢迎开源爱好者给我们投稿&lt;a href="https://github.com/HelloGitHub-Team/Article/blob/master/%E5%88%9B%E4%BD%9C%E9%A1%BB%E7%9F%A5.md"&gt;查看创作须知&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-内容" class="anchor" aria-hidden="true" href="#内容"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;内容&lt;/h2&gt;
&lt;p&gt;每月 28 号发布&lt;a href="/content/last.md"&gt;最新一期&lt;/a&gt; | &lt;a href="https://hellogithub.com" rel="nofollow"&gt;官网&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img class="emoji" title=":shipit:" alt=":shipit:" src="https://github.githubassets.com/images/icons/emoji/shipit.png" height="20" width="20" align="absmiddle"&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="jack_o_lantern" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f383.png"&gt;🎃&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="beer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f37a.png"&gt;🍺&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="fish_cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f365.png"&gt;🍥&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;img class="emoji" title=":octocat:" alt=":octocat:" src="https://github.githubassets.com/images/icons/emoji/octocat.png" height="20" width="20" align="absmiddle"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/43/HelloGitHub43.md"&gt;第 43 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/42/HelloGitHub42.md"&gt;第 42 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/41/HelloGitHub41.md"&gt;第 41 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/40/HelloGitHub40.md"&gt;第 40 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/39/HelloGitHub39.md"&gt;第 39 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/38/HelloGitHub38.md"&gt;第 38 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/37/HelloGitHub37.md"&gt;第 37 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/36/HelloGitHub36.md"&gt;第 36 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/35/HelloGitHub35.md"&gt;第 35 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/34/HelloGitHub34.md"&gt;第 34 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/33/HelloGitHub33.md"&gt;第 33 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/32/HelloGitHub32.md"&gt;第 32 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/31/HelloGitHub31.md"&gt;第 31 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/30/HelloGitHub30.md"&gt;第 30 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/29/HelloGitHub29.md"&gt;第 29 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/28/HelloGitHub28.md"&gt;第 28 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/27/HelloGitHub27.md"&gt;第 27 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/26/HelloGitHub26.md"&gt;第 26 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/25/HelloGitHub25.md"&gt;第 25 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/24/HelloGitHub24.md"&gt;第 24 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/23/HelloGitHub23.md"&gt;第 23 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/22/HelloGitHub22.md"&gt;第 22 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/21/HelloGitHub21.md"&gt;第 21 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/20/HelloGitHub20.md"&gt;第 20 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/19/HelloGitHub19.md"&gt;第 19 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/18/HelloGitHub18.md"&gt;第 18 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/17/HelloGitHub17.md"&gt;第 17 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/16/HelloGitHub16.md"&gt;第 16 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/15/HelloGitHub15.md"&gt;第 15 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/14/HelloGitHub14.md"&gt;第 14 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/13/HelloGitHub13.md"&gt;第 13 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/12/HelloGitHub12.md"&gt;第 12 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/11/HelloGitHub11.md"&gt;第 11 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/10/HelloGitHub10.md"&gt;第 10 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/09/HelloGitHub09.md"&gt;第 09 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/08/HelloGitHub08.md"&gt;第 08 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/07/HelloGitHub07.md"&gt;第 07 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/06/HelloGitHub06.md"&gt;第 06 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/05/HelloGitHub05.md"&gt;第 05 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/04/HelloGitHub04.md"&gt;第 04 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/03/HelloGitHub03.md"&gt;第 03 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/02/HelloGitHub02.md"&gt;第 02 期&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/01/HelloGitHub01.md"&gt;第 01 期&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;欢迎&lt;a href="https://github.com/521xueweihan/HelloGitHub/issues/new"&gt;推荐或自荐项目&lt;/a&gt;成为 &lt;strong&gt;HelloGitHub&lt;/strong&gt; 的&lt;a href="https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md"&gt;贡献者&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-贡献者" class="anchor" aria-hidden="true" href="#贡献者"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;贡献者&lt;/h2&gt;
&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/521xueweihan"&gt;
          &lt;img src="https://avatars2.githubusercontent.com/u/8255800?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;削微寒&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/ming995"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/46031112?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;糖醋里脊&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/FrontMage"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/17007026?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;FrontMage&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/xibinyue"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/14122146?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;xibinyue&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/Eurus-Holmes"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/34226570?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Feiyang Chen&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/ChungZH"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/42088872?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;ChungZH&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/daixiang0"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/26538619?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;daixiang0&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/nivance"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/3291404?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;nivance&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/hellowHuaairen"&gt;
          &lt;img src="https://avatars2.githubusercontent.com/u/19610305?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;hellowHuaairen&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/17665302?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;更多贡献者&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-合作组织" class="anchor" aria-hidden="true" href="#合作组织"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;合作组织&lt;/h2&gt;
&lt;p&gt;欢迎各种&lt;img class="emoji" title=":octocat:" alt=":octocat:" src="https://github.githubassets.com/images/icons/emoji/octocat.png" height="20" width="20" align="absmiddle"&gt;开源组织合作&lt;a href="Mailto:595666367@qq.com"&gt;点击联系我&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/FGDBTKD"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/40509403?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;FGDBTKD&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;AI/ML/DL/NLP&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/d2-projects"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/40857578?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;D2 Projects&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Vue/JavaScript&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/doocs"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/43716716?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Doocs&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Technical Knowledge&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-声明" class="anchor" aria-hidden="true" href="#声明"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;声明&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="nofollow"&gt;&lt;img alt="知识共享许可协议" src="https://camo.githubusercontent.com/1ae74a56e22c4897b6fbfb9f301bd829c77429a7/68747470733a2f2f6c6963656e7365627574746f6e732e6e65742f6c2f62792d6e632d6e642f342e302f38387833312e706e67" data-canonical-src="https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;本作品采用 &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="nofollow"&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 进行许可。&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>521xueweihan</author><guid isPermaLink="false">https://github.com/521xueweihan/HelloGitHub</guid><pubDate>Mon, 11 Nov 2019 00:15:00 GMT</pubDate></item><item><title>dbolya/yolact #16 in Python, This month</title><link>https://github.com/dbolya/yolact</link><description>&lt;p&gt;&lt;i&gt;A simple, fully convolutional model for real-time instance segmentation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-you-only-look-at-coefficients" class="anchor" aria-hidden="true" href="#you-only-look-at-coefficients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Y&lt;/strong&gt;ou &lt;strong&gt;O&lt;/strong&gt;nly &lt;strong&gt;L&lt;/strong&gt;ook &lt;strong&gt;A&lt;/strong&gt;t &lt;strong&gt;C&lt;/strong&gt;oefficien&lt;strong&gt;T&lt;/strong&gt;s&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;    ██╗   ██╗ ██████╗ ██╗      █████╗  ██████╗████████╗
    ╚██╗ ██╔╝██╔═══██╗██║     ██╔══██╗██╔════╝╚══██╔══╝
     ╚████╔╝ ██║   ██║██║     ███████║██║        ██║   
      ╚██╔╝  ██║   ██║██║     ██╔══██║██║        ██║   
       ██║   ╚██████╔╝███████╗██║  ██║╚██████╗   ██║   
       ╚═╝    ╚═════╝ ╚══════╝╚═╝  ╚═╝ ╚═════╝   ╚═╝ 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A simple, fully convolutional model for real-time instance segmentation. This is the code for &lt;a href="https://arxiv.org/abs/1904.02689" rel="nofollow"&gt;our paper&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-iccv-update-v11-released-check-out-the-iccv-trailer-here" class="anchor" aria-hidden="true" href="#iccv-update-v11-released-check-out-the-iccv-trailer-here"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ICCV update (v1.1) released! Check out the ICCV trailer here:&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=0pMfmo8qfpQ" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c9f0f1403e25276c0beea78732b5cec6c9b610ab/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f30704d666d6f38716670512f302e6a7067" alt="IMAGE ALT TEXT HERE" data-canonical-src="https://img.youtube.com/vi/0pMfmo8qfpQ/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Read &lt;a href="CHANGELOG.md"&gt;the changelog&lt;/a&gt; for details on, well, what changed. Oh, and the paper got updated too with pascal results and an appendix with box mAP.&lt;/p&gt;
&lt;p&gt;Some examples from our base model (33.5 fps on a Titan Xp and 29.8 mAP on COCO's &lt;code&gt;test-dev&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_0.png"&gt;&lt;img src="data/yolact_example_0.png" alt="Example 0" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_1.png"&gt;&lt;img src="data/yolact_example_1.png" alt="Example 1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_2.png"&gt;&lt;img src="data/yolact_example_2.png" alt="Example 2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Set up a Python3 environment.&lt;/li&gt;
&lt;li&gt;Install &lt;a href="http://pytorch.org/" rel="nofollow"&gt;Pytorch&lt;/a&gt; 1.0.1 (or higher) and TorchVision.&lt;/li&gt;
&lt;li&gt;Install some other packages:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Cython needs to be installed before pycocotools&lt;/span&gt;
pip install cython
pip install opencv-python pillow pycocotools matplotlib &lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Clone this repository and enter it:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/dbolya/yolact.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; yolact&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you'd like to train YOLACT, download the COCO dataset and the 2014/2017 annotations. Note that this script will take a while and dump 21gb of files into &lt;code&gt;./data/coco&lt;/code&gt;.
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh data/scripts/COCO.sh&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you'd like to evaluate YOLACT on &lt;code&gt;test-dev&lt;/code&gt;, download &lt;code&gt;test-dev&lt;/code&gt; with this script.
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh data/scripts/COCO_test.sh&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-evaluation" class="anchor" aria-hidden="true" href="#evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evaluation&lt;/h1&gt;
&lt;p&gt;As of April 5th, 2019 here are our latest models along with their FPS on a Titan Xp and mAP on &lt;code&gt;test-dev&lt;/code&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Image Size&lt;/th&gt;
&lt;th align="center"&gt;Backbone&lt;/th&gt;
&lt;th align="center"&gt;FPS&lt;/th&gt;
&lt;th align="center"&gt;mAP&lt;/th&gt;
&lt;th&gt;Weights&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Resnet50-FPN&lt;/td&gt;
&lt;td align="center"&gt;42.5&lt;/td&gt;
&lt;td align="center"&gt;28.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1yp7ZbbDwvMiFJEq4ptVKTYTI2VeRDXl0/view?usp=sharing" rel="nofollow"&gt;yolact_resnet50_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EUVpxoSXaqNIlssoLKOEoCcB1m0RpzGq_Khp5n1VX3zcUw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Darknet53-FPN&lt;/td&gt;
&lt;td align="center"&gt;40.0&lt;/td&gt;
&lt;td align="center"&gt;28.7&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1dukLrTzZQEuhzitGkHaGjphlmRJOjVnP/view?usp=sharing" rel="nofollow"&gt;yolact_darknet53_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/ERrao26c8llJn25dIyZPhwMBxUp2GdZTKIMUQA3t0djHLw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Resnet101-FPN&lt;/td&gt;
&lt;td align="center"&gt;33.0&lt;/td&gt;
&lt;td align="center"&gt;29.8&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1UYy3dMapbH1BnmtZU4WH1zbYgOzzHHf_/view?usp=sharing" rel="nofollow"&gt;yolact_base_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EYRWxBEoKU9DiblrWx2M89MBGFkVVB_drlRd_v5sdT3Hgg" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;700&lt;/td&gt;
&lt;td align="center"&gt;Resnet101-FPN&lt;/td&gt;
&lt;td align="center"&gt;23.6&lt;/td&gt;
&lt;td align="center"&gt;31.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1lE4Lz5p25teiXV-6HdTiOJSnS7u7GBzg/view?usp=sharing" rel="nofollow"&gt;yolact_im700_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/Eagg5RSc5hFEhp7sPtvLNyoBjhlf2feog7t8OQzHKKphjw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To evalute the model, put the corresponding weights file in the &lt;code&gt;./weights&lt;/code&gt; directory and run one of the following commands.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quantitative-results-on-coco" class="anchor" aria-hidden="true" href="#quantitative-results-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quantitative Results on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Quantitatively evaluate a trained model on the entire validation set. Make sure you have COCO downloaded as above.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This should get 29.92 validation mask mAP last time I checked.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Output a COCOEval json to submit to the website or to use the run_coco_eval.py script.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This command will create './results/bbox_detections.json' and './results/mask_detections.json' for detection and instance segmentation respectively.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; You can run COCOEval on the files created in the previous command. The performance should match my implementation in eval.py.&lt;/span&gt;
python run_coco_eval.py

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To output a coco json file for test-dev, make sure you have test-dev downloaded from above and go&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json --dataset=coco2017_testdev_dataset&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-qualitative-results-on-coco" class="anchor" aria-hidden="true" href="#qualitative-results-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Qualitative Results on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display qualitative results on COCO. From here on I'll use a confidence threshold of 0.15.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --display&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-benchmarking-on-coco" class="anchor" aria-hidden="true" href="#benchmarking-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmarking on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run just the raw model on the first 1k images of the validation set&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --benchmark --max_images=1000&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-images" class="anchor" aria-hidden="true" href="#images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Images&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display qualitative results on the specified image.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=my_image.png

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process an image and save it to another file.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=input_image.png:output_image.png

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process a whole folder of images.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --images=path/to/input/folder:path/to/output/folder&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-video" class="anchor" aria-hidden="true" href="#video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display a video in real-time. "--video_multiframe" will process that many frames at once for improved performance.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; If you want, use "--display_fps" to draw the FPS directly on the frame.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=my_video.mp4

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display a webcam feed in real-time. If you have multiple webcams pass the index of the webcam you want instead of 0.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=0

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process a video and save it to another file. This uses the same pipeline as the ones above now, so it's fast!&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=input_video.mp4:output_video.mp4&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can tell, &lt;code&gt;eval.py&lt;/code&gt; can do a ton of stuff. Run the &lt;code&gt;--help&lt;/code&gt; command to see everything it can do.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python eval.py --help&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h1&gt;
&lt;p&gt;By default, we train on COCO. Make sure to download the entire dataset using the commands above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To train, grab an imagenet-pretrained model and put it in &lt;code&gt;./weights&lt;/code&gt;.
&lt;ul&gt;
&lt;li&gt;For Resnet101, download &lt;code&gt;resnet101_reducedfc.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/1tvqFPd4bJtakOlmn-uIA492g2qurRChj/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Resnet50, download &lt;code&gt;resnet50-19c8e357.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/1Jy3yCdbatgXa5YYIdTCRrSV0S9V5g1rn/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Darknet53, download &lt;code&gt;darknet53.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/17Y431j4sagFpSReuPNoFcj9h7azDTZFf/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run one of the training commands below.
&lt;ul&gt;
&lt;li&gt;Note that you can press ctrl+c while training and it will save an &lt;code&gt;*_interrupt.pth&lt;/code&gt; file at the current iteration.&lt;/li&gt;
&lt;li&gt;All weights are saved in the &lt;code&gt;./weights&lt;/code&gt; directory by default with the file name &lt;code&gt;&amp;lt;config&amp;gt;_&amp;lt;epoch&amp;gt;_&amp;lt;iter&amp;gt;.pth&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trains using the base config with a batch size of 8 (the default).&lt;/span&gt;
python train.py --config=yolact_base_config

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trains yolact_base_config with a batch_size of 5. For the 550px models, 1 batch takes up around 1.5 gigs of VRAM, so specify accordingly.&lt;/span&gt;
python train.py --config=yolact_base_config --batch_size=5

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Resume training yolact_base with a specific weight file and start from the iteration specified in the weight file's name.&lt;/span&gt;
python train.py --config=yolact_base_config --resume=weights/yolact_base_10_32100.pth --start_iter=-1

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Use the help option to see a description of all available command line arguments&lt;/span&gt;
python train.py --help&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-multi-gpu-support" class="anchor" aria-hidden="true" href="#multi-gpu-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-GPU Support&lt;/h2&gt;
&lt;p&gt;YOLACT now supports multiple GPUs seamlessly during training:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before running any of the scripts, run: &lt;code&gt;export CUDA_VISIBLE_DEVICES=[gpus]&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Where you should replace [gpus] with a comma separated list of the index of each GPU you want to use (e.g., 0,1,2,3).&lt;/li&gt;
&lt;li&gt;You should still do this if only using 1 GPU.&lt;/li&gt;
&lt;li&gt;You can check the indices of your GPUs with &lt;code&gt;nvidia-smi&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Then, simply set the batch size to &lt;code&gt;8*num_gpus&lt;/code&gt; with the training commands above. The training script will automatically scale the hyperparameters to the right values.
&lt;ul&gt;
&lt;li&gt;If you have memory to spare you can increase the batch size further, but keep it a multiple of the number of GPUs you're using.&lt;/li&gt;
&lt;li&gt;If you want to allocate the images per GPU specific for different GPUs, you can use &lt;code&gt;--batch_alloc=[alloc]&lt;/code&gt; where [alloc] is a comma seprated list containing the number of images on each GPU. This must sum to &lt;code&gt;batch_size&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-logging" class="anchor" aria-hidden="true" href="#logging"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Logging&lt;/h2&gt;
&lt;p&gt;YOLACT now logs training and validation information by default. You can disable this with &lt;code&gt;--no_log&lt;/code&gt;. A guide on how to visualize these logs is coming soon, but now you can look at &lt;code&gt;LogVizualizer&lt;/code&gt; in &lt;code&gt;utils/logger.py&lt;/code&gt; for help.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pascal-sbd" class="anchor" aria-hidden="true" href="#pascal-sbd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pascal SBD&lt;/h2&gt;
&lt;p&gt;We also include a config for training on Pascal SBD annotations (for rapid experimentation or comparing with other methods). To train on Pascal SBD, proceed with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download the dataset from &lt;a href="http://home.bharathh.info/pubs/codes/SBD/download.html" rel="nofollow"&gt;here&lt;/a&gt;. It's the first link in the top "Overview" section (and the file is called &lt;code&gt;benchmark.tgz&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Extract the dataset somewhere. In the dataset there should be a folder called &lt;code&gt;dataset/img&lt;/code&gt;. Create the directory &lt;code&gt;./data/sbd&lt;/code&gt; (where &lt;code&gt;.&lt;/code&gt; is YOLACT's root) and copy &lt;code&gt;dataset/img&lt;/code&gt; to &lt;code&gt;./data/sbd/img&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Download the COCO-style annotations from &lt;a href="https://drive.google.com/open?id=1ExrRSPVctHW8Nxrn0SofU1lVhK5Wn0_S" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Extract the annotations into &lt;code&gt;./data/sbd/&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Now you can train using &lt;code&gt;--config=yolact_resnet50_pascal_config&lt;/code&gt;. Check that config to see how to extend it to other models.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will automate this all with a script soon, don't worry. Also, if you want the script I used to convert the annotations, I put it in &lt;code&gt;./scripts/convert_sbd.py&lt;/code&gt;, but you'll have to check how it works to be able to use it because I don't actually remember at this point.&lt;/p&gt;
&lt;p&gt;If you want to verify our results, you can download our &lt;code&gt;yolact_resnet50_pascal_config&lt;/code&gt; weights from &lt;a href="https://drive.google.com/open?id=1yLVwtkRtNxyl0kxeMCtPXJsXFFyc_FHe" rel="nofollow"&gt;here&lt;/a&gt;. This model should get 72.3 mask AP_50 and 56.2 mask AP_70. Note that the "all" AP isn't the same as the "vol" AP reported in others papers for pascal (they use an averages of the thresholds from &lt;code&gt;0.1 - 0.9&lt;/code&gt; in increments of &lt;code&gt;0.1&lt;/code&gt; instead of what COCO uses).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-custom-datasets" class="anchor" aria-hidden="true" href="#custom-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h2&gt;
&lt;p&gt;You can also train on your own dataset by following these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a COCO-style Object Detection JSON annotation file for your dataset. The specification for this can be found &lt;a href="http://cocodataset.org/#format-data" rel="nofollow"&gt;here&lt;/a&gt;. Note that we don't use some fields, so the following may be omitted:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;info&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;liscense&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Under &lt;code&gt;image&lt;/code&gt;: &lt;code&gt;license, flickr_url, coco_url, date_captured&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;categories&lt;/code&gt; (we use our own format for categories, see below)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create a definition for your dataset under &lt;code&gt;dataset_base&lt;/code&gt; in &lt;code&gt;data/config.py&lt;/code&gt; (see the comments in &lt;code&gt;dataset_base&lt;/code&gt; for an explanation of each field):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;my_custom_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; dataset_base.copy({
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;name&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;My Dataset&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_training_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_info&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:   &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_training_annotation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_validation_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid_info&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:   &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_validation_annotation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;has_gt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;True&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;class_names&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_1&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_2&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_3&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;...&lt;/span&gt;)
})&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;A couple things to note:
&lt;ul&gt;
&lt;li&gt;Class IDs in the annotation file should start at 1 and increase sequentially on the order of &lt;code&gt;class_names&lt;/code&gt;. If this isn't the case for your annotation file (like in COCO), see the field &lt;code&gt;label_map&lt;/code&gt; in &lt;code&gt;dataset_base&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you do not want to create a validation split, use the same image path and annotations file for validation. By default (see &lt;code&gt;python train.py --help&lt;/code&gt;), &lt;code&gt;train.py&lt;/code&gt; will output validation mAP for the first 5000 images in the dataset every 2 epochs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Finally, in &lt;code&gt;yolact_base_config&lt;/code&gt; in the same file, change the value for &lt;code&gt;'dataset'&lt;/code&gt; to &lt;code&gt;'my_custom_dataset'&lt;/code&gt; or whatever you named the config object above. Then you can use any of the training commands in the previous section.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-creating-a-custom-dataset-from-scratch" class="anchor" aria-hidden="true" href="#creating-a-custom-dataset-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Creating a Custom Dataset from Scratch&lt;/h4&gt;
&lt;p&gt;See &lt;a href="https://github.com/dbolya/yolact/issues/70#issuecomment-504283008"&gt;this nice post by @Amit12690&lt;/a&gt; for tips on how to annotate a custom dataset and prepare it for use with YOLACT.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;p&gt;If you use YOLACT or this code base in your work, please cite&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{bolya-iccv2019,
  author    = {Daniel Bolya and Chong Zhou and Fanyi Xiao and Yong Jae Lee},
  title     = {YOLACT: {Real-time} Instance Segmentation},
  booktitle = {ICCV},
  year      = {2019},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h1&gt;
&lt;p&gt;For questions about our paper or code, please contact &lt;a href="mailto:dbolya@ucdavis.edu"&gt;Daniel Bolya&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dbolya</author><guid isPermaLink="false">https://github.com/dbolya/yolact</guid><pubDate>Mon, 11 Nov 2019 00:16:00 GMT</pubDate></item><item><title>iGhibli/iOS-DeviceSupport #17 in Python, This month</title><link>https://github.com/iGhibli/iOS-DeviceSupport</link><description>&lt;p&gt;&lt;i&gt;This repository holds the device support files for the iOS, and I will update it regularly.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ios-devicesupport" class="anchor" aria-hidden="true" href="#ios-devicesupport"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;iOS-DeviceSupport&lt;/h1&gt;
&lt;p&gt;This repository holds the device support files for the iOS, and I will update it regularly.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;See docs: &lt;a href="https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/" rel="nofollow"&gt;https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Below command will try to unzip all new device support files to &lt;code&gt;/Applications/Xcode.app&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo ./deploy.py&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can use &lt;code&gt;-t&lt;/code&gt; if your Xcode is not in &lt;code&gt;/Applications/&lt;/code&gt; or has different name.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo ./deploy.py -t /Applications/Xcode&lt;span class="pl-cce"&gt;\ &lt;/span&gt;9.app&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;./deploy.py -h
usage: deploy.py [-h] [-t TARGET]

optional arguments:
  -h, --help  show this &lt;span class="pl-c1"&gt;help&lt;/span&gt; message and &lt;span class="pl-c1"&gt;exit&lt;/span&gt;
  -t TARGET   The path &lt;span class="pl-k"&gt;for&lt;/span&gt; Xcode&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-supported-versions" class="anchor" aria-hidden="true" href="#supported-versions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supported versions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;iOS8
&lt;ul&gt;
&lt;li&gt;8.0 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.1 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.2 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.3 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.4 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS9
&lt;ul&gt;
&lt;li&gt;9.0 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.1 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.2 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.3 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS10
&lt;ul&gt;
&lt;li&gt;10.0 (14A345) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.0 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.1 (14B72) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.1 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.2 (14C92) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.2 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.3 (14E269) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.3 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS11
&lt;ul&gt;
&lt;li&gt;11.0 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.1 (15B87) &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.1 &lt;code&gt;2017/12/11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.2 (15C107) &lt;code&gt;2017/12/11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.2 &lt;code&gt;2018/03/06&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 (15E5167d) &lt;code&gt;2018/01/30&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 (15E5201e) &lt;code&gt;2018/03/06&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 &lt;code&gt;2018/04/09&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F5037c) &lt;code&gt;2018/04/09&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F5061c) &lt;code&gt;2018/07/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F79) &lt;code&gt;2018/07/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 &lt;code&gt;2018/06/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS12
&lt;ul&gt;
&lt;li&gt;12.0 (16A5288q) &lt;code&gt;2018/06/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5308d) &lt;code&gt;2018/06/19&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5318d) &lt;code&gt;2018/06/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5327d) &lt;code&gt;2018/07/20&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5339e) &lt;code&gt;2018/07/31&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5354b) &lt;code&gt;2018/08/15&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A366) &lt;code&gt;2018/09/18&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5059d) &lt;code&gt;2018/09/21&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5068g) &lt;code&gt;2018/10/08&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5084a) &lt;code&gt;2018/10/16&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B91) &lt;code&gt;2018/10/31&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5084a) &lt;code&gt;2018/10/16&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E5181e) &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E5212e) &lt;code&gt;2019/03/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E226) &lt;code&gt;2019/03/27&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.3 &lt;code&gt;2019/06/04&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.4 (16G73) &lt;code&gt;2019/07/22&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.4 (FromXcode_11_Beta_7_xip) &lt;code&gt;2019/09/03&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS13
&lt;ul&gt;
&lt;li&gt;13.0 &lt;code&gt;2019/06/04&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.0 (FromXcode_11_Beta_7_xip) &lt;code&gt;2019/09/03&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.1 &lt;code&gt;2019/08/28&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.2 &lt;code&gt;2019/10/02&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>iGhibli</author><guid isPermaLink="false">https://github.com/iGhibli/iOS-DeviceSupport</guid><pubDate>Mon, 11 Nov 2019 00:17:00 GMT</pubDate></item><item><title>quantopian/zipline #18 in Python, This month</title><link>https://github.com/quantopian/zipline</link><description>&lt;p&gt;&lt;i&gt;Zipline, a Pythonic Algorithmic Trading Library&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;a href="https://www.zipline.io" rel="nofollow"&gt;&lt;img alt="Zipline" src="https://camo.githubusercontent.com/887b8228aa4b569b2a519ef711c7da7e5d6b40cd/68747470733a2f2f6d656469612e7175616e746f7069616e2e636f6d2f6c6f676f732f6f70656e5f736f757263652f7a69706c696e652d6c6f676f2d30335f2e706e67" data-canonical-src="https://media.quantopian.com/logos/open_source/zipline-logo-03_.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="https://gitter.im/quantopian/zipline?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img alt="Gitter" src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/zipline" rel="nofollow"&gt;&lt;img alt="version status" src="https://camo.githubusercontent.com/3eb069ec0b3e276829eb1f88f7c594f33fe1e3d8/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7a69706c696e652e737667" data-canonical-src="https://img.shields.io/pypi/pyversions/zipline.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://travis-ci.org/quantopian/zipline" rel="nofollow"&gt;&lt;img alt="travis status" src="https://camo.githubusercontent.com/29e70eddded2efd56e32b7fc5c150b90820099a1/68747470733a2f2f7472617669732d63692e6f72672f7175616e746f7069616e2f7a69706c696e652e706e673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.org/quantopian/zipline.png?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/quantopian/zipline/branch/master" rel="nofollow"&gt;&lt;img alt="appveyor status" src="https://camo.githubusercontent.com/b0eef78a2d89bc9ea40ea6cfeafb438444705605/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f336467313865363232376476737477362f6272616e63682f6d61737465723f7376673d74727565" data-canonical-src="https://ci.appveyor.com/api/projects/status/3dg18e6227dvstw6/branch/master?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/r/quantopian/zipline" rel="nofollow"&gt;&lt;img alt="Coverage Status" src="https://camo.githubusercontent.com/1a7165b07669cce4c6c69f92ca68d7e3e374bffc/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f7175616e746f7069616e2f7a69706c696e652f62616467652e706e67" data-canonical-src="https://coveralls.io/repos/quantopian/zipline/badge.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zipline is a Pythonic algorithmic trading library. It is an event-driven
system for backtesting. Zipline is currently used in production as the backtesting and live-trading
engine powering &lt;a href="https://www.quantopian.com" rel="nofollow"&gt;Quantopian&lt;/a&gt; -- a free,
community-centered, hosted platform for building and executing trading
strategies.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://groups.google.com/forum/#!forum/zipline" rel="nofollow"&gt;Join our Community!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.zipline.io" rel="nofollow"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Want to Contribute? See our &lt;a href="https://www.zipline.io/development-guidelines" rel="nofollow"&gt;Development Guidelines&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-features"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ease of Use:&lt;/strong&gt; Zipline tries to get out of your way so that you can
focus on algorithm development. See below for a code example.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;"Batteries Included":&lt;/strong&gt; many common statistics like
moving average and linear regression can be readily accessed from
within a user-written algorithm.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyData Integration:&lt;/strong&gt; Input of historical data and output of performance statistics are
based on Pandas DataFrames to integrate nicely into the existing
PyData ecosystem.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Statistics and Machine Learning Libraries:&lt;/strong&gt; You can use libraries like matplotlib, scipy,
statsmodels, and sklearn to support development, analysis, and
visualization of state-of-the-art trading systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-installation"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;a name="user-content-installing-with-pip"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-installing-with-pip" class="anchor" aria-hidden="true" href="#installing-with-pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing With &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Assuming you have all required (see note below) non-Python dependencies, you
can install Zipline with &lt;code&gt;pip&lt;/code&gt; via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ pip install zipline&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Installing Zipline via &lt;code&gt;pip&lt;/code&gt; is slightly more involved than the
average Python package.  Simply running &lt;code&gt;pip install zipline&lt;/code&gt; will likely
fail if you've never installed any scientific Python packages before.&lt;/p&gt;
&lt;p&gt;There are two reasons for the additional complexity:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Zipline ships several C extensions that require access to the CPython C API.
In order to build the C extensions, &lt;code&gt;pip&lt;/code&gt; needs access to the CPython
header files for your Python installation.&lt;/li&gt;
&lt;li&gt;Zipline depends on &lt;a href="https://www.numpy.org/" rel="nofollow"&gt;numpy&lt;/a&gt;, the core library for
numerical array computing in Python.  Numpy depends on having the &lt;a href="https://www.netlib.org/lapack/" rel="nofollow"&gt;LAPACK&lt;/a&gt; linear algebra routines available.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Because LAPACK and the CPython headers are binary dependencies, the correct way
to install them varies from platform to platform.  On Linux, users generally
acquire these dependencies via a package manager like &lt;code&gt;apt&lt;/code&gt;, &lt;code&gt;yum&lt;/code&gt;, or
&lt;code&gt;pacman&lt;/code&gt;.  On OSX, &lt;a href="https://brew.sh/" rel="nofollow"&gt;Homebrew&lt;/a&gt; is a popular choice
providing similar functionality.&lt;/p&gt;
&lt;p&gt;See the full &lt;a href="https://www.zipline.io/install" rel="nofollow"&gt;Zipline Install Documentation&lt;/a&gt; for more information on acquiring
binary dependencies for your specific platform.&lt;/p&gt;
&lt;a name="user-content-conda"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-conda" class="anchor" aria-hidden="true" href="#conda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;conda&lt;/h3&gt;
&lt;p&gt;Another way to install Zipline is via the &lt;code&gt;conda&lt;/code&gt; package manager, which
comes as part of &lt;a href="https://www.anaconda.com/distribution/" rel="nofollow"&gt;Anaconda&lt;/a&gt; or can be
installed via &lt;code&gt;pip install conda&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once set up, you can install Zipline from our &lt;code&gt;Quantopian&lt;/code&gt; channel:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ conda install -c Quantopian zipline&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Currently supported platforms include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GNU/Linux 64-bit&lt;/li&gt;
&lt;li&gt;OSX 64-bit&lt;/li&gt;
&lt;li&gt;Windows 64-bit&lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;
&lt;p&gt;Note&lt;/p&gt;
&lt;p&gt;Windows 32-bit may work; however, it is not currently included in
continuous integration tests.&lt;/p&gt;
&lt;/div&gt;
&lt;a name="user-content-quickstart"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-quickstart" class="anchor" aria-hidden="true" href="#quickstart"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quickstart&lt;/h2&gt;
&lt;p&gt;See our &lt;a href="https://www.zipline.io/beginner-tutorial" rel="nofollow"&gt;getting started tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The following code implements a simple dual moving average algorithm.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; zipline.api &lt;span class="pl-k"&gt;import&lt;/span&gt; order_target, record, symbol

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;initialize&lt;/span&gt;(&lt;span class="pl-smi"&gt;context&lt;/span&gt;):
    context.i &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;
    context.asset &lt;span class="pl-k"&gt;=&lt;/span&gt; symbol(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;AAPL&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)


&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;handle_data&lt;/span&gt;(&lt;span class="pl-smi"&gt;context&lt;/span&gt;, &lt;span class="pl-smi"&gt;data&lt;/span&gt;):
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Skip first 300 days to get full windows&lt;/span&gt;
    context.i &lt;span class="pl-k"&gt;+=&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;
    &lt;span class="pl-k"&gt;if&lt;/span&gt; context.i &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;300&lt;/span&gt;:
        &lt;span class="pl-k"&gt;return&lt;/span&gt;

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Compute averages&lt;/span&gt;
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; data.history() has to be called with the same params&lt;/span&gt;
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; from above and returns a pandas dataframe.&lt;/span&gt;
    short_mavg &lt;span class="pl-k"&gt;=&lt;/span&gt; data.history(context.asset, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;price&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;bar_count&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-v"&gt;frequency&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;1d&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).mean()
    long_mavg &lt;span class="pl-k"&gt;=&lt;/span&gt; data.history(context.asset, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;price&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;bar_count&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;300&lt;/span&gt;, &lt;span class="pl-v"&gt;frequency&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;1d&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).mean()

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trading logic&lt;/span&gt;
    &lt;span class="pl-k"&gt;if&lt;/span&gt; short_mavg &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; long_mavg:
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; order_target orders as many shares as needed to&lt;/span&gt;
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; achieve the desired number of shares.&lt;/span&gt;
        order_target(context.asset, &lt;span class="pl-c1"&gt;100&lt;/span&gt;)
    &lt;span class="pl-k"&gt;elif&lt;/span&gt; short_mavg &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; long_mavg:
        order_target(context.asset, &lt;span class="pl-c1"&gt;0&lt;/span&gt;)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Save values for later inspection&lt;/span&gt;
    record(&lt;span class="pl-v"&gt;AAPL&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;data.current(context.asset, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;price&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
           &lt;span class="pl-v"&gt;short_mavg&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;short_mavg,
           &lt;span class="pl-v"&gt;long_mavg&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;long_mavg)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can then run this algorithm using the Zipline CLI; you'll need a &lt;a href="https://docs.quandl.com/docs#section-authentication" rel="nofollow"&gt;Quandl&lt;/a&gt; API key to ingest the default data bundle.
Once you have your key, run the following from the command line:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ QUANDL_API_KEY=&lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;yourkey&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; zipline ingest -b quandl
$ zipline run -f dual_moving_average.py --start 2014-1-1 --end 2018-1-1 -o dma.pickle&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will download asset pricing data data from quandl, and stream it through the algorithm
over the specified time range. Then, the resulting performance DataFrame is saved in dma.pickle, which you
can load and analyze from within Python.&lt;/p&gt;
&lt;p&gt;You can find other examples in the &lt;code&gt;zipline/examples&lt;/code&gt; directory.&lt;/p&gt;
&lt;a name="user-content-questions"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-questions" class="anchor" aria-hidden="true" href="#questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Questions?&lt;/h2&gt;
&lt;p&gt;If you find a bug, feel free to &lt;a href="https://github.com/quantopian/zipline/issues/new"&gt;open an issue&lt;/a&gt; and fill out the issue template.&lt;/p&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;All contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome. Details on how to set up a development environment can be found in our &lt;a href="https://www.zipline.io/development-guidelines" rel="nofollow"&gt;development guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are looking to start working with the Zipline codebase, navigate to the GitHub issues tab and start looking through interesting issues. Sometimes there are issues labeled as &lt;a href="https://github.com/quantopian/zipline/issues?q=is%3Aissue+is%3Aopen+label%3A%22Beginner+Friendly%22"&gt;Beginner Friendly&lt;/a&gt; or &lt;a href="https://github.com/quantopian/zipline/issues?q=is%3Aissue+is%3Aopen+label%3A%22Help+Wanted%22"&gt;Help Wanted&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feel free to ask questions on the &lt;a href="https://groups.google.com/forum/#!forum/zipline" rel="nofollow"&gt;mailing list&lt;/a&gt; or on &lt;a href="https://gitter.im/quantopian/zipline" rel="nofollow"&gt;Gitter&lt;/a&gt;.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>quantopian</author><guid isPermaLink="false">https://github.com/quantopian/zipline</guid><pubDate>Mon, 11 Nov 2019 00:18:00 GMT</pubDate></item><item><title>albu/albumentations #19 in Python, This month</title><link>https://github.com/albu/albumentations</link><description>&lt;p&gt;&lt;i&gt;fast image augmentation library and easy to use wrapper around other libraries&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-albumentations" class="anchor" aria-hidden="true" href="#albumentations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Albumentations&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/albu/albumentations" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6b8fe9f2ca3f5df1bc4fef4d314834afe83fa0b3/68747470733a2f2f7472617669732d63692e6f72672f616c62752f616c62756d656e746174696f6e732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/albu/albumentations.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://albumentations.readthedocs.io/en/latest/?badge=latest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/416624ea6ead60bdeaa364be35afb166f4ae9ab4/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f616c62756d656e746174696f6e732f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/albumentations/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The library is faster than other libraries on most of the transformations.&lt;/li&gt;
&lt;li&gt;Based on numpy, OpenCV, imgaug picking the best from each of them.&lt;/li&gt;
&lt;li&gt;Simple, flexible API that allows the library to be used in any computer vision pipeline.&lt;/li&gt;
&lt;li&gt;Large, diverse set of transformations.&lt;/li&gt;
&lt;li&gt;Easy to extend the library to wrap around other libraries.&lt;/li&gt;
&lt;li&gt;Easy to extend to other tasks.&lt;/li&gt;
&lt;li&gt;Supports transformations on images, masks, key points and bounding boxes.&lt;/li&gt;
&lt;li&gt;Supports python 2.7-3.7&lt;/li&gt;
&lt;li&gt;Easy integration with PyTorch.&lt;/li&gt;
&lt;li&gt;Easy transfer from torchvision.&lt;/li&gt;
&lt;li&gt;Was used to get top results in many DL competitions at Kaggle, topcoder, CVPR, MICCAI.&lt;/li&gt;
&lt;li&gt;Written by Kaggle Masters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#how-to-use"&gt;How to use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#authors"&gt;Authors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#pypi"&gt;PyPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conda"&gt;Conda&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pixel-level-transforms"&gt;Pixel-level transforms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#spatial-level-transforms"&gt;Spatial-level transforms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#migrating-from-torchvision-to-albumentations"&gt;Migrating from torchvision to albumentations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#benchmarking-results"&gt;Benchmarking results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#contributing"&gt;Contributing&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#adding-new-transforms"&gt;Adding new transforms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#building-the-documentation"&gt;Building the documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#comments"&gt;Comments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#citing"&gt;Citing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#competitions-won-with-the-library"&gt;Competitions won with the library&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#used-by"&gt;Industry users&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-how-to-use" class="anchor" aria-hidden="true" href="#how-to-use"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How to use&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;All in one showcase notebook&lt;/strong&gt; - &lt;a href="https://github.com/albu/albumentations/blob/master/notebooks/showcase.ipynb"&gt;&lt;code&gt;showcase.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt; - &lt;a href="https://github.com/albu/albumentations/blob/master/notebooks/example.ipynb"&gt;&lt;code&gt;example.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Object detection&lt;/strong&gt; - &lt;a href="https://github.com/albu/albumentations/blob/master/notebooks/example_bboxes.ipynb"&gt;&lt;code&gt;example_bboxes.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Non-8-bit images&lt;/strong&gt; - &lt;a href="https://github.com/albu/albumentations/blob/master/notebooks/example_16_bit_tiff.ipynb"&gt;&lt;code&gt;example_16_bit_tiff.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Image segmentation&lt;/strong&gt; &lt;a href="https://github.com/albu/albumentations/blob/master/notebooks/example_kaggle_salt.ipynb"&gt;&lt;code&gt;example_kaggle_salt.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Keypoints&lt;/strong&gt; &lt;a href="https://github.com/albu/albumentations/blob/master/notebooks/example_keypoints.ipynb"&gt;&lt;code&gt;example_keypoints.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custom targets&lt;/strong&gt; &lt;a href="https://github.com/albu/albumentations/blob/master/notebooks/example_multi_target.ipynb"&gt;&lt;code&gt;example_multi_target.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Weather transforms&lt;/strong&gt; &lt;a href="https://github.com/albu/albumentations/blob/master/notebooks/example_weather_transforms.ipynb"&gt;&lt;code&gt;example_weather_transforms.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Serialization&lt;/strong&gt; &lt;a href="https://github.com/albu/albumentations/blob/master/notebooks/serialization.ipynb"&gt;&lt;code&gt;serialization.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Replay/Deterministic mode&lt;/strong&gt; &lt;a href="https://github.com/albu/albumentations/blob/master/notebooks/replay.ipynb"&gt;&lt;code&gt;replay.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can use this &lt;a href="https://colab.research.google.com/drive/1JuZ23u0C0gx93kV0oJ8Mq0B6CBYhPLXy#scrollTo=GwFN-In3iagp&amp;amp;forceEdit=true&amp;amp;offline=true&amp;amp;sandboxMode=true" rel="nofollow"&gt;Google Colaboratory notebook&lt;/a&gt;
to adjust image augmentation parameters and see the resulting images.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/fd2405ab170ab4739c029d7251f5f7b4fac3b41c/68747470733a2f2f686162726173746f726167652e6f72672f776562742f62642f6e652f72762f62646e6572763563746b75646d73617a6e687734637273646669772e6a706567"&gt;&lt;img src="https://camo.githubusercontent.com/fd2405ab170ab4739c029d7251f5f7b4fac3b41c/68747470733a2f2f686162726173746f726167652e6f72672f776562742f62642f6e652f72762f62646e6572763563746b75646d73617a6e687734637273646669772e6a706567" alt="parrot" data-canonical-src="https://habrastorage.org/webt/bd/ne/rv/bdnerv5ctkudmsaznhw4crsdfiw.jpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/43d652646b37ef66762212c0e0d3150ba481347c/68747470733a2f2f686162726173746f726167652e6f72672f776562742f73752f77612f6e702f737577616e70656f36777737777077746f6274727a645f636732302e6a706567"&gt;&lt;img src="https://camo.githubusercontent.com/43d652646b37ef66762212c0e0d3150ba481347c/68747470733a2f2f686162726173746f726167652e6f72672f776562742f73752f77612f6e702f737577616e70656f36777737777077746f6274727a645f636732302e6a706567" alt="inria" data-canonical-src="https://habrastorage.org/webt/su/wa/np/suwanpeo6ww7wpwtobtrzd_cg20.jpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/88e6cb57bed568473e99ee1addefc81865263390/68747470733a2f2f686162726173746f726167652e6f72672f776562742f31692f66692f777a2f31696669777a79306c78657463346e776a7673732d37316e6b77302e6a706567"&gt;&lt;img src="https://camo.githubusercontent.com/88e6cb57bed568473e99ee1addefc81865263390/68747470733a2f2f686162726173746f726167652e6f72672f776562742f31692f66692f777a2f31696669777a79306c78657463346e776a7673732d37316e6b77302e6a706567" alt="medical" data-canonical-src="https://habrastorage.org/webt/1i/fi/wz/1ifiwzy0lxetc4nwjvss-71nkw0.jpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/dfd2d57c087ff30d68958c4ff1aec17a5fdd6b77/68747470733a2f2f686162726173746f726167652e6f72672f776562742f727a2f2d682f336a2f727a2d68336a616c62786963386f5f6668756378797374733474632e6a706567"&gt;&lt;img src="https://camo.githubusercontent.com/dfd2d57c087ff30d68958c4ff1aec17a5fdd6b77/68747470733a2f2f686162726173746f726167652e6f72672f776562742f727a2f2d682f336a2f727a2d68336a616c62786963386f5f6668756378797374733474632e6a706567" alt="vistas" data-canonical-src="https://habrastorage.org/webt/rz/-h/3j/rz-h3jalbxic8o_fhucxysts4tc.jpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/041633dc5d522d6cf583a81d4a1d85be87f44155/68747470733a2f2f686162726173746f726167652e6f72672f776562742f652d2f366b2f7a2d2f652d366b7a2d66756770326865616b336a7a6e733362632d72386f2e6a706567"&gt;&lt;img src="https://camo.githubusercontent.com/041633dc5d522d6cf583a81d4a1d85be87f44155/68747470733a2f2f686162726173746f726167652e6f72672f776562742f652d2f366b2f7a2d2f652d366b7a2d66756770326865616b336a7a6e733362632d72386f2e6a706567" width="100%" data-canonical-src="https://habrastorage.org/webt/e-/6k/z-/e-6kz-fugp2heak3jzns3bc-r8o.jpeg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/al-buslaev/" rel="nofollow"&gt;Alexander Buslaev&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/alex-parinov/" rel="nofollow"&gt;Alex Parinov&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/iglovikov/" rel="nofollow"&gt;Vladimir I. Iglovikov&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/cvtalks/" rel="nofollow"&gt;Evegene Khvedchenya&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/mikhail-druzhinin-548229100/" rel="nofollow"&gt;Mikhail Druzhinin&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-pypi" class="anchor" aria-hidden="true" href="#pypi"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyPI&lt;/h3&gt;
&lt;p&gt;You can use pip to install albumentations:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install albumentations
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to get the latest version of the code before it is released on PyPI you can install the library from GitHub:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install -U git+https://github.com/albu/albumentations
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And it also works in Kaggle GPU kernels &lt;a href="https://www.kaggle.com/creafz/albumentations-installation/" rel="nofollow"&gt;(proof)&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;!pip install albumentations &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-conda" class="anchor" aria-hidden="true" href="#conda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conda&lt;/h3&gt;
&lt;p&gt;To install albumentations using conda we need first to install &lt;code&gt;imgaug&lt;/code&gt; via conda-forge collection&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install -c conda-forge imgaug
conda install albumentations -c albumentations
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;p&gt;The full documentation is available at &lt;a href="https://albumentations.readthedocs.io/en/latest/" rel="nofollow"&gt;albumentations.readthedocs.io&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pixel-level-transforms" class="anchor" aria-hidden="true" href="#pixel-level-transforms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pixel-level transforms&lt;/h2&gt;
&lt;p&gt;Pixel-level transforms will change just an input image and will leave any additional targets such as masks, bounding boxes, and keypoints unchanged. The list of pixel-level transforms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Blur" rel="nofollow"&gt;Blur&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.CLAHE" rel="nofollow"&gt;CLAHE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ChannelDropout" rel="nofollow"&gt;ChannelDropout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ChannelShuffle" rel="nofollow"&gt;ChannelShuffle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.CoarseDropout" rel="nofollow"&gt;CoarseDropout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Cutout" rel="nofollow"&gt;Cutout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Downscale" rel="nofollow"&gt;Downscale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Equalize" rel="nofollow"&gt;Equalize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.FromFloat" rel="nofollow"&gt;FromFloat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.GaussNoise" rel="nofollow"&gt;GaussNoise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.GaussianBlur" rel="nofollow"&gt;GaussianBlur&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.HueSaturationValue" rel="nofollow"&gt;HueSaturationValue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAAdditiveGaussianNoise" rel="nofollow"&gt;IAAAdditiveGaussianNoise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAEmboss" rel="nofollow"&gt;IAAEmboss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAASharpen" rel="nofollow"&gt;IAASharpen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAASuperpixels" rel="nofollow"&gt;IAASuperpixels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ISONoise" rel="nofollow"&gt;ISONoise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ImageCompression" rel="nofollow"&gt;ImageCompression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.InvertImg" rel="nofollow"&gt;InvertImg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.JpegCompression" rel="nofollow"&gt;JpegCompression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.MedianBlur" rel="nofollow"&gt;MedianBlur&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.MotionBlur" rel="nofollow"&gt;MotionBlur&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.MultiplicativeNoise" rel="nofollow"&gt;MultiplicativeNoise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Normalize" rel="nofollow"&gt;Normalize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Posterize" rel="nofollow"&gt;Posterize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RGBShift" rel="nofollow"&gt;RGBShift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomBrightness" rel="nofollow"&gt;RandomBrightness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomBrightnessContrast" rel="nofollow"&gt;RandomBrightnessContrast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomContrast" rel="nofollow"&gt;RandomContrast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomFog" rel="nofollow"&gt;RandomFog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomGamma" rel="nofollow"&gt;RandomGamma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomRain" rel="nofollow"&gt;RandomRain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomShadow" rel="nofollow"&gt;RandomShadow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomSnow" rel="nofollow"&gt;RandomSnow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomSunFlare" rel="nofollow"&gt;RandomSunFlare&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Solarize" rel="nofollow"&gt;Solarize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ToFloat" rel="nofollow"&gt;ToFloat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ToGray" rel="nofollow"&gt;ToGray&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-spatial-level-transforms" class="anchor" aria-hidden="true" href="#spatial-level-transforms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Spatial-level transforms&lt;/h2&gt;
&lt;p&gt;Spatial-level transforms will simultaneously change both an input image as well as additional targets such as masks, bounding boxes, and keypoints. The following table shows which additional targets are supported by each transform.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Transform&lt;/th&gt;
&lt;th align="center"&gt;Image&lt;/th&gt;
&lt;th align="center"&gt;Masks&lt;/th&gt;
&lt;th align="center"&gt;BBoxes&lt;/th&gt;
&lt;th align="center"&gt;Keypoints&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.CenterCrop" rel="nofollow"&gt;CenterCrop&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Crop" rel="nofollow"&gt;Crop&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.CropNonEmptyMaskIfExists" rel="nofollow"&gt;CropNonEmptyMaskIfExists&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ElasticTransform" rel="nofollow"&gt;ElasticTransform&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Flip" rel="nofollow"&gt;Flip&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.GridDistortion" rel="nofollow"&gt;GridDistortion&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.HorizontalFlip" rel="nofollow"&gt;HorizontalFlip&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAAffine" rel="nofollow"&gt;IAAAffine&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAACropAndPad" rel="nofollow"&gt;IAACropAndPad&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAFliplr" rel="nofollow"&gt;IAAFliplr&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAFlipud" rel="nofollow"&gt;IAAFlipud&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAPerspective" rel="nofollow"&gt;IAAPerspective&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/imgaug.html#albumentations.imgaug.transforms.IAAPiecewiseAffine" rel="nofollow"&gt;IAAPiecewiseAffine&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Lambda" rel="nofollow"&gt;Lambda&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.LongestMaxSize" rel="nofollow"&gt;LongestMaxSize&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.MaskDropout" rel="nofollow"&gt;MaskDropout&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NoOp&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.OpticalDistortion" rel="nofollow"&gt;OpticalDistortion&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.PadIfNeeded" rel="nofollow"&gt;PadIfNeeded&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomCrop" rel="nofollow"&gt;RandomCrop&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomCropNearBBox" rel="nofollow"&gt;RandomCropNearBBox&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomGridShuffle" rel="nofollow"&gt;RandomGridShuffle&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomResizedCrop" rel="nofollow"&gt;RandomResizedCrop&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomRotate90" rel="nofollow"&gt;RandomRotate90&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomScale" rel="nofollow"&gt;RandomScale&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomSizedBBoxSafeCrop" rel="nofollow"&gt;RandomSizedBBoxSafeCrop&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomSizedCrop" rel="nofollow"&gt;RandomSizedCrop&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Resize" rel="nofollow"&gt;Resize&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Rotate" rel="nofollow"&gt;Rotate&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ShiftScaleRotate" rel="nofollow"&gt;ShiftScaleRotate&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.SmallestMaxSize" rel="nofollow"&gt;SmallestMaxSize&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Transpose" rel="nofollow"&gt;Transpose&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.VerticalFlip" rel="nofollow"&gt;VerticalFlip&lt;/a&gt;&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;td align="center"&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-migrating-from-torchvision-to-albumentations" class="anchor" aria-hidden="true" href="#migrating-from-torchvision-to-albumentations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Migrating from torchvision to albumentations&lt;/h2&gt;
&lt;p&gt;Migrating from torchvision to albumentations is simple - you just need to change a few lines of code.
Albumentations has equivalents for common torchvision transforms as well as plenty of transforms that are not presented in torchvision.
&lt;a href="https://github.com/albu/albumentations/blob/master/notebooks/migrating_from_torchvision_to_albumentations.ipynb"&gt;&lt;code&gt;migrating_from_torchvision_to_albumentations.ipynb&lt;/code&gt;&lt;/a&gt; shows how one can migrate code from torchvision to albumentations.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-benchmarking-results" class="anchor" aria-hidden="true" href="#benchmarking-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmarking results&lt;/h2&gt;
&lt;p&gt;To run the benchmark yourself follow the instructions in &lt;a href="https://github.com/albu/albumentations/blob/master/benchmark/README.md"&gt;benchmark/README.md&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Results for running the benchmark on first 2000 images from the ImageNet validation set using an Intel Xeon Platinum 8168 CPU.
All outputs are converted to a contiguous NumPy array with the np.uint8 data type.
The table shows how many images per second can be processed on a single core, higher is better.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align="center"&gt;albumentations&lt;br&gt;0.4.2&lt;/th&gt;
&lt;th align="center"&gt;imgaug&lt;br&gt;0.3.0&lt;/th&gt;
&lt;th align="center"&gt;torchvision (Pillow-SIMD backend)&lt;br&gt;0.4.1&lt;/th&gt;
&lt;th align="center"&gt;keras&lt;br&gt;2.3.1&lt;/th&gt;
&lt;th align="center"&gt;augmentor&lt;br&gt;0.2.6&lt;/th&gt;
&lt;th align="center"&gt;solt&lt;br&gt;0.1.8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;HorizontalFlip&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2183&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;1403&lt;/td&gt;
&lt;td align="center"&gt;1757&lt;/td&gt;
&lt;td align="center"&gt;1068&lt;/td&gt;
&lt;td align="center"&gt;1779&lt;/td&gt;
&lt;td align="center"&gt;1031&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VerticalFlip&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;4217&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;2334&lt;/td&gt;
&lt;td align="center"&gt;1538&lt;/td&gt;
&lt;td align="center"&gt;4196&lt;/td&gt;
&lt;td align="center"&gt;1541&lt;/td&gt;
&lt;td align="center"&gt;3820&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Rotate&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;456&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;368&lt;/td&gt;
&lt;td align="center"&gt;163&lt;/td&gt;
&lt;td align="center"&gt;32&lt;/td&gt;
&lt;td align="center"&gt;60&lt;/td&gt;
&lt;td align="center"&gt;116&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ShiftScaleRotate&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;800&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;549&lt;/td&gt;
&lt;td align="center"&gt;146&lt;/td&gt;
&lt;td align="center"&gt;34&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brightness&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2209&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;1288&lt;/td&gt;
&lt;td align="center"&gt;405&lt;/td&gt;
&lt;td align="center"&gt;211&lt;/td&gt;
&lt;td align="center"&gt;403&lt;/td&gt;
&lt;td align="center"&gt;2070&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Contrast&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2215&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;1387&lt;/td&gt;
&lt;td align="center"&gt;338&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;337&lt;/td&gt;
&lt;td align="center"&gt;2073&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BrightnessContrast&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2208&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;740&lt;/td&gt;
&lt;td align="center"&gt;193&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;193&lt;/td&gt;
&lt;td align="center"&gt;1060&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ShiftRGB&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2214&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;1303&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;407&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ShiftHSV&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;468&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;443&lt;/td&gt;
&lt;td align="center"&gt;61&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;144&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gamma&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2281&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;730&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;925&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Grayscale&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;5019&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;436&lt;/td&gt;
&lt;td align="center"&gt;788&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;1451&lt;/td&gt;
&lt;td align="center"&gt;4191&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RandomCrop64&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;173877&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;3340&lt;/td&gt;
&lt;td align="center"&gt;43792&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;36869&lt;/td&gt;
&lt;td align="center"&gt;36178&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PadToSize512&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2906&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;553&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;2711&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resize512&lt;/td&gt;
&lt;td align="center"&gt;663&lt;/td&gt;
&lt;td align="center"&gt;506&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;968&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;954&lt;/td&gt;
&lt;td align="center"&gt;673&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RandomSizedCrop_64_512&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;2565&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;933&lt;/td&gt;
&lt;td align="center"&gt;1395&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;1353&lt;/td&gt;
&lt;td align="center"&gt;2360&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Equalize&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;759&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;457&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;684&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Python and library versions: Python 3.7.5 (default, Oct 19 2019, 00:03:48) [GCC 8.3.0], numpy 1.17.3, pillow-simd 6.0.0.post0, opencv-python 4.1.1.26, scikit-image 0.16.2, scipy 1.3.1.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;To create a pull request to the repository follow the documentation at &lt;a href="docs/contributing.rst"&gt;docs/contributing.rst&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-adding-new-transforms" class="anchor" aria-hidden="true" href="#adding-new-transforms"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding new transforms&lt;/h3&gt;
&lt;p&gt;If you are contributing a new transformation, make sure to update &lt;a href="#pixel-level-transforms"&gt;"Pixel-level transforms"&lt;/a&gt; or/and &lt;a href="#spatial-level-transforms"&gt;"Spatial-level transforms"&lt;/a&gt; sections of this file (&lt;code&gt;README.md&lt;/code&gt;). To do this, simply run (with python3 only):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python3 tools/make_transforms_docs.py make
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and copy/paste the results into the corresponding sections. To validate your modifications, you
can run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python3 tools/make_transforms_docs.py check README.md
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-building-the-documentation" class="anchor" aria-hidden="true" href="#building-the-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Building the documentation&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Go to &lt;code&gt;docs/&lt;/code&gt; directory
&lt;pre&gt;&lt;code&gt;cd docs
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Install required libraries
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Build html files
&lt;pre&gt;&lt;code&gt;make html
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Open &lt;code&gt;_build/html/index.html&lt;/code&gt; in browser.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Alternatively, you can start a web server that rebuilds the documentation
automatically when a change is detected by running &lt;code&gt;make livehtml&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-competitions-won-with-the-library" class="anchor" aria-hidden="true" href="#competitions-won-with-the-library"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Competitions won with the library&lt;/h2&gt;
&lt;p&gt;Albumentations are widely used in Computer Vision Competitions at Kaggle an other platforms.&lt;/p&gt;
&lt;p&gt;You can find their names and links to the solutions &lt;a href="docs/hall_of_fame.rst"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-used-by" class="anchor" aria-hidden="true" href="#used-by"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Used by&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.lyft.com/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ec245c80a1442100762e1e23dff14e110beedfad/68747470733a2f2f686162726173746f726167652e6f72672f776562742f63652f62732f73612f6365627373616a665f3561737374357973686d79796b716a6863672e706e67" width="100" data-canonical-src="https://habrastorage.org/webt/ce/bs/sa/cebssajf_5asst5yshmyykqjhcg.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.x5.ru/en" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/db5e57318cc14a7aba9b56b12100a62e3547b85a/68747470733a2f2f686162726173746f726167652e6f72672f776562742f39792f64762f66312f3979647666316662786f746b6c366e79687964726e3976386371772e706e67" width="100" data-canonical-src="https://habrastorage.org/webt/9y/dv/f1/9ydvf1fbxotkl6nyhydrn9v8cqw.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://imedhub.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ece351193f2589965b06f7c9538ee04a34cbfbb3/68747470733a2f2f686162726173746f726167652e6f72672f776562742f65712f38782f6d2d2f657138786d2d666a66785f75716b6b61345f656b787364777469712e706e67" width="100" data-canonical-src="https://habrastorage.org/webt/eq/8x/m-/eq8xm-fjfx_uqkka4_ekxsdwtiq.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-comments" class="anchor" aria-hidden="true" href="#comments"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Comments&lt;/h2&gt;
&lt;p&gt;In some systems, in the multiple GPU regime PyTorch may deadlock the DataLoader if OpenCV was compiled with OpenCL optimizations. Adding the following two lines before the library import may help. For more details &lt;a href="https://github.com/pytorch/pytorch/issues/1355"&gt;https://github.com/pytorch/pytorch/issues/1355&lt;/a&gt;&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;cv2.setNumThreads(&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
cv2.ocl.setUseOpenCL(&lt;span class="pl-c1"&gt;False&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-citing" class="anchor" aria-hidden="true" href="#citing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing&lt;/h1&gt;
&lt;p&gt;If you find this library useful for your research, please consider citing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{2018arXiv180906839B,
    author = {A. Buslaev, A. Parinov, E. Khvedchenya, V.~I. Iglovikov and A.~A. Kalinin},
     title = "{Albumentations: fast and flexible image augmentations}",
   journal = {ArXiv e-prints},
    eprint = {1809.06839},
      year = 2018
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find the full list of papers that cite Albumentations &lt;a href="docs/citations.rst"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>albu</author><guid isPermaLink="false">https://github.com/albu/albumentations</guid><pubDate>Mon, 11 Nov 2019 00:19:00 GMT</pubDate></item><item><title>sundowndev/PhoneInfoga #20 in Python, This month</title><link>https://github.com/sundowndev/PhoneInfoga</link><description>&lt;p&gt;&lt;i&gt;Advanced information gathering &amp; OSINT tool for phone numbers&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d3fc46eb3ce41e8df8b75fabf670cc20eef2545a/68747470733a2f2f692e696d6775722e636f6d2f4c7455476e46332e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/d3fc46eb3ce41e8df8b75fabf670cc20eef2545a/68747470733a2f2f692e696d6775722e636f6d2f4c7455476e46332e706e67" width="500" data-canonical-src="https://i.imgur.com/LtUGnF3.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a href="https://travis-ci.org/sundowndev/PhoneInfoga" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/46d99e4427dc377be912f425a6527c6785733524/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f73756e646f776e6465762f50686f6e65496e666f67612f6d61737465722e7376673f7374796c653d666c61742d737175617265" alt="Build Status" data-canonical-src="https://img.shields.io/travis/sundowndev/PhoneInfoga/master.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://hub.docker.com/r/sundowndev/phoneinfoga/builds" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/117d6963fac80d7ad53079b83a8eb326aabb0c16/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f636c6f75642f6275696c642f73756e646f776e6465762f70686f6e65696e666f67612e7376673f7374796c653d666c61742d737175617265" alt="Build Status" data-canonical-src="https://img.shields.io/docker/cloud/build/sundowndev/phoneinfoga.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/7d89761ee8648eceb9db7eba9b49cd3c985f9c3d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d626c75652e7376673f7374796c653d666c61742d737175617265" alt="Python version" data-canonical-src="https://img.shields.io/badge/python-3.6-blue.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/sundowndev/PhoneInfoga/releases"&gt;
    &lt;img src="https://camo.githubusercontent.com/a4003bf13d0817cc46214e39aa5f5957c1ac549a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f53756e646f776e4445562f50686f6e65496e666f67612e7376673f7374796c653d666c61742d737175617265" alt="Latest version" data-canonical-src="https://img.shields.io/github/release/SundownDEV/PhoneInfoga.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/sundowndev/PhoneInfoga/blob/master/LICENSE"&gt;
    &lt;img src="https://camo.githubusercontent.com/46c222ff7fbbf26999df33d732182ec6307e69f8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f73756e646f776e6465762f50686f6e65496e666f67612e7376673f7374796c653d666c61742d737175617265" alt="License" data-canonical-src="https://img.shields.io/github/license/sundowndev/PhoneInfoga.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;h4 align="center"&gt;&lt;a id="user-content-information-gathering--osint-reconnaissance-tool-for-phone-numbers" class="anchor" aria-hidden="true" href="#information-gathering--osint-reconnaissance-tool-for-phone-numbers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Information gathering &amp;amp; OSINT reconnaissance tool for phone numbers&lt;/h4&gt;
&lt;p align="center"&gt;
  &lt;a href="https://sundowndev.github.io/PhoneInfoga/" rel="nofollow"&gt;Documentation&lt;/a&gt; •
  &lt;a href="https://sundowndev.github.io/PhoneInfoga/usage/" rel="nofollow"&gt;Basic usage&lt;/a&gt; •
  &lt;a href="https://sundowndev.github.io/PhoneInfoga/resources/" rel="nofollow"&gt;OSINT resources&lt;/a&gt; •
  &lt;a href="https://medium.com/@SundownDEV/phone-number-scanning-osint-recon-tool-6ad8f0cac27b" rel="nofollow"&gt;Related blog post&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;PhoneInfoga is one of the most advanced tools to scan phone numbers using only free resources. The goal is to first gather standard information such as country, area, carrier and line type on any international phone numbers with a very good accuracy. Then search for footprints on search engines to try to find the VoIP provider or identify the owner.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Check if phone number exists and is possible&lt;/li&gt;
&lt;li&gt;Gather standard informations such as country, line type and carrier&lt;/li&gt;
&lt;li&gt;OSINT footprinting using external APIs, Google Hacking, phone books &amp;amp; search engines&lt;/li&gt;
&lt;li&gt;Check for reputation reports, social media, disposable numbers and more&lt;/li&gt;
&lt;li&gt;Scan several numbers at once&lt;/li&gt;
&lt;li&gt;Use custom formatting for more effective OSINT reconnaissance&lt;/li&gt;
&lt;li&gt;Automatic footprinting on several custom formats&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/48694e177cacb5bdbcda70d8dcdd57c1c8e1f512/68747470733a2f2f692e696d6775722e636f6d2f71436b677a7a382e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/48694e177cacb5bdbcda70d8dcdd57c1c8e1f512/68747470733a2f2f692e696d6775722e636f6d2f71436b677a7a382e706e67" alt="Footprinting process" data-canonical-src="https://i.imgur.com/qCkgzz8.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This tool is licensed under the GNU General Public License v3.0.&lt;/p&gt;
&lt;p&gt;Some parts of this code comes from &lt;a href="https://github.com/m4ll0k/infoga"&gt;Infoga&lt;/a&gt;, another project licensed under GPLv3.0.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.flaticon.com/free-icon/fingerprint-search-symbol-of-secret-service-investigation_48838" rel="nofollow"&gt;Icon&lt;/a&gt; made by &lt;a href="https://www.freepik.com/" title="Freepik" rel="nofollow"&gt;Freepik&lt;/a&gt; from &lt;a href="https://www.flaticon.com/" title="Flaticon" rel="nofollow"&gt;flaticon.com&lt;/a&gt; is licensed by &lt;a href="http://creativecommons.org/licenses/by/3.0/" title="Creative Commons BY 3.0" rel="nofollow"&gt;CC 3.0 BY&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://app.fossa.io/projects/git%2Bgithub.com%2Fsundowndev%2FPhoneInfoga?ref=badge_large" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c1bf6c1d6350a0785cc8b07161bcc1cb587e7063/68747470733a2f2f6170702e666f7373612e696f2f6170692f70726f6a656374732f6769742532426769746875622e636f6d25324673756e646f776e64657625324650686f6e65496e666f67612e7376673f747970653d6c61726765" alt="FOSSA Status" data-canonical-src="https://app.fossa.io/api/projects/git%2Bgithub.com%2Fsundowndev%2FPhoneInfoga.svg?type=large" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sundowndev</author><guid isPermaLink="false">https://github.com/sundowndev/PhoneInfoga</guid><pubDate>Mon, 11 Nov 2019 00:20:00 GMT</pubDate></item><item><title>getsentry/sentry #21 in Python, This month</title><link>https://github.com/getsentry/sentry</link><description>&lt;p&gt;&lt;i&gt;Sentry is cross-platform application monitoring, with a focus on error reporting.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;/p&gt;&lt;p align="center"&gt;
    &lt;a href="https://sentry.io/?utm_source=github&amp;amp;utm_medium=logo" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/2dfeafbee0904d6df16ddf7200993dace1629e60/68747470733a2f2f73656e7472792d6272616e642e73746f726167652e676f6f676c65617069732e636f6d2f73656e7472792d6c6f676f2d626c61636b2e706e67" alt="Sentry" height="72" data-canonical-src="https://sentry-brand.storage.googleapis.com/sentry-logo-black.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p align="center"&gt;
    Users and logs provide clues. Sentry provides answers.
  &lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;a name="user-content-what-s-sentry"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-whats-sentry" class="anchor" aria-hidden="true" href="#whats-sentry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's Sentry?&lt;/h2&gt;
&lt;p&gt;Sentry fundamentally is a service that helps you monitor and fix crashes in realtime.
The server is in Python, but it contains a full API for sending events from any
language, in any application.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-1.png"&gt;&lt;img src="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-1.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-2.png"&gt;&lt;img src="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-2.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-3.png"&gt;&lt;img src="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-3.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;&lt;a name="user-content-official-sentry-sdks"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-official-sentry-sdks" class="anchor" aria-hidden="true" href="#official-sentry-sdks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Official Sentry SDKs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-javascript"&gt;JavaScript&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/react-native-sentry"&gt;React-Native&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-python"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/raven-ruby"&gt;Ruby&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-php"&gt;PHP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-go"&gt;Go&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-java"&gt;Java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-cocoa"&gt;Objective-C/Swift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-dotnet"&gt;C#&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/perl-raven"&gt;Perl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-elixir"&gt;Elixir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-laravel"&gt;Laravel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-resources"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.sentry.io/" rel="nofollow"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://forum.sentry.io/" rel="nofollow"&gt;Community&lt;/a&gt; (Bugs, feature requests, general questions)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.sentry.io/internal/contributing/" rel="nofollow"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry/issues"&gt;Bug Tracker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://discord.gg/ez5KZN7" rel="nofollow"&gt;Discord&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.transifex.com/getsentry/sentry/" rel="nofollow"&gt;Transifex&lt;/a&gt; (Translate Sentry!)&lt;/li&gt;
&lt;/ul&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>getsentry</author><guid isPermaLink="false">https://github.com/getsentry/sentry</guid><pubDate>Mon, 11 Nov 2019 00:21:00 GMT</pubDate></item><item><title>eriklindernoren/PyTorch-GAN #22 in Python, This month</title><link>https://github.com/eriklindernoren/PyTorch-GAN</link><description>&lt;p&gt;&lt;i&gt;PyTorch implementations of Generative Adversarial Networks.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="assets/logo.png"&gt;&lt;img src="assets/logo.png" width="480" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pytorch-gan" class="anchor" aria-hidden="true" href="#pytorch-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch-GAN&lt;/h2&gt;
&lt;p&gt;Collection of PyTorch implementations of Generative Adversarial Network varieties presented in research papers. Model architectures will not always mirror the ones proposed in the papers, but I have chosen to focus on getting the core ideas covered instead of getting every layer configuration right. Contributions and suggestions of GANs to implement are very welcomed.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;See also:&lt;/b&gt; &lt;a href="https://github.com/eriklindernoren/Keras-GAN"&gt;Keras-GAN&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#implementations"&gt;Implementations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#auxiliary-classifier-gan"&gt;Auxiliary Classifier GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#adversarial-autoencoder"&gt;Adversarial Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#began"&gt;BEGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bicyclegan"&gt;BicycleGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#boundary-seeking-gan"&gt;Boundary-Seeking GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cluster-gan"&gt;Cluster GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conditional-gan"&gt;Conditional GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#context-conditional-gan"&gt;Context-Conditional GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#context-encoder"&gt;Context Encoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#coupled-gan"&gt;Coupled GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cyclegan"&gt;CycleGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-convolutional-gan"&gt;Deep Convolutional GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#discogan"&gt;DiscoGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dragan"&gt;DRAGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dualgan"&gt;DualGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#energy-based-gan"&gt;Energy-Based GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#enhanced-super-resolution-gan"&gt;Enhanced Super-Resolution GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#gan"&gt;GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#infogan"&gt;InfoGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#least-squares-gan"&gt;Least Squares GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#munit"&gt;MUNIT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pix2pix"&gt;Pix2Pix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pixelda"&gt;PixelDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#relativistic-gan"&gt;Relativistic GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#semi-supervised-gan"&gt;Semi-Supervised GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#softmax-gan"&gt;Softmax GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stargan"&gt;StarGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#super-resolution-gan"&gt;Super-Resolution GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unit"&gt;UNIT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#wasserstein-gan"&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#wasserstein-gan-gp"&gt;Wasserstein GAN GP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#wasserstein-gan-div"&gt;Wasserstein GAN DIV&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/eriklindernoren/PyTorch-GAN
$ cd PyTorch-GAN/
$ sudo pip3 install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-implementations" class="anchor" aria-hidden="true" href="#implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementations&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-auxiliary-classifier-gan" class="anchor" aria-hidden="true" href="#auxiliary-classifier-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Auxiliary Classifier GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Auxiliary Classifier Generative Adversarial Network&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Augustus Odena, Christopher Olah, Jonathon Shlens&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract" class="anchor" aria-hidden="true" href="#abstract"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1610.09585" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/acgan/acgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example" class="anchor" aria-hidden="true" href="#run-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/acgan/
$ python3 acgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/acgan.gif"&gt;&lt;img src="assets/acgan.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-adversarial-autoencoder" class="anchor" aria-hidden="true" href="#adversarial-autoencoder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adversarial Autoencoder&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Adversarial Autoencoder&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-1" class="anchor" aria-hidden="true" href="#authors-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-1" class="anchor" aria-hidden="true" href="#abstract-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;n this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1511.05644" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/aae/aae.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-1" class="anchor" aria-hidden="true" href="#run-example-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/aae/
$ python3 aae.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-began" class="anchor" aria-hidden="true" href="#began"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BEGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;BEGAN: Boundary Equilibrium Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-2" class="anchor" aria-hidden="true" href="#authors-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;David Berthelot, Thomas Schumm, Luke Metz&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-2" class="anchor" aria-hidden="true" href="#abstract-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1703.10717" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/began/began.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-2" class="anchor" aria-hidden="true" href="#run-example-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/began/
$ python3 began.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-bicyclegan" class="anchor" aria-hidden="true" href="#bicyclegan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BicycleGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Toward Multimodal Image-to-Image Translation&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-3" class="anchor" aria-hidden="true" href="#authors-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-3" class="anchor" aria-hidden="true" href="#abstract-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1711.11586" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/bicyclegan/bicyclegan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/bicyclegan_architecture.jpg"&gt;&lt;img src="assets/bicyclegan_architecture.jpg" width="800" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-3" class="anchor" aria-hidden="true" href="#run-example-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_pix2pix_dataset.sh edges2shoes
$ cd ../implementations/bicyclegan/
$ python3 bicyclegan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/bicyclegan.png"&gt;&lt;img src="assets/bicyclegan.png" width="480" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Various style translations by varying the latent code.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-boundary-seeking-gan" class="anchor" aria-hidden="true" href="#boundary-seeking-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Boundary-Seeking GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Boundary-Seeking Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-4" class="anchor" aria-hidden="true" href="#authors-4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;R Devon Hjelm, Athul Paul Jacob, Tong Che, Adam Trischler, Kyunghyun Cho, Yoshua Bengio&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-4" class="anchor" aria-hidden="true" href="#abstract-4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Generative adversarial networks (GANs) are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training, and we demonstrate this on Celeba, Large-scale Scene Understanding (LSUN) bedrooms, and Imagenet without conditioning.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1702.08431" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/bgan/bgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-4" class="anchor" aria-hidden="true" href="#run-example-4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/bgan/
$ python3 bgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-cluster-gan" class="anchor" aria-hidden="true" href="#cluster-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cluster GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;ClusterGAN: Latent Space Clustering in Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-5" class="anchor" aria-hidden="true" href="#authors-5"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Sudipto Mukherjee, Himanshu Asnani, Eugene Lin, Sreeram Kannan&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-5" class="anchor" aria-hidden="true" href="#abstract-5"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Generative Adversarial networks (GANs) have obtained remarkable success in many unsupervised learning tasks and
unarguably, clustering is an important unsupervised learning problem. While one can potentially exploit the
latent-space back-projection in GANs to cluster, we demonstrate that the cluster structure is not retained in the
GAN latent space.  In this paper, we propose ClusterGAN as a new mechanism for clustering using GANs. By sampling
latent variables from a mixture of one-hot encoded variables and continuous latent variables, coupled with an
inverse network (which projects the data to the latent space) trained jointly with a clustering specific loss, we
are able to achieve clustering in the latent space. Our results show a remarkable phenomenon that GANs can preserve
latent space interpolation across categories, even though the discriminator is never exposed to such vectors. We
compare our results with various clustering baselines and demonstrate superior performance on both synthetic and
real datasets.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1809.03627" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/cluster_gan/clustergan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code based on a full PyTorch &lt;a href="https://github.com/zhampel/clusterGAN"&gt;[implementation]&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-5" class="anchor" aria-hidden="true" href="#run-example-5"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/cluster_gan/
$ python3 clustergan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/cluster_gan.gif"&gt;&lt;img src="assets/cluster_gan.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-conditional-gan" class="anchor" aria-hidden="true" href="#conditional-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conditional GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Conditional Generative Adversarial Nets&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-6" class="anchor" aria-hidden="true" href="#authors-6"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Mehdi Mirza, Simon Osindero&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-6" class="anchor" aria-hidden="true" href="#abstract-6"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1411.1784" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/cgan/cgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-6" class="anchor" aria-hidden="true" href="#run-example-6"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/cgan/
$ python3 cgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/cgan.gif"&gt;&lt;img src="assets/cgan.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-context-conditional-gan" class="anchor" aria-hidden="true" href="#context-conditional-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Context-Conditional GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-7" class="anchor" aria-hidden="true" href="#authors-7"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Emily Denton, Sam Gross, Rob Fergus&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-7" class="anchor" aria-hidden="true" href="#abstract-7"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1611.06430" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/ccgan/ccgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-7" class="anchor" aria-hidden="true" href="#run-example-7"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/ccgan/
$ python3 ccgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-context-encoder" class="anchor" aria-hidden="true" href="#context-encoder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Context Encoder&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Context Encoders: Feature Learning by Inpainting&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-8" class="anchor" aria-hidden="true" href="#authors-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-8" class="anchor" aria-hidden="true" href="#abstract-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1604.07379" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/context_encoder/context_encoder.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-8" class="anchor" aria-hidden="true" href="#run-example-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/context_encoder/
&amp;lt;follow steps at the top of context_encoder.py&amp;gt;
$ python3 context_encoder.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/context_encoder.png"&gt;&lt;img src="assets/context_encoder.png" width="640" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Rows: Masked | Inpainted | Original | Masked | Inpainted | Original
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-coupled-gan" class="anchor" aria-hidden="true" href="#coupled-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Coupled GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Coupled Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-9" class="anchor" aria-hidden="true" href="#authors-9"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Ming-Yu Liu, Oncel Tuzel&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-9" class="anchor" aria-hidden="true" href="#abstract-9"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1606.07536" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/cogan/cogan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-9" class="anchor" aria-hidden="true" href="#run-example-9"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/cogan/
$ python3 cogan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/cogan.gif"&gt;&lt;img src="assets/cogan.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Generated MNIST and MNIST-M images
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-cyclegan" class="anchor" aria-hidden="true" href="#cyclegan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CycleGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-10" class="anchor" aria-hidden="true" href="#authors-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-10" class="anchor" aria-hidden="true" href="#abstract-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G:X→Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F:Y→X and introduce a cycle consistency loss to push F(G(X))≈X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1703.10593" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/cyclegan/cyclegan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c653ddc55471557b851a7059540e80799fad7e29/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6379636c6567616e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/c653ddc55471557b851a7059540e80799fad7e29/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6379636c6567616e2e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/cyclegan.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-10" class="anchor" aria-hidden="true" href="#run-example-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_cyclegan_dataset.sh monet2photo
$ cd ../implementations/cyclegan/
$ python3 cyclegan.py --dataset_name monet2photo
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/cyclegan.png"&gt;&lt;img src="assets/cyclegan.png" width="900" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Monet to photo translations.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-deep-convolutional-gan" class="anchor" aria-hidden="true" href="#deep-convolutional-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Convolutional GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Deep Convolutional Generative Adversarial Network&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-11" class="anchor" aria-hidden="true" href="#authors-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Alec Radford, Luke Metz, Soumith Chintala&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-11" class="anchor" aria-hidden="true" href="#abstract-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1511.06434" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/dcgan/dcgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-11" class="anchor" aria-hidden="true" href="#run-example-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/dcgan/
$ python3 dcgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/dcgan.gif"&gt;&lt;img src="assets/dcgan.gif" width="240" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-discogan" class="anchor" aria-hidden="true" href="#discogan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DiscoGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Learning to Discover Cross-Domain Relations with Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-12" class="anchor" aria-hidden="true" href="#authors-12"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, Jiwon Kim&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-12" class="anchor" aria-hidden="true" href="#abstract-12"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1703.05192" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/discogan/discogan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e004d49943b2b1264496d5db1a9bff807afa8a68/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f646973636f67616e5f6172636869746563747572652e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/e004d49943b2b1264496d5db1a9bff807afa8a68/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f646973636f67616e5f6172636869746563747572652e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/discogan_architecture.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-12" class="anchor" aria-hidden="true" href="#run-example-12"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_pix2pix_dataset.sh edges2shoes
$ cd ../implementations/discogan/
$ python3 discogan.py --dataset_name edges2shoes
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/discogan.png"&gt;&lt;img src="assets/discogan.png" width="480" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Rows from top to bottom: (1) Real image from domain A (2) Translated image from &lt;br&gt;
    domain A (3) Reconstructed image from domain A (4) Real image from domain B (5) &lt;br&gt;
    Translated image from domain B (6) Reconstructed image from domain B
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-dragan" class="anchor" aria-hidden="true" href="#dragan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DRAGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;On Convergence and Stability of GANs&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-13" class="anchor" aria-hidden="true" href="#authors-13"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Naveen Kodali, Jacob Abernethy, James Hays, Zsolt Kira&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-13" class="anchor" aria-hidden="true" href="#abstract-13"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We propose studying GAN training dynamics as regret minimization, which is in contrast to the popular view that there is consistent minimization of a divergence between real and generated distributions. We analyze the convergence of GAN training from this new point of view to understand why mode collapse happens. We hypothesize the existence of undesirable local equilibria in this non-convex game to be responsible for mode collapse. We observe that these local equilibria often exhibit sharp gradients of the discriminator function around some real data points. We demonstrate that these degenerate local equilibria can be avoided with a gradient penalty scheme called DRAGAN. We show that DRAGAN enables faster training, achieves improved stability with fewer mode collapses, and leads to generator networks with better modeling performance across a variety of architectures and objective functions.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1705.07215" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/dragan/dragan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-13" class="anchor" aria-hidden="true" href="#run-example-13"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/dragan/
$ python3 dragan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-dualgan" class="anchor" aria-hidden="true" href="#dualgan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DualGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;DualGAN: Unsupervised Dual Learning for Image-to-Image Translation&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-14" class="anchor" aria-hidden="true" href="#authors-14"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Zili Yi, Hao Zhang, Ping Tan, Minglun Gong&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-14" class="anchor" aria-hidden="true" href="#abstract-14"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1704.02510" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/dualgan/dualgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-14" class="anchor" aria-hidden="true" href="#run-example-14"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_pix2pix_dataset.sh facades
$ cd ../implementations/dualgan/
$ python3 dualgan.py --dataset_name facades
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-energy-based-gan" class="anchor" aria-hidden="true" href="#energy-based-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Energy-Based GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Energy-based Generative Adversarial Network&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-15" class="anchor" aria-hidden="true" href="#authors-15"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Junbo Zhao, Michael Mathieu, Yann LeCun&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-15" class="anchor" aria-hidden="true" href="#abstract-15"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We introduce the "Energy-based Generative Adversarial Network" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1609.03126" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/ebgan/ebgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-15" class="anchor" aria-hidden="true" href="#run-example-15"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/ebgan/
$ python3 ebgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-enhanced-super-resolution-gan" class="anchor" aria-hidden="true" href="#enhanced-super-resolution-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Enhanced Super-Resolution GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-16" class="anchor" aria-hidden="true" href="#authors-16"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-16" class="anchor" aria-hidden="true" href="#abstract-16"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at &lt;a href="https://github.com/xinntao/ESRGAN"&gt;this https URL&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1809.00219" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/esrgan/esrgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-16" class="anchor" aria-hidden="true" href="#run-example-16"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/esrgan/
&amp;lt;follow steps at the top of esrgan.py&amp;gt;
$ python3 esrgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/enhanced_superresgan.png"&gt;&lt;img src="assets/enhanced_superresgan.png" width="320" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Nearest Neighbor Upsampling | ESRGAN
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-gan" class="anchor" aria-hidden="true" href="#gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Generative Adversarial Network&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-17" class="anchor" aria-hidden="true" href="#authors-17"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-17" class="anchor" aria-hidden="true" href="#abstract-17"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1406.2661" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/gan/gan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-17" class="anchor" aria-hidden="true" href="#run-example-17"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/gan/
$ python3 gan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/gan.gif"&gt;&lt;img src="assets/gan.gif" width="240" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-infogan" class="anchor" aria-hidden="true" href="#infogan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;InfoGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-18" class="anchor" aria-hidden="true" href="#authors-18"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-18" class="anchor" aria-hidden="true" href="#abstract-18"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1606.03657" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/infogan/infogan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-18" class="anchor" aria-hidden="true" href="#run-example-18"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/infogan/
$ python3 infogan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/infogan.gif"&gt;&lt;img src="assets/infogan.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Result of varying categorical latent variable by column.
&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/infogan.png"&gt;&lt;img src="assets/infogan.png" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Result of varying continuous latent variable by row.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-least-squares-gan" class="anchor" aria-hidden="true" href="#least-squares-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Least Squares GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Least Squares Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-19" class="anchor" aria-hidden="true" href="#authors-19"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, Stephen Paul Smolley&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-19" class="anchor" aria-hidden="true" href="#abstract-19"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson χ2 divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1611.04076" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/lsgan/lsgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-19" class="anchor" aria-hidden="true" href="#run-example-19"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/lsgan/
$ python3 lsgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-munit" class="anchor" aria-hidden="true" href="#munit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MUNIT&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Multimodal Unsupervised Image-to-Image Translation&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-20" class="anchor" aria-hidden="true" href="#authors-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-20" class="anchor" aria-hidden="true" href="#abstract-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at &lt;a href="https://github.com/nvlabs/MUNIT"&gt;this https URL&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1804.04732" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/munit/munit.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-20" class="anchor" aria-hidden="true" href="#run-example-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_pix2pix_dataset.sh edges2shoes
$ cd ../implementations/munit/
$ python3 munit.py --dataset_name edges2shoes
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/munit.png"&gt;&lt;img src="assets/munit.png" width="480" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Results by varying the style code.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pix2pix" class="anchor" aria-hidden="true" href="#pix2pix"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pix2Pix&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Unpaired Image-to-Image Translation with Conditional Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-21" class="anchor" aria-hidden="true" href="#authors-21"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-21" class="anchor" aria-hidden="true" href="#abstract-21"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1611.07004" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/pix2pix/pix2pix.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e8c023b62678aa244f1a474bf643c66c45ef0feb/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f706978327069785f6172636869746563747572652e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/e8c023b62678aa244f1a474bf643c66c45ef0feb/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f706978327069785f6172636869746563747572652e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/pix2pix_architecture.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-21" class="anchor" aria-hidden="true" href="#run-example-21"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_pix2pix_dataset.sh facades
$ cd ../implementations/pix2pix/
$ python3 pix2pix.py --dataset_name facades
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/pix2pix.png"&gt;&lt;img src="assets/pix2pix.png" width="480" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Rows from top to bottom: (1) The condition for the generator (2) Generated image &lt;br&gt;
    based of condition (3) The true corresponding image to the condition
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pixelda" class="anchor" aria-hidden="true" href="#pixelda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PixelDA&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-22" class="anchor" aria-hidden="true" href="#authors-22"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-22" class="anchor" aria-hidden="true" href="#abstract-22"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images often fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that attempt to map representations between the two domains or learn to extract features that are domain-invariant. In this work, we present a new approach that learns, in an unsupervised manner, a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1612.05424" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/pixelda/pixelda.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-mnist-to-mnist-m-classification" class="anchor" aria-hidden="true" href="#mnist-to-mnist-m-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MNIST to MNIST-M Classification&lt;/h4&gt;
&lt;p&gt;Trains a classifier on images that have been translated from the source domain (MNIST) to the target domain (MNIST-M) using the annotations of the source domain images. The classification network is trained jointly with the generator network to optimize the generator for both providing a proper domain translation and also for preserving the semantics of the source domain image. The classification network trained on translated images is compared to the naive solution of training a classifier on MNIST and evaluating it on MNIST-M. The naive model manages a 55% classification accuracy on MNIST-M while the one trained during domain adaptation achieves a 95% classification accuracy.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/pixelda/
$ python3 pixelda.py
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th align="center"&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Naive&lt;/td&gt;
&lt;td align="center"&gt;55%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PixelDA&lt;/td&gt;
&lt;td align="center"&gt;95%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/pixelda.png"&gt;&lt;img src="assets/pixelda.png" width="480" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Rows from top to bottom: (1) Real images from MNIST (2) Translated images from &lt;br&gt;
    MNIST to MNIST-M (3) Examples of images from MNIST-M
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-relativistic-gan" class="anchor" aria-hidden="true" href="#relativistic-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Relativistic GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;The relativistic discriminator: a key element missing from standard GAN&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-23" class="anchor" aria-hidden="true" href="#authors-23"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Alexia Jolicoeur-Martineau&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-23" class="anchor" aria-hidden="true" href="#abstract-23"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs.
We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function.
Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1807.00734" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/relativistic_gan/relativistic_gan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-22" class="anchor" aria-hidden="true" href="#run-example-22"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/relativistic_gan/
$ python3 relativistic_gan.py                 # Relativistic Standard GAN
$ python3 relativistic_gan.py --rel_avg_gan   # Relativistic Average GAN
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-semi-supervised-gan" class="anchor" aria-hidden="true" href="#semi-supervised-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Semi-Supervised GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Semi-Supervised Generative Adversarial Network&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-24" class="anchor" aria-hidden="true" href="#authors-24"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Augustus Odena&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-24" class="anchor" aria-hidden="true" href="#abstract-24"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels. We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes. At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G. We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1606.01583" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/sgan/sgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-23" class="anchor" aria-hidden="true" href="#run-example-23"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/sgan/
$ python3 sgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-softmax-gan" class="anchor" aria-hidden="true" href="#softmax-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Softmax GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Softmax GAN&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-25" class="anchor" aria-hidden="true" href="#authors-25"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Min Lin&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-25" class="anchor" aria-hidden="true" href="#abstract-25"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Softmax GAN is a novel variant of Generative Adversarial Network (GAN). The key idea of Softmax GAN is to replace the classification loss in the original GAN with a softmax cross-entropy loss in the sample space of one single batch. In the adversarial learning of N real training samples and M generated samples, the target of discriminator training is to distribute all the probability mass to the real samples, each with probability 1M, and distribute zero probability to generated data. In the generator training phase, the target is to assign equal probability to all data points in the batch, each with probability 1M+N. While the original GAN is closely related to Noise Contrastive Estimation (NCE), we show that Softmax GAN is the Importance Sampling version of GAN. We futher demonstrate with experiments that this simple change stabilizes GAN training.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1704.06191" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/softmax_gan/softmax_gan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-24" class="anchor" aria-hidden="true" href="#run-example-24"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/softmax_gan/
$ python3 softmax_gan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-stargan" class="anchor" aria-hidden="true" href="#stargan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;StarGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-26" class="anchor" aria-hidden="true" href="#authors-26"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-26" class="anchor" aria-hidden="true" href="#abstract-26"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1711.09020" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/stargan/stargan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-25" class="anchor" aria-hidden="true" href="#run-example-25"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/stargan/
&amp;lt;follow steps at the top of stargan.py&amp;gt;
$ python3 stargan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/stargan.png"&gt;&lt;img src="assets/stargan.png" width="640" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Original | Black Hair | Blonde Hair | Brown Hair | Gender Flip | Aged
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-super-resolution-gan" class="anchor" aria-hidden="true" href="#super-resolution-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Super-Resolution GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-27" class="anchor" aria-hidden="true" href="#authors-27"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-27" class="anchor" aria-hidden="true" href="#abstract-27"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1609.04802" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/srgan/srgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/07288b4b467fbf547c6757a448f8e786bf20f295/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f737570657272657367616e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/07288b4b467fbf547c6757a448f8e786bf20f295/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f737570657272657367616e2e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/superresgan.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-26" class="anchor" aria-hidden="true" href="#run-example-26"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/srgan/
&amp;lt;follow steps at the top of srgan.py&amp;gt;
$ python3 srgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/superresgan.png"&gt;&lt;img src="assets/superresgan.png" width="320" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Nearest Neighbor Upsampling | SRGAN
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-unit" class="anchor" aria-hidden="true" href="#unit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;UNIT&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Unsupervised Image-to-Image Translation Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-28" class="anchor" aria-hidden="true" href="#authors-28"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Ming-Yu Liu, Thomas Breuel, Jan Kautz&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-28" class="anchor" aria-hidden="true" href="#abstract-28"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in this &lt;a href="https://github.com/mingyuliutw/unit"&gt;https URL&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1703.00848" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/unit/unit.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-27" class="anchor" aria-hidden="true" href="#run-example-27"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_cyclegan_dataset.sh apple2orange
$ cd implementations/unit/
$ python3 unit.py --dataset_name apple2orange
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-wasserstein-gan" class="anchor" aria-hidden="true" href="#wasserstein-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wasserstein GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Wasserstein GAN&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-29" class="anchor" aria-hidden="true" href="#authors-29"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Martin Arjovsky, Soumith Chintala, Léon Bottou&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-29" class="anchor" aria-hidden="true" href="#abstract-29"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1701.07875" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/wgan/wgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-28" class="anchor" aria-hidden="true" href="#run-example-28"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/wgan/
$ python3 wgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-wasserstein-gan-gp" class="anchor" aria-hidden="true" href="#wasserstein-gan-gp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wasserstein GAN GP&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Improved Training of Wasserstein GANs&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-30" class="anchor" aria-hidden="true" href="#authors-30"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-30" class="anchor" aria-hidden="true" href="#abstract-30"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1704.00028" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/wgan_gp/wgan_gp.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-29" class="anchor" aria-hidden="true" href="#run-example-29"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/wgan_gp/
$ python3 wgan_gp.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/wgan_gp.gif"&gt;&lt;img src="assets/wgan_gp.gif" width="240" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-wasserstein-gan-div" class="anchor" aria-hidden="true" href="#wasserstein-gan-div"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wasserstein GAN DIV&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Wasserstein Divergence for GANs&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-31" class="anchor" aria-hidden="true" href="#authors-31"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Jiqing Wu, Zhiwu Huang, Janine Thoma, Dinesh Acharya, Luc Van Gool&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-31" class="anchor" aria-hidden="true" href="#abstract-31"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;In many domains of computer vision, generative adversarial networks (GANs) have achieved great success, among which the fam-
ily of Wasserstein GANs (WGANs) is considered to be state-of-the-art due to the theoretical contributions and competitive qualitative performance. However, it is very challenging to approximate the k-Lipschitz constraint required by the Wasserstein-1 metric (W-met). In this paper, we propose a novel Wasserstein divergence (W-div), which is a relaxed version of W-met and does not require the k-Lipschitz constraint.As a concrete application, we introduce a Wasserstein divergence objective for GANs (WGAN-div), which can faithfully approximate W-div through optimization. Under various settings, including progressive growing training, we demonstrate the stability of the proposed WGAN-div owing to its theoretical and practical advantages over WGANs. Also, we study the quantitative and visual performance of WGAN-div on standard image synthesis benchmarks, showing the superior performance of WGAN-div compared to the state-of-the-art methods.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1712.01026" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/wgan_div/wgan_div.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-30" class="anchor" aria-hidden="true" href="#run-example-30"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/wgan_div/
$ python3 wgan_div.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/wgan_div.png"&gt;&lt;img src="assets/wgan_div.png" width="240" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>eriklindernoren</author><guid isPermaLink="false">https://github.com/eriklindernoren/PyTorch-GAN</guid><pubDate>Mon, 11 Nov 2019 00:22:00 GMT</pubDate></item><item><title>psf/black #23 in Python, This month</title><link>https://github.com/psf/black</link><description>&lt;p&gt;&lt;i&gt;The uncompromising Python code formatter&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/psf/black/master/docs/_static/logo2-readme.png"&gt;&lt;img src="https://raw.githubusercontent.com/psf/black/master/docs/_static/logo2-readme.png" alt="Black Logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-the-uncompromising-code-formatter" class="anchor" aria-hidden="true" href="#the-uncompromising-code-formatter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Uncompromising Code Formatter&lt;/h2&gt;
&lt;p align="center"&gt;
&lt;a href="https://travis-ci.com/psf/black" rel="nofollow"&gt;&lt;img alt="Build Status" src="https://camo.githubusercontent.com/0d62c6ce125db151bb0bc13cbc834c0d0522ed88/68747470733a2f2f7472617669732d63692e636f6d2f7073662f626c61636b2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.com/psf/black.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://black.readthedocs.io/en/stable/?badge=stable" rel="nofollow"&gt;&lt;img alt="Documentation Status" src="https://camo.githubusercontent.com/ab197284ad0cd9cae552157b75f815b640eba827/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626c61636b2f62616467652f3f76657273696f6e3d737461626c65" data-canonical-src="https://readthedocs.org/projects/black/badge/?version=stable" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/psf/black?branch=master" rel="nofollow"&gt;&lt;img alt="Coverage Status" src="https://camo.githubusercontent.com/b77842b75f031fbf9bbf690eb55585ae55a8321d/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f7073662f626c61636b2f62616467652e7376673f6272616e63683d6d6173746572" data-canonical-src="https://coveralls.io/repos/github/psf/black/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/black/blob/master/LICENSE"&gt;&lt;img alt="License: MIT" src="https://camo.githubusercontent.com/14a9abb7e83098f2949f26d2190e04fb1bd52c06/68747470733a2f2f626c61636b2e72656164746865646f63732e696f2f656e2f737461626c652f5f7374617469632f6c6963656e73652e737667" data-canonical-src="https://black.readthedocs.io/en/stable/_static/license.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/black/" rel="nofollow"&gt;&lt;img alt="PyPI" src="https://camo.githubusercontent.com/5f6551edb3716d9b0a5659caf441d987bb1527a6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f626c61636b" data-canonical-src="https://img.shields.io/pypi/v/black" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pepy.tech/project/black" rel="nofollow"&gt;&lt;img alt="Downloads" src="https://camo.githubusercontent.com/7b3026c6e1fecc9574cb4b93a3925e392bee087d/68747470733a2f2f706570792e746563682f62616467652f626c61636b" data-canonical-src="https://pepy.tech/badge/black" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/black"&gt;&lt;img alt="Code style: black" src="https://camo.githubusercontent.com/28a51fe3a2c05048d8ca8ecd039d6b1619037326/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Any color you like.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is the uncompromising Python code formatter. By using it, you agree to cede
control over minutiae of hand-formatting. In return, &lt;em&gt;Black&lt;/em&gt; gives you speed,
determinism, and freedom from &lt;code&gt;pycodestyle&lt;/code&gt; nagging about formatting. You will save time
and mental energy for more important matters.&lt;/p&gt;
&lt;p&gt;Blackened code looks the same regardless of the project you're reading. Formatting
becomes transparent after a while and you can focus on the content instead.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; makes code review faster by producing the smallest diffs possible.&lt;/p&gt;
&lt;p&gt;Try it out now using the &lt;a href="https://black.now.sh" rel="nofollow"&gt;Black Playground&lt;/a&gt;. Watch the
&lt;a href="https://youtu.be/esZLCuWs_2Y" rel="nofollow"&gt;PyCon 2019 talk&lt;/a&gt; to learn more.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Contents:&lt;/em&gt; &lt;strong&gt;&lt;a href="#installation-and-usage"&gt;Installation and usage&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#the-black-code-style"&gt;Code style&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#pyprojecttoml"&gt;pyproject.toml&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#editor-integration"&gt;Editor integration&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#blackd"&gt;blackd&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#version-control-integration"&gt;Version control integration&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#ignoring-unmodified-files"&gt;Ignoring unmodified files&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#used-by"&gt;Used by&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#testimonials"&gt;Testimonials&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#show-your-style"&gt;Show your style&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#contributing-to-black"&gt;Contributing&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#change-log"&gt;Change Log&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#authors"&gt;Authors&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-installation-and-usage" class="anchor" aria-hidden="true" href="#installation-and-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation and usage&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; can be installed by running &lt;code&gt;pip install black&lt;/code&gt;. It requires Python 3.6.0+ to
run but you can reformat Python 2 code with it, too.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h3&gt;
&lt;p&gt;To get started right away with sensible defaults:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;black {source_file_or_directory}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-command-line-options" class="anchor" aria-hidden="true" href="#command-line-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Command line options&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; doesn't provide many options. You can list them by running &lt;code&gt;black --help&lt;/code&gt;:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;black [OPTIONS] [SRC]...

Options:
  -c, --code TEXT                 Format the code passed in as a string.
  -l, --line-length INTEGER       How many characters per line to allow.
                                  [default: 88]
  -t, --target-version [py27|py33|py34|py35|py36|py37|py38]
                                  Python versions that should be supported by
                                  Black's output. [default: per-file auto-
                                  detection]
  --py36                          Allow using Python 3.6-only syntax on all
                                  input files.  This will put trailing commas
                                  in function signatures and calls also after
                                  *args and **kwargs. Deprecated; use
                                  --target-version instead. [default: per-file
                                  auto-detection]
  --pyi                           Format all input files like typing stubs
                                  regardless of file extension (useful when
                                  piping source on standard input).
  -S, --skip-string-normalization
                                  Don't normalize string quotes or prefixes.
  --check                         Don't write the files back, just return the
                                  status.  Return code 0 means nothing would
                                  change.  Return code 1 means some files
                                  would be reformatted.  Return code 123 means
                                  there was an internal error.
  --diff                          Don't write the files back, just output a
                                  diff for each file on stdout.
  --fast / --safe                 If --fast given, skip temporary sanity
                                  checks. [default: --safe]
  --include TEXT                  A regular expression that matches files and
                                  directories that should be included on
                                  recursive searches.  An empty value means
                                  all files are included regardless of the
                                  name.  Use forward slashes for directories
                                  on all platforms (Windows, too).  Exclusions
                                  are calculated first, inclusions later.
                                  [default: \.pyi?$]
  --exclude TEXT                  A regular expression that matches files and
                                  directories that should be excluded on
                                  recursive searches.  An empty value means no
                                  paths are excluded. Use forward slashes for
                                  directories on all platforms (Windows, too).
                                  Exclusions are calculated first, inclusions
                                  later.  [default: /(\.eggs|\.git|\.hg|\.mypy
                                  _cache|\.nox|\.tox|\.venv|_build|buck-
                                  out|build|dist)/]
  -q, --quiet                     Don't emit non-error messages to stderr.
                                  Errors are still emitted, silence those with
                                  2&amp;gt;/dev/null.
  -v, --verbose                   Also emit messages to stderr about files
                                  that were not changed or were ignored due to
                                  --exclude=.
  --version                       Show the version and exit.
  --config PATH                   Read configuration from PATH.
  -h, --help                      Show this message and exit.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is a well-behaved Unix-style command-line tool:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it does nothing if no sources are passed to it;&lt;/li&gt;
&lt;li&gt;it will read from standard input and write to standard output if &lt;code&gt;-&lt;/code&gt; is used as the
filename;&lt;/li&gt;
&lt;li&gt;it only outputs messages to users on standard error;&lt;/li&gt;
&lt;li&gt;exits with code 0 unless an internal error occurred (or &lt;code&gt;--check&lt;/code&gt; was used).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-note-this-is-a-beta-product" class="anchor" aria-hidden="true" href="#note-this-is-a-beta-product"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NOTE: This is a beta product&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is already &lt;a href="#used-by"&gt;successfully used&lt;/a&gt; by many projects, small and big. It
also sports a decent test suite. However, it is still very new. Things will probably be
wonky for a while. This is made explicit by the "Beta" trove classifier, as well as by
the "b" in the version number. What this means for you is that &lt;strong&gt;until the formatter
becomes stable, you should expect some formatting to change in the future&lt;/strong&gt;. That being
said, no drastic stylistic changes are planned, mostly responses to bug reports.&lt;/p&gt;
&lt;p&gt;Also, as a temporary safety measure, &lt;em&gt;Black&lt;/em&gt; will check that the reformatted code still
produces a valid AST that is equivalent to the original. This slows it down. If you're
feeling confident, use &lt;code&gt;--fast&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-black-code-style" class="anchor" aria-hidden="true" href="#the-black-code-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The &lt;em&gt;Black&lt;/em&gt; code style&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; reformats entire files in place. It is not configurable. It doesn't take
previous formatting into account. It doesn't reformat blocks that start with
&lt;code&gt;# fmt: off&lt;/code&gt; and end with &lt;code&gt;# fmt: on&lt;/code&gt;. &lt;code&gt;# fmt: on/off&lt;/code&gt; have to be on the same level of
indentation. It also recognizes &lt;a href="https://github.com/google/yapf"&gt;YAPF&lt;/a&gt;'s block comments
to the same effect, as a courtesy for straddling code.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-black-wraps-lines" class="anchor" aria-hidden="true" href="#how-black-wraps-lines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How &lt;em&gt;Black&lt;/em&gt; wraps lines&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; ignores previous formatting and applies uniform horizontal and vertical
whitespace to your code. The rules for horizontal whitespace can be summarized as: do
whatever makes &lt;code&gt;pycodestyle&lt;/code&gt; happy. The coding style used by &lt;em&gt;Black&lt;/em&gt; can be viewed as a
strict subset of PEP 8.&lt;/p&gt;
&lt;p&gt;As for vertical whitespace, &lt;em&gt;Black&lt;/em&gt; tries to render one full expression or simple
statement per line. If this fits the allotted line length, great.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

j &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;,
     &lt;span class="pl-c1"&gt;2&lt;/span&gt;,
     &lt;span class="pl-c1"&gt;3&lt;/span&gt;,
]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

j &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If not, &lt;em&gt;Black&lt;/em&gt; will look at the contents of the first outer matching brackets and put
that in a separate indented line.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

ImportantClass.important_method(exc, limit, lookup_lines, capture_locals, extra_argument)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

ImportantClass.important_method(
    exc, limit, lookup_lines, capture_locals, extra_argument
)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If that still doesn't fit the bill, it will decompose the internal expression further
using the same rule, indenting matching brackets every time. If the contents of the
matching brackets pair are comma-separated (like an argument list, or a dict literal,
and so on) then &lt;em&gt;Black&lt;/em&gt; will first try to keep them on the same line with the matching
brackets. If that doesn't work, it will put all of them in separate lines.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;very_important_function&lt;/span&gt;(&lt;span class="pl-smi"&gt;template&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;, &lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;variables&lt;/span&gt;, &lt;span class="pl-smi"&gt;file&lt;/span&gt;: os.PathLike, &lt;span class="pl-smi"&gt;engine&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;, &lt;span class="pl-smi"&gt;header&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-smi"&gt;debug&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;False&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Applies `variables` to the `template` and writes to `file`.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-v"&gt;file&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;w&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;very_important_function&lt;/span&gt;(
    &lt;span class="pl-smi"&gt;template&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;,
    &lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;variables&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;file&lt;/span&gt;: os.PathLike,
    &lt;span class="pl-smi"&gt;engine&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;header&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;debug&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;False&lt;/span&gt;,
):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Applies `variables` to the `template` and writes to `file`.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-v"&gt;file&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;w&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        &lt;span class="pl-c1"&gt;...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You might have noticed that closing brackets are always dedented and that a trailing
comma is always added. Such formatting produces smaller diffs; when you add or remove an
element, it's always just one line. Also, having the closing bracket dedented provides a
clear delimiter between two distinct sections of the code that otherwise share the same
indentation level (like the arguments list and the docstring in the example above).&lt;/p&gt;
&lt;p&gt;If a data structure literal (tuple, list, set, dict) or a line of "from" imports cannot
fit in the allotted length, it's always split into one element per line. This minimizes
diffs as well as enables readers of code to find which commit introduced a particular
entry. This also makes &lt;em&gt;Black&lt;/em&gt; compatible with &lt;a href="https://pypi.org/p/isort/" rel="nofollow"&gt;isort&lt;/a&gt; with
the following configuration.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;A compatible `.isort.cfg`&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;[settings]
multi_line_output=3
include_trailing_comma=True
force_grid_wrap=0
use_parentheses=True
line_length=88
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The equivalent command line is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ isort --multi-line=3 --trailing-comma --force-grid-wrap=0 --use-parentheses --line-width=88 [ file.py ]
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-line-length" class="anchor" aria-hidden="true" href="#line-length"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Line length&lt;/h3&gt;
&lt;p&gt;You probably noticed the peculiar default line length. &lt;em&gt;Black&lt;/em&gt; defaults to 88 characters
per line, which happens to be 10% over 80. This number was found to produce
significantly shorter files than sticking with 80 (the most popular), or even 79 (used
by the standard library). In general,
&lt;a href="https://youtu.be/wf-BqAjZb8M?t=260" rel="nofollow"&gt;90-ish seems like the wise choice&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you're paid by the line of code you write, you can pass &lt;code&gt;--line-length&lt;/code&gt; with a lower
number. &lt;em&gt;Black&lt;/em&gt; will try to respect that. However, sometimes it won't be able to without
breaking other rules. In those rare cases, auto-formatted code will exceed your allotted
limit.&lt;/p&gt;
&lt;p&gt;You can also increase it, but remember that people with sight disabilities find it
harder to work with line lengths exceeding 100 characters. It also adversely affects
side-by-side diff review on typical screen resolutions. Long lines also make it harder
to present code neatly in documentation or talk slides.&lt;/p&gt;
&lt;p&gt;If you're using Flake8, you can bump &lt;code&gt;max-line-length&lt;/code&gt; to 88 and forget about it.
Alternatively, use &lt;a href="https://github.com/PyCQA/flake8-bugbear"&gt;Bugbear&lt;/a&gt;'s B950 warning
instead of E501 and keep the max line length at 80 which you are probably already using.
You'd do it like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-ini"&gt;&lt;pre&gt;&lt;span class="pl-en"&gt;[flake8]&lt;/span&gt;
&lt;span class="pl-k"&gt;max-line-length&lt;/span&gt; = 80
...
&lt;span class="pl-k"&gt;select&lt;/span&gt; = C,E,F,W,B,B950
&lt;span class="pl-k"&gt;ignore&lt;/span&gt; = E203, E501, W503&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You'll find &lt;em&gt;Black&lt;/em&gt;'s own .flake8 config file is configured like this. Explanation of
why W503 and E203 are disabled can be found further in this documentation. And if you're
curious about the reasoning behind B950,
&lt;a href="https://github.com/PyCQA/flake8-bugbear#opinionated-warnings"&gt;Bugbear's documentation&lt;/a&gt;
explains it. The tl;dr is "it's like highway speed limits, we won't bother you if you
overdo it by a few km/h".&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-empty-lines" class="anchor" aria-hidden="true" href="#empty-lines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Empty lines&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; avoids spurious vertical whitespace. This is in the spirit of PEP 8 which says
that in-function vertical whitespace should only be used sparingly.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will allow single empty lines inside functions, and single and double empty
lines on module level left by the original editors, except when they're within
parenthesized expressions. Since such expressions are always reformatted to fit minimal
space, this whitespace is lost.&lt;/p&gt;
&lt;p&gt;It will also insert proper spacing before and after function definitions. It's one line
before and after inner functions and two lines before and after module-level functions
and classes. &lt;em&gt;Black&lt;/em&gt; will not put empty lines between function/class definitions and
standalone comments that immediately precede the given function/class.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will enforce single empty lines between a class-level docstring and the first
following field or method. This conforms to
&lt;a href="https://www.python.org/dev/peps/pep-0257/#multi-line-docstrings" rel="nofollow"&gt;PEP 257&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; won't insert empty lines after function docstrings unless that empty line is
required due to an inner function starting immediately after.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-trailing-commas" class="anchor" aria-hidden="true" href="#trailing-commas"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trailing commas&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will add trailing commas to expressions that are split by comma where each
element is on its own line. This includes function signatures.&lt;/p&gt;
&lt;p&gt;Unnecessary trailing commas are removed if an expression fits in one line. This makes it
1% more likely that your line won't exceed the allotted line length limit. Moreover, in
this scenario, if you added another argument to your call, you'd probably fit it in the
same line anyway. That doesn't make diffs any larger.&lt;/p&gt;
&lt;p&gt;One exception to removing trailing commas is tuple expressions with just one element. In
this case &lt;em&gt;Black&lt;/em&gt; won't touch the single trailing comma as this would unexpectedly
change the underlying data type. Note that this is also the case when commas are used
while indexing. This is a tuple in disguise: &lt;code&gt;numpy_array[3, ]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One exception to adding trailing commas is function signatures containing &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;*args&lt;/code&gt;,
or &lt;code&gt;**kwargs&lt;/code&gt;. In this case a trailing comma is only safe to use on Python 3.6. &lt;em&gt;Black&lt;/em&gt;
will detect if your file is already 3.6+ only and use trailing commas in this situation.
If you wonder how it knows, it looks for f-strings and existing use of trailing commas
in function signatures that have stars in them. In other words, if you'd like a trailing
comma in this situation and &lt;em&gt;Black&lt;/em&gt; didn't recognize it was safe to do so, put it there
manually and &lt;em&gt;Black&lt;/em&gt; will keep it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-strings" class="anchor" aria-hidden="true" href="#strings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Strings&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; prefers double quotes (&lt;code&gt;"&lt;/code&gt; and &lt;code&gt;"""&lt;/code&gt;) over single quotes (&lt;code&gt;'&lt;/code&gt; and &lt;code&gt;'''&lt;/code&gt;). It
will replace the latter with the former as long as it does not result in more backslash
escapes than before.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; also standardizes string prefixes, making them always lowercase. On top of that,
if your code is already Python 3.6+ only or it's using the &lt;code&gt;unicode_literals&lt;/code&gt; future
import, &lt;em&gt;Black&lt;/em&gt; will remove &lt;code&gt;u&lt;/code&gt; from the string prefix as it is meaningless in those
scenarios.&lt;/p&gt;
&lt;p&gt;The main reason to standardize on a single form of quotes is aesthetics. Having one kind
of quotes everywhere reduces reader distraction. It will also enable a future version of
&lt;em&gt;Black&lt;/em&gt; to merge consecutive string literals that ended up on the same line (see
&lt;a href="https://github.com/psf/black/issues/26"&gt;#26&lt;/a&gt; for details).&lt;/p&gt;
&lt;p&gt;Why settle on double quotes? They anticipate apostrophes in English text. They match the
docstring standard described in
&lt;a href="https://www.python.org/dev/peps/pep-0257/#what-is-a-docstring" rel="nofollow"&gt;PEP 257&lt;/a&gt;. An empty
string in double quotes (&lt;code&gt;""&lt;/code&gt;) is impossible to confuse with a one double-quote
regardless of fonts and syntax highlighting used. On top of this, double quotes for
strings are consistent with C which Python interacts a lot with.&lt;/p&gt;
&lt;p&gt;On certain keyboard layouts like US English, typing single quotes is a bit easier than
double quotes. The latter requires use of the Shift key. My recommendation here is to
keep using whatever is faster to type and let &lt;em&gt;Black&lt;/em&gt; handle the transformation.&lt;/p&gt;
&lt;p&gt;If you are adopting &lt;em&gt;Black&lt;/em&gt; in a large project with pre-existing string conventions
(like the popular
&lt;a href="https://stackoverflow.com/a/56190" rel="nofollow"&gt;"single quotes for data, double quotes for human-readable strings"&lt;/a&gt;),
you can pass &lt;code&gt;--skip-string-normalization&lt;/code&gt; on the command line. This is meant as an
adoption helper, avoid using this for new projects.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-numeric-literals" class="anchor" aria-hidden="true" href="#numeric-literals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Numeric literals&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; standardizes most numeric literals to use lowercase letters for the syntactic
parts and uppercase letters for the digits themselves: &lt;code&gt;0xAB&lt;/code&gt; instead of &lt;code&gt;0XAB&lt;/code&gt; and
&lt;code&gt;1e10&lt;/code&gt; instead of &lt;code&gt;1E10&lt;/code&gt;. Python 2 long literals are styled as &lt;code&gt;2L&lt;/code&gt; instead of &lt;code&gt;2l&lt;/code&gt; to
avoid confusion between &lt;code&gt;l&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-line-breaks--binary-operators" class="anchor" aria-hidden="true" href="#line-breaks--binary-operators"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Line breaks &amp;amp; binary operators&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will break a line before a binary operator when splitting a block of code over
multiple lines. This is so that &lt;em&gt;Black&lt;/em&gt; is compliant with the recent changes in the
&lt;a href="https://www.python.org/dev/peps/pep-0008/#should-a-line-break-before-or-after-a-binary-operator" rel="nofollow"&gt;PEP 8&lt;/a&gt;
style guide, which emphasizes that this approach improves readability.&lt;/p&gt;
&lt;p&gt;This behaviour may raise &lt;code&gt;W503 line break before binary operator&lt;/code&gt; warnings in style
guide enforcement tools like Flake8. Since &lt;code&gt;W503&lt;/code&gt; is not PEP 8 compliant, you should
tell Flake8 to ignore these warnings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-slices" class="anchor" aria-hidden="true" href="#slices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Slices&lt;/h3&gt;
&lt;p&gt;PEP 8
&lt;a href="https://www.python.org/dev/peps/pep-0008/#whitespace-in-expressions-and-statements" rel="nofollow"&gt;recommends&lt;/a&gt;
to treat &lt;code&gt;:&lt;/code&gt; in slices as a binary operator with the lowest priority, and to leave an
equal amount of space on either side, except if a parameter is omitted (e.g.
&lt;code&gt;ham[1 + 1 :]&lt;/code&gt;). It also states that for extended slices, both &lt;code&gt;:&lt;/code&gt; operators have to
have the same amount of spacing, except if a parameter is omitted (&lt;code&gt;ham[1 + 1 ::]&lt;/code&gt;).
&lt;em&gt;Black&lt;/em&gt; enforces these rules consistently.&lt;/p&gt;
&lt;p&gt;This behaviour may raise &lt;code&gt;E203 whitespace before ':'&lt;/code&gt; warnings in style guide
enforcement tools like Flake8. Since &lt;code&gt;E203&lt;/code&gt; is not PEP 8 compliant, you should tell
Flake8 to ignore these warnings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-parentheses" class="anchor" aria-hidden="true" href="#parentheses"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Parentheses&lt;/h3&gt;
&lt;p&gt;Some parentheses are optional in the Python grammar. Any expression can be wrapped in a
pair of parentheses to form an atom. There are a few interesting cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;if (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;while (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;for (...) in (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;assert (...), (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;from X import (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;assignments like:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;target = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;target: type = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;some, *un, packing = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;augmented += (...)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In those cases, parentheses are removed when the entire statement fits in one line, or
if the inner expression doesn't have any delimiters to further split on. If there is
only a single delimiter and the expression starts or ends with a bracket, the
parenthesis can also be successfully omitted since the existing bracket pair will
organize the expression neatly anyway. Otherwise, the parentheses are added.&lt;/p&gt;
&lt;p&gt;Please note that &lt;em&gt;Black&lt;/em&gt; does not add or remove any additional nested parentheses that
you might want to have for clarity or further code organization. For example those
parentheses are not going to be removed:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; (this &lt;span class="pl-k"&gt;or&lt;/span&gt; that)
decision &lt;span class="pl-k"&gt;=&lt;/span&gt; (maybe.this() &lt;span class="pl-k"&gt;and&lt;/span&gt; values &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;) &lt;span class="pl-k"&gt;or&lt;/span&gt; (maybe.that() &lt;span class="pl-k"&gt;and&lt;/span&gt; values &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-call-chains" class="anchor" aria-hidden="true" href="#call-chains"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Call chains&lt;/h3&gt;
&lt;p&gt;Some popular APIs, like ORMs, use call chaining. This API style is known as a
&lt;a href="https://en.wikipedia.org/wiki/Fluent_interface" rel="nofollow"&gt;fluent interface&lt;/a&gt;. &lt;em&gt;Black&lt;/em&gt; formats
those by treating dots that follow a call or an indexing operation like a very low
priority delimiter. It's easier to show the behavior than to explain it. Look at the
example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;example&lt;/span&gt;(&lt;span class="pl-smi"&gt;session&lt;/span&gt;):
    result &lt;span class="pl-k"&gt;=&lt;/span&gt; (
        session.query(models.Customer.id)
        .filter(
            models.Customer.account_id &lt;span class="pl-k"&gt;==&lt;/span&gt; account_id,
            models.Customer.email &lt;span class="pl-k"&gt;==&lt;/span&gt; email_address,
        )
        .order_by(models.Customer.id.asc())
        .all()
    )&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-typing-stub-files" class="anchor" aria-hidden="true" href="#typing-stub-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Typing stub files&lt;/h3&gt;
&lt;p&gt;PEP 484 describes the syntax for type hints in Python. One of the use cases for typing
is providing type annotations for modules which cannot contain them directly (they might
be written in C, or they might be third-party, or their implementation may be overly
dynamic, and so on).&lt;/p&gt;
&lt;p&gt;To solve this,
&lt;a href="https://www.python.org/dev/peps/pep-0484/#stub-files" rel="nofollow"&gt;stub files with the &lt;code&gt;.pyi&lt;/code&gt; file extension&lt;/a&gt;
can be used to describe typing information for an external module. Those stub files omit
the implementation of classes and functions they describe, instead they only contain the
structure of the file (listing globals, functions, and classes with their members). The
recommended code style for those files is more terse than PEP 8:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prefer &lt;code&gt;...&lt;/code&gt; on the same line as the class/function signature;&lt;/li&gt;
&lt;li&gt;avoid vertical whitespace between consecutive module-level functions, names, or
methods and fields within a single class;&lt;/li&gt;
&lt;li&gt;use a single blank line between top-level class definitions, or none if the classes
are very small.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; enforces the above rules. There are additional guidelines for formatting &lt;code&gt;.pyi&lt;/code&gt;
file that are not enforced yet but might be in a future version of the formatter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;all function bodies should be empty (contain &lt;code&gt;...&lt;/code&gt; instead of the body);&lt;/li&gt;
&lt;li&gt;do not use docstrings;&lt;/li&gt;
&lt;li&gt;prefer &lt;code&gt;...&lt;/code&gt; over &lt;code&gt;pass&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;for arguments with a default, use &lt;code&gt;...&lt;/code&gt; instead of the actual default;&lt;/li&gt;
&lt;li&gt;avoid using string literals in type annotations, stub files support forward references
natively (like Python 3.7 code with &lt;code&gt;from __future__ import annotations&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;use variable annotations instead of type comments, even for stubs that target older
versions of Python;&lt;/li&gt;
&lt;li&gt;for arguments that default to &lt;code&gt;None&lt;/code&gt;, use &lt;code&gt;Optional[]&lt;/code&gt; explicitly;&lt;/li&gt;
&lt;li&gt;use &lt;code&gt;float&lt;/code&gt; instead of &lt;code&gt;Union[int, float]&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pyprojecttoml" class="anchor" aria-hidden="true" href="#pyprojecttoml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pyproject.toml&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is able to read project-specific default values for its command line options
from a &lt;code&gt;pyproject.toml&lt;/code&gt; file. This is especially useful for specifying custom
&lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt; patterns for your project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro-tip&lt;/strong&gt;: If you're asking yourself "Do I need to configure anything?" the answer is
"No". &lt;em&gt;Black&lt;/em&gt; is all about sensible defaults.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-what-on-earth-is-a-pyprojecttoml-file" class="anchor" aria-hidden="true" href="#what-on-earth-is-a-pyprojecttoml-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What on Earth is a &lt;code&gt;pyproject.toml&lt;/code&gt; file?&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.python.org/dev/peps/pep-0518/" rel="nofollow"&gt;PEP 518&lt;/a&gt; defines &lt;code&gt;pyproject.toml&lt;/code&gt; as a
configuration file to store build system requirements for Python projects. With the help
of tools like &lt;a href="https://poetry.eustace.io/" rel="nofollow"&gt;Poetry&lt;/a&gt; or
&lt;a href="https://flit.readthedocs.io/en/latest/" rel="nofollow"&gt;Flit&lt;/a&gt; it can fully replace the need for
&lt;code&gt;setup.py&lt;/code&gt; and &lt;code&gt;setup.cfg&lt;/code&gt; files.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-where-black-looks-for-the-file" class="anchor" aria-hidden="true" href="#where-black-looks-for-the-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Where &lt;em&gt;Black&lt;/em&gt; looks for the file&lt;/h3&gt;
&lt;p&gt;By default &lt;em&gt;Black&lt;/em&gt; looks for &lt;code&gt;pyproject.toml&lt;/code&gt; starting from the common base directory of
all files and directories passed on the command line. If it's not there, it looks in
parent directories. It stops looking when it finds the file, or a &lt;code&gt;.git&lt;/code&gt; directory, or a
&lt;code&gt;.hg&lt;/code&gt; directory, or the root of the file system, whichever comes first.&lt;/p&gt;
&lt;p&gt;If you're formatting standard input, &lt;em&gt;Black&lt;/em&gt; will look for configuration starting from
the current working directory.&lt;/p&gt;
&lt;p&gt;You can also explicitly specify the path to a particular file that you want with
&lt;code&gt;--config&lt;/code&gt;. In this situation &lt;em&gt;Black&lt;/em&gt; will not look for any other file.&lt;/p&gt;
&lt;p&gt;If you're running with &lt;code&gt;--verbose&lt;/code&gt;, you will see a blue message if a file was found and
used.&lt;/p&gt;
&lt;p&gt;Please note &lt;code&gt;blackd&lt;/code&gt; will not use &lt;code&gt;pyproject.toml&lt;/code&gt; configuration.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-configuration-format" class="anchor" aria-hidden="true" href="#configuration-format"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration format&lt;/h3&gt;
&lt;p&gt;As the file extension suggests, &lt;code&gt;pyproject.toml&lt;/code&gt; is a
&lt;a href="https://github.com/toml-lang/toml"&gt;TOML&lt;/a&gt; file. It contains separate sections for
different tools. &lt;em&gt;Black&lt;/em&gt; is using the &lt;code&gt;[tool.black]&lt;/code&gt; section. The option keys are the
same as long names of options on the command line.&lt;/p&gt;
&lt;p&gt;Note that you have to use single-quoted strings in TOML for regular expressions. It's
the equivalent of r-strings in Python. Multiline strings are treated as verbose regular
expressions by Black. Use &lt;code&gt;[ ]&lt;/code&gt; to denote a significant space character.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Example `pyproject.toml`&lt;/summary&gt;
&lt;div class="highlight highlight-source-toml"&gt;&lt;pre&gt;[&lt;span class="pl-en"&gt;tool&lt;/span&gt;.&lt;span class="pl-en"&gt;black&lt;/span&gt;]
&lt;span class="pl-smi"&gt;line-length&lt;/span&gt; = &lt;span class="pl-c1"&gt;88&lt;/span&gt;
&lt;span class="pl-smi"&gt;target-version&lt;/span&gt; = [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;py37&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
&lt;span class="pl-smi"&gt;include&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;\.pyi?$&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-smi"&gt;exclude&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'''&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;(&lt;/span&gt;
&lt;span class="pl-s"&gt;  /(&lt;/span&gt;
&lt;span class="pl-s"&gt;      \.eggs         # exclude a few common directories in the&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.git          # root of the project&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.hg&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.mypy_cache&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.tox&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.venv&lt;/span&gt;
&lt;span class="pl-s"&gt;    | _build&lt;/span&gt;
&lt;span class="pl-s"&gt;    | buck-out&lt;/span&gt;
&lt;span class="pl-s"&gt;    | build&lt;/span&gt;
&lt;span class="pl-s"&gt;    | dist&lt;/span&gt;
&lt;span class="pl-s"&gt;  )/&lt;/span&gt;
&lt;span class="pl-s"&gt;  | foo.py           # also separately exclude a file named foo.py in&lt;/span&gt;
&lt;span class="pl-s"&gt;                     # the root of the project&lt;/span&gt;
&lt;span class="pl-s"&gt;)&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'''&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-lookup-hierarchy" class="anchor" aria-hidden="true" href="#lookup-hierarchy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lookup hierarchy&lt;/h3&gt;
&lt;p&gt;Command-line options have defaults that you can see in &lt;code&gt;--help&lt;/code&gt;. A &lt;code&gt;pyproject.toml&lt;/code&gt; can
override those defaults. Finally, options provided by the user on the command line
override both.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will only ever use one &lt;code&gt;pyproject.toml&lt;/code&gt; file during an entire run. It doesn't
look for multiple files, and doesn't compose configuration from different levels of the
file hierarchy.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-editor-integration" class="anchor" aria-hidden="true" href="#editor-integration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Editor integration&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-emacs" class="anchor" aria-hidden="true" href="#emacs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Emacs&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/proofit404/blacken"&gt;proofit404/blacken&lt;/a&gt; or
&lt;a href="https://github.com/jorgenschaefer/elpy"&gt;Elpy&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pycharmintellij-idea" class="anchor" aria-hidden="true" href="#pycharmintellij-idea"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyCharm/IntelliJ IDEA&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;code&gt;black&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;pip install black&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Locate your &lt;code&gt;black&lt;/code&gt; installation folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On macOS / Linux / BSD:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;which black&lt;/span&gt;
&lt;span class="pl-c1"&gt;/usr/local/bin/black  # possible location&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On Windows:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;where black&lt;/span&gt;
&lt;span class="pl-c1"&gt;%LocalAppData%\Programs\Python\Python36-32\Scripts\black.exe  # possible location&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Open External tools in PyCharm/IntelliJ IDEA&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On macOS:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PyCharm -&amp;gt; Preferences -&amp;gt; Tools -&amp;gt; External Tools&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;On Windows / Linux / BSD:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;File -&amp;gt; Settings -&amp;gt; Tools -&amp;gt; External Tools&lt;/code&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;
&lt;p&gt;Click the + icon to add a new external tool with the following values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Name: Black&lt;/li&gt;
&lt;li&gt;Description: Black is the uncompromising Python code formatter.&lt;/li&gt;
&lt;li&gt;Program: &amp;lt;install_location_from_step_2&amp;gt;&lt;/li&gt;
&lt;li&gt;Arguments: &lt;code&gt;"$FilePath$"&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Format the currently opened file by selecting &lt;code&gt;Tools -&amp;gt; External Tools -&amp;gt; black&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alternatively, you can set a keyboard shortcut by navigating to
&lt;code&gt;Preferences or Settings -&amp;gt; Keymap -&amp;gt; External Tools -&amp;gt; External Tools - Black&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optionally, run &lt;em&gt;Black&lt;/em&gt; on every file save:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure you have the
&lt;a href="https://plugins.jetbrains.com/plugin/7177-file-watchers" rel="nofollow"&gt;File Watcher&lt;/a&gt; plugin
installed.&lt;/li&gt;
&lt;li&gt;Go to &lt;code&gt;Preferences or Settings -&amp;gt; Tools -&amp;gt; File Watchers&lt;/code&gt; and click &lt;code&gt;+&lt;/code&gt; to add a
new watcher:
&lt;ul&gt;
&lt;li&gt;Name: Black&lt;/li&gt;
&lt;li&gt;File type: Python&lt;/li&gt;
&lt;li&gt;Scope: Project Files&lt;/li&gt;
&lt;li&gt;Program: &amp;lt;install_location_from_step_2&amp;gt;&lt;/li&gt;
&lt;li&gt;Arguments: &lt;code&gt;$FilePath$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Output paths to refresh: &lt;code&gt;$FilePath$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Working directory: &lt;code&gt;$ProjectFileDir$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Uncheck "Auto-save edited files to trigger the watcher"&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-wing-ide" class="anchor" aria-hidden="true" href="#wing-ide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wing IDE&lt;/h3&gt;
&lt;p&gt;Wing supports black via the OS Commands tool, as explained in the Wing documentation on
&lt;a href="https://wingware.com/doc/edit/pep8" rel="nofollow"&gt;pep8 formatting&lt;/a&gt;. The detailed procedure is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;code&gt;black&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;pip install black&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Make sure it runs from the command line, e.g.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;black --help&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;In Wing IDE, activate the &lt;strong&gt;OS Commands&lt;/strong&gt; panel and define the command &lt;strong&gt;black&lt;/strong&gt; to
execute black on the currently selected file:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Use the Tools -&amp;gt; OS Commands menu selection&lt;/li&gt;
&lt;li&gt;click on &lt;strong&gt;+&lt;/strong&gt; in &lt;strong&gt;OS Commands&lt;/strong&gt; -&amp;gt; New: Command line..
&lt;ul class="contains-task-list"&gt;
&lt;li&gt;Title: black&lt;/li&gt;
&lt;li&gt;Command Line: black %s&lt;/li&gt;
&lt;li&gt;I/O Encoding: Use Default&lt;/li&gt;
&lt;li&gt;Key Binding: F1&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Raise OS Commands when executed&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Auto-save files before execution&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Line mode&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Select a file in the editor and press &lt;strong&gt;F1&lt;/strong&gt; , or whatever key binding you selected
in step 3, to reformat the file.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-vim" class="anchor" aria-hidden="true" href="#vim"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vim&lt;/h3&gt;
&lt;p&gt;Commands and shortcuts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;:Black&lt;/code&gt; to format the entire file (ranges not supported);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:BlackUpgrade&lt;/code&gt; to upgrade &lt;em&gt;Black&lt;/em&gt; inside the virtualenv;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:BlackVersion&lt;/code&gt; to get the current version of &lt;em&gt;Black&lt;/em&gt; inside the virtualenv.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Configuration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;g:black_fast&lt;/code&gt; (defaults to &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_linelength&lt;/code&gt; (defaults to &lt;code&gt;88&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_skip_string_normalization&lt;/code&gt; (defaults to &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_virtualenv&lt;/code&gt; (defaults to &lt;code&gt;~/.vim/black&lt;/code&gt; or &lt;code&gt;~/.local/share/nvim/black&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To install with &lt;a href="https://github.com/junegunn/vim-plug"&gt;vim-plug&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Plug 'psf/black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or with &lt;a href="https://github.com/VundleVim/Vundle.vim"&gt;Vundle&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Plugin 'psf/black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or you can copy the plugin from
&lt;a href="https://github.com/psf/black/tree/master/plugin/black.vim"&gt;plugin/black.vim&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p ~/.vim/pack/python/start/black/plugin
curl https://raw.githubusercontent.com/psf/black/master/plugin/black.vim -o ~/.vim/pack/python/start/black/plugin/black.vim
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me know if this requires any changes to work with Vim 8's builtin &lt;code&gt;packadd&lt;/code&gt;, or
Pathogen, and so on.&lt;/p&gt;
&lt;p&gt;This plugin &lt;strong&gt;requires Vim 7.0+ built with Python 3.6+ support&lt;/strong&gt;. It needs Python 3.6 to
be able to run &lt;em&gt;Black&lt;/em&gt; inside the Vim process which is much faster than calling an
external command.&lt;/p&gt;
&lt;p&gt;On first run, the plugin creates its own virtualenv using the right Python version and
automatically installs &lt;em&gt;Black&lt;/em&gt;. You can upgrade it later by calling &lt;code&gt;:BlackUpgrade&lt;/code&gt; and
restarting Vim.&lt;/p&gt;
&lt;p&gt;If you need to do anything special to make your virtualenv work and install &lt;em&gt;Black&lt;/em&gt; (for
example you want to run a version from master), create a virtualenv manually and point
&lt;code&gt;g:black_virtualenv&lt;/code&gt; to it. The plugin will use it.&lt;/p&gt;
&lt;p&gt;To run &lt;em&gt;Black&lt;/em&gt; on save, add the following line to &lt;code&gt;.vimrc&lt;/code&gt; or &lt;code&gt;init.vim&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autocmd BufWritePre *.py execute ':Black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run &lt;em&gt;Black&lt;/em&gt; on a key press (e.g. F9 below), add this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nnoremap &amp;lt;F9&amp;gt; :Black&amp;lt;CR&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;How to get Vim with Python 3.6?&lt;/strong&gt; On Ubuntu 17.10 Vim comes with Python 3.6 by
default. On macOS with Homebrew run: &lt;code&gt;brew install vim --with-python3&lt;/code&gt;. When building
Vim from source, use: &lt;code&gt;./configure --enable-python3interp=yes&lt;/code&gt;. There's many guides
online how to do this.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-visual-studio-code" class="anchor" aria-hidden="true" href="#visual-studio-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Studio Code&lt;/h3&gt;
&lt;p&gt;Use the
&lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-python.python" rel="nofollow"&gt;Python extension&lt;/a&gt;
(&lt;a href="https://code.visualstudio.com/docs/python/editing#_formatting" rel="nofollow"&gt;instructions&lt;/a&gt;).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-sublimetext-3" class="anchor" aria-hidden="true" href="#sublimetext-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SublimeText 3&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/jgirardet/sublack"&gt;sublack plugin&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-jupyter-notebook-magic" class="anchor" aria-hidden="true" href="#jupyter-notebook-magic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebook Magic&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/csurfer/blackcellmagic"&gt;blackcellmagic&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python-language-server" class="anchor" aria-hidden="true" href="#python-language-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Language Server&lt;/h3&gt;
&lt;p&gt;If your editor supports the &lt;a href="https://langserver.org/" rel="nofollow"&gt;Language Server Protocol&lt;/a&gt; (Atom,
Sublime Text, Visual Studio Code and many more), you can use the
&lt;a href="https://github.com/palantir/python-language-server"&gt;Python Language Server&lt;/a&gt; with the
&lt;a href="https://github.com/rupert/pyls-black"&gt;pyls-black&lt;/a&gt; plugin.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-atomnuclide" class="anchor" aria-hidden="true" href="#atomnuclide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Atom/Nuclide&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://atom.io/packages/python-black" rel="nofollow"&gt;python-black&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-kakoune" class="anchor" aria-hidden="true" href="#kakoune"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kakoune&lt;/h3&gt;
&lt;p&gt;Add the following hook to your kakrc, then run black with &lt;code&gt;:format&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hook global WinSetOption filetype=python %{
    set-option window formatcmd 'black -q  -'
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-other-editors" class="anchor" aria-hidden="true" href="#other-editors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other editors&lt;/h3&gt;
&lt;p&gt;Other editors will require external contributions.&lt;/p&gt;
&lt;p&gt;Patches welcome! &lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;✨&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f370.png"&gt;🍰&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;✨&lt;/g-emoji&gt;&lt;/p&gt;
&lt;p&gt;Any tool that can pipe code through &lt;em&gt;Black&lt;/em&gt; using its stdio mode (just
&lt;a href="https://www.tldp.org/LDP/abs/html/special-chars.html#DASHREF2" rel="nofollow"&gt;use &lt;code&gt;-&lt;/code&gt; as the file name&lt;/a&gt;).
The formatted code will be returned on stdout (unless &lt;code&gt;--check&lt;/code&gt; was passed). &lt;em&gt;Black&lt;/em&gt;
will still emit messages on stderr but that shouldn't affect your use case.&lt;/p&gt;
&lt;p&gt;This can be used for example with PyCharm's or IntelliJ's
&lt;a href="https://www.jetbrains.com/help/pycharm/file-watchers.html" rel="nofollow"&gt;File Watchers&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-blackd" class="anchor" aria-hidden="true" href="#blackd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;blackd&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; is a small HTTP server that exposes &lt;em&gt;Black&lt;/em&gt;'s functionality over a simple
protocol. The main benefit of using it is to avoid paying the cost of starting up a new
&lt;em&gt;Black&lt;/em&gt; process every time you want to blacken a file.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-usage-1" class="anchor" aria-hidden="true" href="#usage-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; is not packaged alongside &lt;em&gt;Black&lt;/em&gt; by default because it has additional
dependencies. You will need to do &lt;code&gt;pip install black[d]&lt;/code&gt; to install it.&lt;/p&gt;
&lt;p&gt;You can start the server on the default port, binding only to the local interface by
running &lt;code&gt;blackd&lt;/code&gt;. You will see a single line mentioning the server's version, and the
host and port it's listening on. &lt;code&gt;blackd&lt;/code&gt; will then print an access log similar to most
web servers on standard output, merged with any exception traces caused by invalid
formatting requests.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; provides even less options than &lt;em&gt;Black&lt;/em&gt;. You can see them by running
&lt;code&gt;blackd --help&lt;/code&gt;:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;Usage: blackd [OPTIONS]

Options:
  --bind-host TEXT                Address to bind the server to.
  --bind-port INTEGER             Port to listen on
  --version                       Show the version and exit.
  -h, --help                      Show this message and exit.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no official blackd client tool (yet!). You can test that blackd is working
using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;blackd --bind-port 9090 &amp;amp;  # or let blackd choose a port
curl -s -XPOST "localhost:9090" -d "print('valid')"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-protocol" class="anchor" aria-hidden="true" href="#protocol"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Protocol&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; only accepts &lt;code&gt;POST&lt;/code&gt; requests at the &lt;code&gt;/&lt;/code&gt; path. The body of the request should
contain the python source code to be formatted, encoded according to the &lt;code&gt;charset&lt;/code&gt; field
in the &lt;code&gt;Content-Type&lt;/code&gt; request header. If no &lt;code&gt;charset&lt;/code&gt; is specified, &lt;code&gt;blackd&lt;/code&gt; assumes
&lt;code&gt;UTF-8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are a few HTTP headers that control how the source is formatted. These correspond
to command line flags for &lt;em&gt;Black&lt;/em&gt;. There is one exception to this: &lt;code&gt;X-Protocol-Version&lt;/code&gt;
which if present, should have the value &lt;code&gt;1&lt;/code&gt;, otherwise the request is rejected with
&lt;code&gt;HTTP 501&lt;/code&gt; (Not Implemented).&lt;/p&gt;
&lt;p&gt;The headers controlling how code is formatted are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;X-Line-Length&lt;/code&gt;: corresponds to the &lt;code&gt;--line-length&lt;/code&gt; command line flag.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Skip-String-Normalization&lt;/code&gt;: corresponds to the &lt;code&gt;--skip-string-normalization&lt;/code&gt;
command line flag. If present and its value is not the empty string, no string
normalization will be performed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Fast-Or-Safe&lt;/code&gt;: if set to &lt;code&gt;fast&lt;/code&gt;, &lt;code&gt;blackd&lt;/code&gt; will act as &lt;em&gt;Black&lt;/em&gt; does when passed the
&lt;code&gt;--fast&lt;/code&gt; command line flag.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Python-Variant&lt;/code&gt;: if set to &lt;code&gt;pyi&lt;/code&gt;, &lt;code&gt;blackd&lt;/code&gt; will act as &lt;em&gt;Black&lt;/em&gt; does when passed the
&lt;code&gt;--pyi&lt;/code&gt; command line flag. Otherwise, its value must correspond to a Python version or
a set of comma-separated Python versions, optionally prefixed with &lt;code&gt;py&lt;/code&gt;. For example,
to request code that is compatible with Python 3.5 and 3.6, set the header to
&lt;code&gt;py3.5,py3.6&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Diff&lt;/code&gt;: corresponds to the &lt;code&gt;--diff&lt;/code&gt; command line flag. If present, a diff of the
formats will be output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of these headers are set to invalid values, &lt;code&gt;blackd&lt;/code&gt; returns a &lt;code&gt;HTTP 400&lt;/code&gt; error
response, mentioning the name of the problematic header in the message body.&lt;/p&gt;
&lt;p&gt;Apart from the above, &lt;code&gt;blackd&lt;/code&gt; can produce the following response codes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;HTTP 204&lt;/code&gt;: If the input is already well-formatted. The response body is empty.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 200&lt;/code&gt;: If formatting was needed on the input. The response body contains the
blackened Python code, and the &lt;code&gt;Content-Type&lt;/code&gt; header is set accordingly.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 400&lt;/code&gt;: If the input contains a syntax error. Details of the error are returned in
the response body.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 500&lt;/code&gt;: If there was any kind of error while trying to format the input. The
response body contains a textual representation of the error.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The response headers include a &lt;code&gt;X-Black-Version&lt;/code&gt; header containing the version of
&lt;em&gt;Black&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-version-control-integration" class="anchor" aria-hidden="true" href="#version-control-integration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Version control integration&lt;/h2&gt;
&lt;p&gt;Use &lt;a href="https://pre-commit.com/" rel="nofollow"&gt;pre-commit&lt;/a&gt;. Once you
&lt;a href="https://pre-commit.com/#install" rel="nofollow"&gt;have it installed&lt;/a&gt;, add this to the
&lt;code&gt;.pre-commit-config.yaml&lt;/code&gt; in your repository:&lt;/p&gt;
&lt;div class="highlight highlight-source-yaml"&gt;&lt;pre&gt;&lt;span class="pl-ent"&gt;repos&lt;/span&gt;:
  - &lt;span class="pl-ent"&gt;repo&lt;/span&gt;: &lt;span class="pl-s"&gt;https://github.com/psf/black&lt;/span&gt;
    &lt;span class="pl-ent"&gt;rev&lt;/span&gt;: &lt;span class="pl-s"&gt;stable&lt;/span&gt;
    &lt;span class="pl-ent"&gt;hooks&lt;/span&gt;:
      - &lt;span class="pl-ent"&gt;id&lt;/span&gt;: &lt;span class="pl-s"&gt;black&lt;/span&gt;
        &lt;span class="pl-ent"&gt;language_version&lt;/span&gt;: &lt;span class="pl-s"&gt;python3.6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then run &lt;code&gt;pre-commit install&lt;/code&gt; and you're ready to go.&lt;/p&gt;
&lt;p&gt;Avoid using &lt;code&gt;args&lt;/code&gt; in the hook. Instead, store necessary configuration in
&lt;code&gt;pyproject.toml&lt;/code&gt; so that editors and command-line usage of Black all behave consistently
for your project. See &lt;em&gt;Black&lt;/em&gt;'s own &lt;a href="/pyproject.toml"&gt;pyproject.toml&lt;/a&gt; for an example.&lt;/p&gt;
&lt;p&gt;If you're already using Python 3.7, switch the &lt;code&gt;language_version&lt;/code&gt; accordingly. Finally,
&lt;code&gt;stable&lt;/code&gt; is a tag that is pinned to the latest release on PyPI. If you'd rather run on
master, this is also an option.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ignoring-unmodified-files" class="anchor" aria-hidden="true" href="#ignoring-unmodified-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ignoring unmodified files&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; remembers files it has already formatted, unless the &lt;code&gt;--diff&lt;/code&gt; flag is used or
code is passed via standard input. This information is stored per-user. The exact
location of the file depends on the &lt;em&gt;Black&lt;/em&gt; version and the system on which &lt;em&gt;Black&lt;/em&gt; is
run. The file is non-portable. The standard location on common operating systems is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Windows:
&lt;code&gt;C:\\Users\&amp;lt;username&amp;gt;\AppData\Local\black\black\Cache\&amp;lt;version&amp;gt;\cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;macOS:
&lt;code&gt;/Users/&amp;lt;username&amp;gt;/Library/Caches/black/&amp;lt;version&amp;gt;/cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Linux:
&lt;code&gt;/home/&amp;lt;username&amp;gt;/.cache/black/&amp;lt;version&amp;gt;/cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;file-mode&lt;/code&gt; is an int flag that determines whether the file was formatted as 3.6+ only,
as .pyi, and whether string normalization was omitted.&lt;/p&gt;
&lt;p&gt;To override the location of these files on macOS or Linux, set the environment variable
&lt;code&gt;XDG_CACHE_HOME&lt;/code&gt; to your preferred location. For example, if you want to put the cache
in the directory you're running &lt;em&gt;Black&lt;/em&gt; from, set &lt;code&gt;XDG_CACHE_HOME=.cache&lt;/code&gt;. &lt;em&gt;Black&lt;/em&gt; will
then write the above files to &lt;code&gt;.cache/black/&amp;lt;version&amp;gt;/&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-used-by" class="anchor" aria-hidden="true" href="#used-by"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Used by&lt;/h2&gt;
&lt;p&gt;The following notable open-source projects trust &lt;em&gt;Black&lt;/em&gt; with enforcing a consistent
code style: pytest, tox, Pyramid, Django Channels, Hypothesis, attrs, SQLAlchemy,
Poetry, PyPA applications (Warehouse, Pipenv, virtualenv), pandas, Pillow, every Datadog
Agent Integration.&lt;/p&gt;
&lt;p&gt;Are we missing anyone? Let us know.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-testimonials" class="anchor" aria-hidden="true" href="#testimonials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testimonials&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Dusty Phillips&lt;/strong&gt;,
&lt;a href="https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;amp;field-keywords=dusty+phillips" rel="nofollow"&gt;writer&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is opinionated so you don't have to be.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hynek Schlawack&lt;/strong&gt;, &lt;a href="https://www.attrs.org/" rel="nofollow"&gt;creator of &lt;code&gt;attrs&lt;/code&gt;&lt;/a&gt;, core developer of
Twisted and CPython:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An auto-formatter that doesn't suck is all I want for Xmas!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Carl Meyer&lt;/strong&gt;, &lt;a href="https://www.djangoproject.com/" rel="nofollow"&gt;Django&lt;/a&gt; core developer:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At least the name is good.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kenneth Reitz&lt;/strong&gt;, creator of &lt;a href="http://python-requests.org/" rel="nofollow"&gt;&lt;code&gt;requests&lt;/code&gt;&lt;/a&gt; and
&lt;a href="https://docs.pipenv.org/" rel="nofollow"&gt;&lt;code&gt;pipenv&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This vastly improves the formatting of our code. Thanks a ton!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-show-your-style" class="anchor" aria-hidden="true" href="#show-your-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Show your style&lt;/h2&gt;
&lt;p&gt;Use the badge in your project's README.md:&lt;/p&gt;
&lt;div class="highlight highlight-source-gfm"&gt;&lt;pre&gt;[![&lt;span class="pl-e"&gt;Code style: black&lt;/span&gt;](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the badge in README.rst:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks like this:
&lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://camo.githubusercontent.com/28a51fe3a2c05048d8ca8ecd039d6b1619037326/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing-to-black" class="anchor" aria-hidden="true" href="#contributing-to-black"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing to &lt;em&gt;Black&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;In terms of inspiration, &lt;em&gt;Black&lt;/em&gt; is about as configurable as &lt;em&gt;gofmt&lt;/em&gt;. This is
deliberate.&lt;/p&gt;
&lt;p&gt;Bug reports and fixes are always welcome! However, before you suggest a new feature or
configuration knob, ask yourself why you want it. If it enables better integration with
some workflow, fixes an inconsistency, speeds things up, and so on - go for it! On the
other hand, if your answer is "because I don't like a particular formatting" then you're
not ready to embrace &lt;em&gt;Black&lt;/em&gt; yet. Such changes are unlikely to get accepted. You can
still try but prepare to be disappointed.&lt;/p&gt;
&lt;p&gt;More details can be found in &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-change-log" class="anchor" aria-hidden="true" href="#change-log"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Change Log&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1910b0" class="anchor" aria-hidden="true" href="#1910b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;19.10b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added support for PEP 572 assignment expressions (#711)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for PEP 570 positional-only arguments (#943)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for async generators (#593)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for pre-splitting collections by putting an explicit trailing comma
inside (#826)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;black -c&lt;/code&gt; as a way to format code passed from the command line (#761)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;--safe now works with Python 2 code (#840)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed grammar selection for Python 2-specific code (#765)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed feature detection for trailing commas in function definitions and call sites
(#763)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt;/&lt;code&gt;# fmt: on&lt;/code&gt; comment pairs placed multiple times within the same block of
code now behave correctly (#1005)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on Windows machines with more than 61 cores (#838)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on standalone comments prepended with a backslash (#767)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on &lt;code&gt;from&lt;/code&gt; ... &lt;code&gt;import&lt;/code&gt; blocks with comments (#829)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on Python 3.7 on some platform configurations (#494)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer fails on comments in from-imports (#671)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer fails when the file starts with a backslash (#922)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer merges regular comments with type comments (#1027)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer splits long lines that contain type comments (#997)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;removed unnecessary parentheses around &lt;code&gt;yield&lt;/code&gt; expressions (#834)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added parentheses around long tuples in unpacking assignments (#832)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added parentheses around complex powers when they are prefixed by a unary operator
(#646)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed bug that led &lt;em&gt;Black&lt;/em&gt; format some code with a line length target of 1 (#762)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer introduces quotes in f-string subexpressions on string boundaries
(#863)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if &lt;em&gt;Black&lt;/em&gt; puts parenthesis around a single expression, it moves comments to the
wrapped expression instead of after the brackets (#872)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; now returns the version of &lt;em&gt;Black&lt;/em&gt; in the response headers (#1013)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; can now output the diff of formats on source code when the &lt;code&gt;X-Diff&lt;/code&gt; header is
provided (#969)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-193b0" class="anchor" aria-hidden="true" href="#193b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;19.3b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;new option &lt;code&gt;--target-version&lt;/code&gt; to control which Python versions &lt;em&gt;Black&lt;/em&gt;-formatted code
should target (#618)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;deprecated &lt;code&gt;--py36&lt;/code&gt; (use &lt;code&gt;--target-version=py36&lt;/code&gt; instead) (#724)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer normalizes numeric literals to include &lt;code&gt;_&lt;/code&gt; separators (#696)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;long &lt;code&gt;del&lt;/code&gt; statements are now split into multiple lines (#698)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;type comments are no longer mangled in function signatures&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;improved performance of formatting deeply nested data structures (#509)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now properly formats multiple files in parallel on Windows (#632)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now creates cache files atomically which allows it to be used in parallel
pipelines (like &lt;code&gt;xargs -P8&lt;/code&gt;) (#673)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now correctly indents comments in files that were previously formatted with
tabs (#262)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; now supports CORS (#622)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-189b0" class="anchor" aria-hidden="true" href="#189b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.9b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;numeric literals are now formatted by &lt;em&gt;Black&lt;/em&gt; (#452, #461, #464, #469):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;numeric literals are normalized to include &lt;code&gt;_&lt;/code&gt; separators on Python 3.6+ code&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--skip-numeric-underscore-normalization&lt;/code&gt; to disable the above behavior and
leave numeric underscores as they were in the input&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code with &lt;code&gt;_&lt;/code&gt; in numeric literals is recognized as Python 3.6+&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;most letters in numeric literals are lowercased (e.g., in &lt;code&gt;1e10&lt;/code&gt;, &lt;code&gt;0x01&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hexadecimal digits are always uppercased (e.g. &lt;code&gt;0xBADC0DE&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;blackd&lt;/code&gt;, see &lt;a href="#blackd"&gt;its documentation&lt;/a&gt; for more info (#349)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;adjacent string literals are now correctly split into multiple lines (#463)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;trailing comma is now added to single imports that don't fit on a line (#250)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cache is now populated when &lt;code&gt;--check&lt;/code&gt; is successful for a file which speeds up
consecutive checks of properly formatted unmodified files (#448)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;whitespace at the beginning of the file is now removed (#399)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed mangling &lt;a href="http://mpastell.com/pweave/" rel="nofollow"&gt;pweave&lt;/a&gt; and
&lt;a href="https://pythonhosted.org/spyder/" rel="nofollow"&gt;Spyder IDE&lt;/a&gt; special comments (#532)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when unpacking big tuples (#267)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of &lt;code&gt;__future__&lt;/code&gt; imports with renames (#389)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed scope of &lt;code&gt;# fmt: off&lt;/code&gt; when directly preceding &lt;code&gt;yield&lt;/code&gt; and other nodes (#385)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed formatting of lambda expressions with default arguments (#468)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed &lt;code&gt;async for&lt;/code&gt; statements: &lt;em&gt;Black&lt;/em&gt; no longer breaks them into separate lines (#372)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;note: the Vim plugin stopped registering &lt;code&gt;,=&lt;/code&gt; as a default chord as it turned out to
be a bad idea (#415)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b4" class="anchor" aria-hidden="true" href="#186b4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hotfix: don't freeze when multiple comments directly precede &lt;code&gt;# fmt: off&lt;/code&gt; (#371)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b3" class="anchor" aria-hidden="true" href="#186b3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;typing stub files (&lt;code&gt;.pyi&lt;/code&gt;) now have blank lines added after constants (#340)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt; and &lt;code&gt;# fmt: on&lt;/code&gt; are now much more dependable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;they now work also within bracket pairs (#329)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they now correctly work across function/class boundaries (#335)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they now work when an indentation block starts with empty lines or misaligned
comments (#334)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;made Click not fail on invalid environments; note that Click is right but the
likelihood we'll need to access non-ASCII file paths when dealing with Python source
code is low (#277)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed improper formatting of f-strings with quotes inside interpolated expressions
(#322)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown when long list literals where found in a file&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown on AST nodes with very many siblings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed cannibalizing backslashes during string normalization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed a crash due to symbolic links pointing outside of the project directory (#338)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b2" class="anchor" aria-hidden="true" href="#186b2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--config&lt;/code&gt; (#65)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;-h&lt;/code&gt; equivalent to &lt;code&gt;--help&lt;/code&gt; (#316)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed improper unmodified file caching when &lt;code&gt;-S&lt;/code&gt; was used&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra space in string unpacking (#305)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed formatting of empty triple quoted strings (#313)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown in comment placement calculation on lines without comments&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b1" class="anchor" aria-hidden="true" href="#186b1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;hotfix: don't output human-facing information on stdout (#299)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hotfix: don't output cake emoji on non-zero return code (#300)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b0" class="anchor" aria-hidden="true" href="#186b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt; (#270)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--skip-string-normalization&lt;/code&gt; (#118)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--verbose&lt;/code&gt; (#283)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the header output in &lt;code&gt;--diff&lt;/code&gt; now actually conforms to the unified diff spec&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed long trivial assignments being wrapped in unnecessary parentheses (#273)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary parentheses when a line contained multiline strings (#232)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed stdin handling not working correctly if an old version of Click was used (#276)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now preserves line endings when formatting a file in place (#258)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-185b1" class="anchor" aria-hidden="true" href="#185b1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.5b1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--pyi&lt;/code&gt; (#249)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--py36&lt;/code&gt; (#249)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python grammar pickle caches are stored with the formatting caches, making &lt;em&gt;Black&lt;/em&gt;
work in environments where site-packages is not user-writable (#192)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now enforces a PEP 257 empty line after a class-level docstring (and/or
fields) and the first method&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid code produced when standalone comments were present in a trailer that
was omitted from line splitting on a large expression (#237)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed optional parentheses being removed within &lt;code&gt;# fmt: off&lt;/code&gt; sections (#224)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid code produced when stars in very long imports were incorrectly wrapped
in optional parentheses (#234)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when inline comments were moved around in a trailer that was
omitted from line splitting on a large expression (#238)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra empty line between a class declaration and the first method if no class
docstring or fields are present (#219)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra empty line between a function signature and an inner function or inner
class (#196)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-185b0" class="anchor" aria-hidden="true" href="#185b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.5b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;call chains are now formatted according to the
&lt;a href="https://en.wikipedia.org/wiki/Fluent_interface" rel="nofollow"&gt;fluent interfaces&lt;/a&gt; style (#67)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;data structure literals (tuples, lists, dictionaries, and sets) are now also always
exploded like imports when they don't fit in a single line (#152)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;slices are now formatted according to PEP 8 (#178)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;parentheses are now also managed automatically on the right-hand side of assignments
and return statements (#140)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;math operators now use their respective priorities for delimiting multiline
expressions (#148)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;optional parentheses are now omitted on expressions that start or end with a bracket
and only contain a single operator (#177)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;empty parentheses in a class definition are now removed (#145, #180)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;string prefixes are now standardized to lowercase and &lt;code&gt;u&lt;/code&gt; is removed on Python 3.6+
only code and Python 2.7+ code with the &lt;code&gt;unicode_literals&lt;/code&gt; future import (#188, #198,
#199)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;typing stub files (&lt;code&gt;.pyi&lt;/code&gt;) are now formatted in a style that is consistent with PEP
484 (#207, #210)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;progress when reformatting many files is now reported incrementally&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed trailers (content with brackets) being unnecessarily exploded into their own
lines after a dedented closing bracket (#119)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed an invalid trailing comma sometimes left in imports (#185)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed non-deterministic formatting when multiple pairs of removable parentheses were
used (#183)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed multiline strings being unnecessarily wrapped in optional parentheses in long
assignments (#215)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed not splitting long from-imports with only a single name&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed Python 3.6+ file discovery by also looking at function calls with unpacking.
This fixed non-deterministic formatting if trailing commas where used both in function
signatures with stars and function calls with stars but the former would be
reformatted to a single line.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed crash on dealing with optional parentheses (#193)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed "is", "is not", "in", and "not in" not considered operators for splitting
purposes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed crash when dead symlinks where encountered&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a4" class="anchor" aria-hidden="true" href="#184a4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;don't populate the cache on &lt;code&gt;--check&lt;/code&gt; (#175)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a3" class="anchor" aria-hidden="true" href="#184a3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added a "cache"; files already reformatted that haven't changed on disk won't be
reformatted again (#109)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;--check&lt;/code&gt; and &lt;code&gt;--diff&lt;/code&gt; are no longer mutually exclusive (#149)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;generalized star expression handling, including double stars; this fixes
multiplication making expressions "unsafe" for trailing commas (#132)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer enforces putting empty lines behind control flow statements (#90)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now splits imports like "Mode 3 + trailing comma" of isort (#127)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed comment indentation when a standalone comment closes a block (#16, #32)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed standalone comments receiving extra empty lines if immediately preceding a
class, def, or decorator (#56, #154)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed &lt;code&gt;--diff&lt;/code&gt; not showing entire path (#130)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of complex expressions after star and double stars in function calls
(#2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid splitting on comma in lambda arguments (#133)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed missing splits of ternary expressions (#141)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a2" class="anchor" aria-hidden="true" href="#184a2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of unaligned standalone comments (#99, #112)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed placement of dictionary unpacking inside dictionary literals (#111)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vim plugin now works on Windows, too&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when encountering unnecessarily escaped quotes in a string
(#120)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a1" class="anchor" aria-hidden="true" href="#184a1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--quiet&lt;/code&gt; (#78)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added automatic parentheses management (#4)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;a href="https://pre-commit.com" rel="nofollow"&gt;pre-commit&lt;/a&gt; integration (#103, #104)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed reporting on &lt;code&gt;--check&lt;/code&gt; with multiple files (#101, #102)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed removing backslash escapes from raw strings (#100, #105)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a0" class="anchor" aria-hidden="true" href="#184a0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--diff&lt;/code&gt; (#87)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;add line breaks before all delimiters, except in cases like commas, to better comply
with PEP 8 (#73)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;standardize string literals to use double quotes (almost) everywhere (#75)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed handling of standalone comments within nested bracketed expressions; &lt;em&gt;Black&lt;/em&gt;
will no longer produce super long lines or put all standalone comments at the end of
the expression (#22)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed 18.3a4 regression: don't crash and burn on empty lines with trailing whitespace
(#80)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed 18.3a4 regression: &lt;code&gt;# yapf: disable&lt;/code&gt; usage as trailing comment would cause
&lt;em&gt;Black&lt;/em&gt; to not emit the rest of the file (#95)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when CTRL+C is pressed while formatting many files, &lt;em&gt;Black&lt;/em&gt; no longer freaks out with
a flurry of asyncio-related exceptions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only allow up to two empty lines on module level and only single empty lines within
functions (#74)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a4" class="anchor" aria-hidden="true" href="#183a4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt; and &lt;code&gt;# fmt: on&lt;/code&gt; are implemented (#5)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;automatic detection of deprecated Python 2 forms of print statements and exec
statements in the formatted file (#49)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;use proper spaces for complex expressions in default values of typed function
arguments (#60)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only return exit code 1 when --check is used (#50)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;don't remove single trailing commas from square bracket indexing (#59)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;don't omit whitespace if the previous factor leaf wasn't a math operator (#55)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;omit extra space in kwarg unpacking if it's the first argument (#46)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;omit extra space in
&lt;a href="http://www.sphinx-doc.org/en/stable/ext/autodoc.html#directive-autoattribute" rel="nofollow"&gt;Sphinx auto-attribute comments&lt;/a&gt;
(#68)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a3" class="anchor" aria-hidden="true" href="#183a3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;don't remove single empty lines outside of bracketed expressions (#19)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added ability to pipe formatting from stdin to stdin (#25)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;restored ability to format code with legacy usage of &lt;code&gt;async&lt;/code&gt; as a name (#20, #42)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;even better handling of numpy-style array indexing (#33, again)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a2" class="anchor" aria-hidden="true" href="#183a2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;changed positioning of binary operators to occur at beginning of lines instead of at
the end, following
&lt;a href="https://github.com/python/peps/commit/c59c4376ad233a62ca4b3a6060c81368bd21e85b"&gt;a recent change to PEP 8&lt;/a&gt;
(#21)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ignore empty bracket pairs while splitting. This avoids very weirdly looking
formattings (#34, #35)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;remove a trailing comma if there is a single argument to a call&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if top level functions were separated by a comment, don't put four empty lines after
the upper function&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting of newlines with imports&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unintentional folding of post scriptum standalone comments into last statement
if it was a simple statement (#18, #28)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed missing space in numpy-style array indexing (#33)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after star-based unary expressions (#31)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a1" class="anchor" aria-hidden="true" href="#183a1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--check&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only put trailing commas in function signatures and calls if it's safe to do so. If
the file is Python 3.6+ it's always safe, otherwise only safe if there are no &lt;code&gt;*args&lt;/code&gt;
or &lt;code&gt;**kwargs&lt;/code&gt; used in the signature or call. (#8)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid spacing of dots in relative imports (#6, #13)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid splitting after comma on unpacked variables in for-loops (#23)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space in parenthesized set expressions (#7)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after opening parentheses and in default arguments (#14, #17)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after unary operators when the operand was a complex expression
(#15)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a0" class="anchor" aria-hidden="true" href="#183a0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;first published version, Happy &lt;g-emoji class="g-emoji" alias="cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f370.png"&gt;🍰&lt;/g-emoji&gt; Day 2018!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;alpha quality&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;date-versioned (see: &lt;a href="https://calver.org/" rel="nofollow"&gt;https://calver.org/&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;p&gt;Glued together by &lt;a href="mailto:lukasz@langa.pl"&gt;Łukasz Langa&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Maintained with &lt;a href="mailto:carolcode@willingconsulting.com"&gt;Carol Willing&lt;/a&gt;,
&lt;a href="mailto:carl@oddbird.net"&gt;Carl Meyer&lt;/a&gt;,
&lt;a href="mailto:jelle.zijlstra@gmail.com"&gt;Jelle Zijlstra&lt;/a&gt;,
&lt;a href="mailto:mail@autophagy.io"&gt;Mika Naylor&lt;/a&gt;, and
&lt;a href="mailto:zsol.zsol@gmail.com"&gt;Zsolt Dollenstein&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Multiple contributions by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mailto:cryptolabour@gmail.com"&gt;Abdur-Rahmaan Janhangeer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:me@adamj.eu"&gt;Adam Johnson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@grande.coffee"&gt;Alexander Huynh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:andrew.thorp.dev@gmail.com"&gt;Andrew Thorp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:dyuuus@yandex.ru"&gt;Andrey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:andy@andyfreeland.net"&gt;Andy Freeland&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:asottile@umich.edu"&gt;Anthony Sottile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:arjaan.buijk@gmail.com"&gt;Arjaan Buijk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:proofit404@gmail.com"&gt;Artem Malyshev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:asgerdrewsen@gmail.com"&gt;Asger Hautop Drewsen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:raf@durin42.com"&gt;Augie Fackler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:aviskarkc10@gmail.com"&gt;Aviskar KC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@benjam.info"&gt;Benjamin Woodruff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:brandtbucher@gmail.com"&gt;Brandt Bucher&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Charles Reid&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:christian@python.org"&gt;Christian Heimes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:chuck.wooters@microsoft.com"&gt;Chuck Wooters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@thequod.de"&gt;Daniel Hahler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:polycitizen@gmail.com"&gt;Daniel M. Capella&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Daniele Esposti&lt;/li&gt;
&lt;li&gt;dylanjblack&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:eli@treuherz.com"&gt;Eli Treuherz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:fthiery@gmail.com"&gt;Florent Thiery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;hauntsaninja&lt;/li&gt;
&lt;li&gt;Hugo van Kemenade&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ivan.katanic@gmail.com"&gt;Ivan Katanić&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:me@jasonfried.info"&gt;Jason Fried&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ijkl@netc.fr"&gt;jgirardet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:jma353@cornell.edu"&gt;Joe Antonakakis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:jon.dufresne@gmail.com"&gt;Jon Dufresne&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ojiidotch@gmail.com"&gt;Jonas Obrist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:joshbode@fastmail.com"&gt;Josh Bode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:hello@juanlu.space"&gt;Juan Luis Cano Rodríguez&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:katie@glasnt.com"&gt;Katie McLaughlin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lawrence Chan&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:mail@linusgroh.de"&gt;Linus Groh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:luka.sterbic@gmail.com"&gt;Luka Sterbic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mariatta&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:vaneseltine@gmail.com"&gt;Matt VanEseltine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:michael.flaxman@gmail.com"&gt;Michael Flaxman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sully@msully.net"&gt;Michael J. Sullivan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:michael@mcclimon.org"&gt;Michael McClimon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:miggaiowski@gmail.com"&gt;Miguel Gaiowski&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:roshi@fedoraproject.org"&gt;Mike&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:minho42@gmail.com"&gt;Min ho Kim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:miroslav@miki725.com"&gt;Miroslav Shubernetskiy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:neraste.herr10@gmail.com"&gt;Neraste&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ofekmeister@gmail.com"&gt;Ofek Lev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:osaetindaniel@gmail.com"&gt;Osaetin Daniel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:Pablogsal@gmail.com"&gt;Pablo Galindo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:mail@peterbe.com"&gt;Peter Bengtsson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;pmacosta&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:rishijha424@gmail.com"&gt;Rishikesh Jha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:hi@stavros.io"&gt;Stavros Korokithakis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sirosen@globus.org"&gt;Stephen Rosen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:snlkapil@gmail.com"&gt;Sunil Kapil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:thomas.c.lu@gmail.com"&gt;Thom Lu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:tom@tomchristie.com"&gt;Tom Christie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:uranusjr@gmail.com"&gt;Tzu-ping Chung&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ukshah2@illinois.edu"&gt;Utsav Shah&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;vezeli&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sharma.vishwas88@gmail.com"&gt;Vishwas B Sharma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:yngve@hoiseth.net"&gt;Yngve Høiseth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:1998uriyyo@gmail.com"&gt;Yurii Karabas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>psf</author><guid isPermaLink="false">https://github.com/psf/black</guid><pubDate>Mon, 11 Nov 2019 00:23:00 GMT</pubDate></item><item><title>PaddlePaddle/models #24 in Python, This month</title><link>https://github.com/PaddlePaddle/models</link><description>&lt;p&gt;&lt;i&gt;Pre-trained and Reproduced Deep Learning Models （『飞桨』官方模型库，包含多种学术前沿和工业场景验证的深度学习模型）&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-paddlepaddle-models" class="anchor" aria-hidden="true" href="#paddlepaddle-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PaddlePaddle Models&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/models"&gt;&lt;img src="https://camo.githubusercontent.com/c7b42a2a9563380b01c0f92d8a41413c4c0047ef/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d627269676874677265656e2e7376673f7374796c653d666c6174" alt="Documentation Status" data-canonical-src="https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="LICENSE"&gt;&lt;img src="https://camo.githubusercontent.com/8f54547853cfad57acfc8e06e6008cc296cda34d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865253230322d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/license-Apache%202-blue.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PaddlePaddle 提供了丰富的计算单元，使得用户可以采用模块化的方法解决各种学习问题。在此Repo中，我们展示了如何用 PaddlePaddle来解决常见的机器学习任务，提供若干种不同的易学易用的神经网络模型。PaddlePaddle用户可领取&lt;strong&gt;免费Tesla V100在线算力资源&lt;/strong&gt;，高效训练模型，&lt;strong&gt;每日登陆即送12小时&lt;/strong&gt;，&lt;strong&gt;连续五天运行再加送48小时&lt;/strong&gt;，&lt;a href="http://ai.baidu.com/support/news?action=detail&amp;amp;id=981" rel="nofollow"&gt;前往使用免费算力&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-目录" class="anchor" aria-hidden="true" href="#目录"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#PaddleCV"&gt;智能视觉(PaddleCV)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB"&gt;图像分类&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"&gt;目标检测&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2"&gt;图像分割&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B"&gt;关键点检测&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"&gt;图像生成&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E5%9C%BA%E6%99%AF%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB"&gt;场景文字识别&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0"&gt;度量学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E8%A7%86%E9%A2%91%E5%88%86%E7%B1%BB%E5%92%8C%E5%8A%A8%E4%BD%9C%E5%AE%9A%E4%BD%8D"&gt;视频分类和动作定位&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#PaddleNLP"&gt;智能文本处理(PaddleNLP)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#NLP-%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF"&gt;NLP 基础技术&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#NLP-%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF"&gt;NLP 核心技术&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#NLP-%E7%B3%BB%E7%BB%9F%E5%BA%94%E7%94%A8"&gt;NLP 系统应用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#PaddleRec"&gt;智能推荐(PaddleRec)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#PaddleSpeech"&gt;智能语音(PaddleSpeech)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#PaddleSlim"&gt;模型压缩(PaddleSlim)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9E%8B"&gt;其他模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#%E5%BF%AB%E9%80%9F%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%E5%BA%93"&gt;快速下载模型库&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-paddlecv" class="anchor" aria-hidden="true" href="#paddlecv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PaddleCV&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-图像分类" class="anchor" aria-hidden="true" href="#图像分类"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;图像分类&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;图像分类&lt;/a&gt; 是根据图像的语义信息对不同类别图像进行区分，是计算机视觉中重要的基础问题，是物体检测、图像分割、物体跟踪、行为分析、人脸识别等其他高层视觉任务的基础，在许多领域都有着广泛的应用。如：安防领域的人脸识别和智能视频分析等，交通领域的交通场景识别，互联网领域基于内容的图像检索和相册自动归类，医学领域的图像识别等。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;模型名称&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;模型简介&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;数据集&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;评估指标 top-1/top-5 accuracy&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;AlexNet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;首次在 CNN 中成功的应用了 ReLU, Dropout 和 LRN，并使用 GPU 进行运算加速&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;56.72%/79.17%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;VGG19&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在 AlexNet 的基础上使用 3*3 小卷积核，增加网络深度，具有很好的泛化能力&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;72.56%/90.93%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;GoogLeNet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在不增加计算负载的前提下增加了网络的深度和宽度，性能更加优越&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;70.70%/89.66%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;ResNet50&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Residual Network，引入了新的残差结构，解决了随着网络加深，准确率下降的问题&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;76.50%/93.00%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;ResNet200_vd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;融合多种对 ResNet 改进策略，ResNet200_vd 的 top1 准确率达到 80.93%&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;80.93%/95.33%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;Inceptionv4&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;将 Inception 模块与 Residual Connection 进行结合，通过ResNet的结构极大地加速训练并获得性能的提升&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;80.77%/95.26%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;MobileNetV1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;将传统的卷积结构改造成两层卷积结构的网络，在基本不影响准确率的前提下大大减少计算时间，更适合移动端和嵌入式视觉应用&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;70.99%/89.68%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;MobileNetV2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;MobileNet结构的微调，直接在 thinner 的 bottleneck层上进行 skip learning 连接以及对 bottleneck layer 不进行 ReLu 非线性处理可取得更好的结果&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;72.15%/90.65%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;SENet154_vd&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在ResNeXt 基础、上加入了 SE(Sequeeze-and-Excitation) 模块，提高了识别准确率，在 ILSVRC 2017 的分类项目中取得了第一名&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;81.40%/95.48%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;ShuffleNetV2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ECCV2018，轻量级 CNN 网络，在速度和准确度之间做了很好地平衡。在同等复杂度下，比 ShuffleNet 和 MobileNetv2 更准确，更适合移动端以及无人车领域&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;70.03%/89.17%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;efficientNet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;同时对模型的分辨率，通道数和深度。进行缩放，用极少的参数就可以达到SOTA的精度。&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;77.38%/93.31%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;xception71&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;对inception-v3的改进，用深度可分离卷积代替普通卷积，降低参数量的同时提高了精度。&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;81.11%/95.45%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;dpn107&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;融合了densenet和resnext的特点。&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;80.89%/95.32%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;mobilenetV3_small_x1_0&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在v2的基础上增加了se模块，并且使用hard-swish激活函数。在分类、检测、分割等视觉任务上都有不错表现。&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;67.46%/87.12%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;DarkNet53&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;检测框架yolov3使用的backbone，在分类和检测任务上都有不错表现。&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;78.04%/94.05%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;DenseNet161&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;提出了密集连接的网络结构，更加有利于信息流的传递。&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;78.57%/94.14%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;ResNeXt152_vd_64x4d&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;提出了cardinatity的概念，用于作为模型复杂度的另外一个度量，并依据该概念有效地提升了模型精度。&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;81.08%/95.34%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;SqueezeNet1_1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;提出了新的网络架构Fire Module，通过减少参数来进行模型压缩。&lt;/td&gt;
&lt;td&gt;ImageNet-2012验证集&lt;/td&gt;
&lt;td&gt;60.08%/81.85%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;更多图像分类模型请参考 &lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification"&gt;Image Classification&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-目标检测" class="anchor" aria-hidden="true" href="#目标检测"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;目标检测&lt;/h3&gt;
&lt;p&gt;目标检测任务的目标是给定一张图像或是一个视频帧，让计算机找出其中所有目标的位置，并给出每个目标的具体类别。对于计算机而言，能够“看到”的是图像被编码之后的数字，但很难解图像或是视频帧中出现了人或是物体这样的高层语义概念，也就更加难以定位目标出现在图像中哪个区域。目标检测模型请参考 &lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleDetection"&gt;PaddleDetection&lt;/a&gt;。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型名称&lt;/th&gt;
&lt;th&gt;模型简介&lt;/th&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;th&gt;评估指标   mAP&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleDetection"&gt;SSD&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;很好的继承了 MobileNet 预测速度快，易于部署的特点，能够很好的在多种设备上完成图像目标检测任务&lt;/td&gt;
&lt;td&gt;VOC07 test&lt;/td&gt;
&lt;td&gt;mAP   = 73.32%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleDetection"&gt;Faster-RCNN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;创造性地采用卷积网络自行产生建议框，并且和目标检测网络共享卷积网络，建议框数目减少，质量提高&lt;/td&gt;
&lt;td&gt;MS-COCO&lt;/td&gt;
&lt;td&gt;基于ResNet 50  mAP(0.50: 0.95) = 36.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleDetection"&gt;Mask-RCNN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;经典的两阶段框架，在 Faster R-CNN模型基础上添加分割分支，得到掩码结果，实现了掩码和类别预测关系的解藕，可得到像素级别的检测结果。&lt;/td&gt;
&lt;td&gt;MS-COCO&lt;/td&gt;
&lt;td&gt;基于ResNet 50   Mask mAP（0.50: 0.95） = 31.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleDetection"&gt;RetinaNet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;经典的一阶段框架，由主干网络、FPN结构、和两个分别用于回归物体位置和预测物体类别的子网络组成。在训练过程中使用 Focal Loss，解决了传统一阶段检测器存在前景背景类别不平衡的问题，进一步提高了一阶段检测器的精度。&lt;/td&gt;
&lt;td&gt;MS-COCO&lt;/td&gt;
&lt;td&gt;基于ResNet 50 mAP (0.50: 0.95) = 36%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleDetection"&gt;YOLOv3&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;速度和精度均衡的目标检测网络，相比于原作者 darknet 中的 YOLO v3 实现，PaddlePaddle 实现参考了论文 &lt;a href="https://arxiv.org/pdf/1812.01187.pdf" rel="nofollow"&gt;Bag of Tricks for Image Classification with Convolutional Neural Networks&lt;/a&gt; 增加了 mixup，label_smooth 等处理，精度 (mAP(0.50: 0.95)) 相比于原作者提高了 4.7 个绝对百分点，在此基础上加入 synchronize batch normalization, 最终精度相比原作者提高 5.9 个绝对百分点。&lt;/td&gt;
&lt;td&gt;MS-COCO&lt;/td&gt;
&lt;td&gt;基于DarkNet   mAP(0.50: 0.95)=   38.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/face_detection"&gt;PyramidBox&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;PyramidBox&lt;/strong&gt; &lt;strong&gt;模型是百度自主研发的人脸检测模型&lt;/strong&gt;，利用上下文信息解决困难人脸的检测问题，网络表达能力高，鲁棒性强。于18年3月份在 WIDER Face 数据集上取得第一名&lt;/td&gt;
&lt;td&gt;WIDER FACE&lt;/td&gt;
&lt;td&gt;mAP   （Easy/Medium/Hard   set）=   96.0%/ 94.8%/ 88.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleDetection"&gt;Cascade RCNN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Cascade R-CNN 在 Faster R-CNN 框架下，通过级联多个检测器，在训练过程中选取不同的 IoU 阈值，逐步提高目标定位的精度，从而获取优异的检测性能。&lt;/td&gt;
&lt;td&gt;MS-COCO&lt;/td&gt;
&lt;td&gt;基于ResNet 50 mAP (0.50: 0.95) = 40.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleDetection"&gt;Faceboxes&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;经典的人脸检测网络，被称为“高精度 CPU 实时人脸检测器”。网络中使用率 CReLU、density_prior_bo x等组件，使得模型的精度和速度得到平衡与提升。相比于 PyramidBox，预测与计算更快，模型更小，精度也保持高水平。&lt;/td&gt;
&lt;td&gt;WIDER FACE&lt;/td&gt;
&lt;td&gt;mAP (Easy/Medium/Hard Set) = 0.898/0.872/0.752&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleDetection"&gt;BlazeFace&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;高速的人脸检测网络，由5个单的和6个双 BlazeBlocks、和 SSD 的架构构成。它轻巧但性能良好，并且专为移动 GPU 推理量身定制。&lt;/td&gt;
&lt;td&gt;WIDER FACE&lt;/td&gt;
&lt;td&gt;mAP Easy/Medium/Hard Set = 0.915/0.892/0.797&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-图像分割" class="anchor" aria-hidden="true" href="#图像分割"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;图像分割&lt;/h3&gt;
&lt;p&gt;图像语义分割顾名思义是将图像像素按照表达的语义含义的不同进行分组/分割，图像语义是指对图像内容的理解，例如，能够描绘出什么物体在哪里做了什么事情等，分割是指对图片中的每个像素点进行标注，标注属于哪一类别。近年来用在无人车驾驶技术中分割街景来避让行人和车辆、医疗影像分析中辅助诊断等。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型名称&lt;/th&gt;
&lt;th&gt;模型简介&lt;/th&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;th&gt;评估指标&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/icnet"&gt;ICNet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;主要用于图像实时语义分割，能够兼顾速度和准确性，易于线上部署&lt;/td&gt;
&lt;td&gt;Cityscape&lt;/td&gt;
&lt;td&gt;Mean IoU=67.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/deeplabv3%2B"&gt;DeepLab   V3+&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;通过 encoder-decoder 进行多尺度信息的融合，同时保留了原来的空洞卷积和 ASSP 层，   其骨干网络使用了 Xception 模型，提高了语义分割的健壮性和运行速率&lt;/td&gt;
&lt;td&gt;Cityscape&lt;/td&gt;
&lt;td&gt;Mean IoU=78.81%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-关键点检测" class="anchor" aria-hidden="true" href="#关键点检测"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;关键点检测&lt;/h3&gt;
&lt;p&gt;人体骨骼关键点检测 (Pose Estimation) 主要检测人体的一些关键点，如关节，五官等，通过关键点描述人体骨骼信息。人体骨骼关键点检测对于描述人体姿态，预测人体行为至关重要。是诸多计算机视觉任务的基础，例如动作分类，异常行为检测，以及自动驾驶等等。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型名称&lt;/th&gt;
&lt;th&gt;模型简介&lt;/th&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;th&gt;评估指标&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/human_pose_estimation"&gt;Simple   Baselines&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;coco2018 关键点检测项目亚军方案，网络结构非常简单，效果达到 state of the art&lt;/td&gt;
&lt;td&gt;COCO val2017&lt;/td&gt;
&lt;td&gt;AP =   72.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-图像生成" class="anchor" aria-hidden="true" href="#图像生成"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;图像生成&lt;/h3&gt;
&lt;p&gt;图像生成是指根据输入向量，生成目标图像。这里的输入向量可以是随机的噪声或用户指定的条件向量。具体的应用场景有：手写体生成、人脸合成、风格迁移、图像修复等。&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleGAN"&gt;PaddleGAN&lt;/a&gt; 包含和图像生成相关的多个模型。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型名称&lt;/th&gt;
&lt;th&gt;模型简介&lt;/th&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleGAN"&gt;CGAN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;条件生成对抗网络，一种带条件约束的 GAN，使用额外信息对模型增加条件，可以指导数据生成过程&lt;/td&gt;
&lt;td&gt;Mnist&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleGAN"&gt;DCGAN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;深度卷积生成对抗网络，将 GAN 和卷积网络结合起来，以解决 GAN 训练不稳定的问题&lt;/td&gt;
&lt;td&gt;Mnist&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleGAN"&gt;Pix2Pix&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;图像翻译，通过成对图片将某一类图片转换成另外一类图片，可用于风格迁移&lt;/td&gt;
&lt;td&gt;Cityscapes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleGAN"&gt;CycleGAN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;图像翻译，可以通过非成对的图片将某一类图片转换成另外一类图片，可用于风格迁移&lt;/td&gt;
&lt;td&gt;Cityscapes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleGAN"&gt;StarGAN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;多领域属性迁移，引入辅助分类帮助单个判别器判断多个属性，可用于人脸属性转换&lt;/td&gt;
&lt;td&gt;Celeba&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleGAN"&gt;AttGAN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;利用分类损失和重构损失来保证改变特定的属性，可用于人脸特定属性转换&lt;/td&gt;
&lt;td&gt;Celeba&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleGAN"&gt;STGAN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;人脸特定属性转换，只输入有变化的标签，引入 GRU 结构，更好的选择变化的属性&lt;/td&gt;
&lt;td&gt;Celeba&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleGAN"&gt;SPADE&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;提出一种考虑空间语义信息的归一化方法，从而更好的保留语义信息，生成更为逼真的图像，可用于图像翻译。&lt;/td&gt;
&lt;td&gt;Cityscapes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-场景文字识别" class="anchor" aria-hidden="true" href="#场景文字识别"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;场景文字识别&lt;/h3&gt;
&lt;p&gt;场景文字识别是在图像背景复杂、分辨率低下、字体多样、分布随意等情况下，将图像信息转化为文字序列的过程，可认为是一种特别的翻译过程：将图像输入翻译为自然语言输出。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型名称&lt;/th&gt;
&lt;th&gt;模型简介&lt;/th&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;th&gt;评估指标&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/ocr_recognition"&gt;CRNN-CTC&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;使用 CTC model 识别图片中单行英文字符，用于端到端的文本行图片识别方法&lt;/td&gt;
&lt;td&gt;单行不定长的英文字符串图片&lt;/td&gt;
&lt;td&gt;错误率= 22.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/ocr_recognition"&gt;OCR   Attention&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;使用 attention 识别图片中单行英文字符，用于端到端的自然场景文本识别&lt;/td&gt;
&lt;td&gt;单行不定长的英文字符串图片&lt;/td&gt;
&lt;td&gt;错误率 = 15.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-度量学习" class="anchor" aria-hidden="true" href="#度量学习"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;度量学习&lt;/h3&gt;
&lt;p&gt;度量学习也称作距离度量学习、相似度学习，通过学习对象之间的距离，度量学习能够用于分析对象时间的关联、比较关系，在实际问题中应用较为广泛，可应用于辅助分类、聚类问题，也广泛用于图像检索、人脸识别等领域。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型名称&lt;/th&gt;
&lt;th&gt;模型简介&lt;/th&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;th&gt;评估指标 Recall@Rank-1（使用arcmargin训练）&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/metric_learning"&gt;ResNet50未微调&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;使用 arcmargin loss 训练的特征模型&lt;/td&gt;
&lt;td&gt;Stanford   Online Product(SOP)&lt;/td&gt;
&lt;td&gt;78.11%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/metric_learning"&gt;ResNet50使用triplet微调&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在 arcmargin loss 基础上，使用 triplet loss 微调的特征模型&lt;/td&gt;
&lt;td&gt;Stanford   Online Product(SOP)&lt;/td&gt;
&lt;td&gt;79.21%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/metric_learning"&gt;ResNet50使用quadruplet微调&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在 arcmargin loss 基础上，使用 quadruplet loss 微调的特征模型&lt;/td&gt;
&lt;td&gt;Stanford   Online Product(SOP)&lt;/td&gt;
&lt;td&gt;79.59%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/metric_learning"&gt;ResNet50使用eml微调&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在 arcmargin loss 基础上，使用 eml loss 微调的特征模型&lt;/td&gt;
&lt;td&gt;Stanford   Online Product(SOP)&lt;/td&gt;
&lt;td&gt;80.11%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/metric_learning"&gt;ResNet50使用npairs微调&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;在 arcmargin loss基础上，使用npairs loss 微调的特征模型&lt;/td&gt;
&lt;td&gt;Stanford   Online Product(SOP)&lt;/td&gt;
&lt;td&gt;79.81%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-视频分类和动作定位" class="anchor" aria-hidden="true" href="#视频分类和动作定位"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;视频分类和动作定位&lt;/h3&gt;
&lt;p&gt;视频分类和动作定位是视频理解任务的基础。视频数据包含语音、图像等多种信息，因此理解视频任务不仅需要处理语音和图像，还需要提取视频帧时间序列中的上下文信息。视频分类模型提供了提取全局时序特征的方法，主要方式有卷积神经网络 (C3D, I3D, C2D等)，神经网络和传统图像算法结合 (VLAD 等)，循环神经网络等建模方法。视频动作定位模型需要同时识别视频动作的类别和起止时间点，通常采用类似于图像目标检测中的算法在时间维度上进行建模。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型名称&lt;/th&gt;
&lt;th&gt;模型简介&lt;/th&gt;
&lt;th&gt;数据集&lt;/th&gt;
&lt;th&gt;评估指标&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleVideo"&gt;TSN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ECCV'16 提出的基于 2D-CNN 经典解决方案&lt;/td&gt;
&lt;td&gt;Kinetics-400&lt;/td&gt;
&lt;td&gt;Top-1 = 67%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleVideo"&gt;Non-Local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;视频非局部关联建模模型&lt;/td&gt;
&lt;td&gt;Kinetics-400&lt;/td&gt;
&lt;td&gt;Top-1 = 74%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleVideo"&gt;StNet&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;AAAI'19 提出的视频联合时空建模方法&lt;/td&gt;
&lt;td&gt;Kinetics-400&lt;/td&gt;
&lt;td&gt;Top-1 = 69%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleVideo"&gt;TSM&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;基于时序移位的简单高效视频时空建模方法&lt;/td&gt;
&lt;td&gt;Kinetics-400&lt;/td&gt;
&lt;td&gt;Top-1 = 70%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleVideo"&gt;Attention   LSTM&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;常用模型，速度快精度高&lt;/td&gt;
&lt;td&gt;Youtube-8M&lt;/td&gt;
&lt;td&gt;GAP   = 86%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleVideo"&gt;Attention   Cluster&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;CVPR'18 提出的视频多模态特征注意力聚簇融合方法&lt;/td&gt;
&lt;td&gt;Youtube-8M&lt;/td&gt;
&lt;td&gt;GAP   = 84%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleVideo"&gt;NeXtVlad&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2nd-Youtube-8M 比赛第 3 名的模型&lt;/td&gt;
&lt;td&gt;Youtube-8M&lt;/td&gt;
&lt;td&gt;GAP   = 87%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleVideo"&gt;C-TCN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2018 年 ActivityNet 夺冠方案&lt;/td&gt;
&lt;td&gt;ActivityNet1.3&lt;/td&gt;
&lt;td&gt;MAP=31%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleVideo"&gt;BSN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;为视频动作定位问题提供高效的 proposal 生成方法&lt;/td&gt;
&lt;td&gt;ActivityNet1.3&lt;/td&gt;
&lt;td&gt;AUC=66.64%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleVideo"&gt;BMN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2019 年 ActivityNet 夺冠方案&lt;/td&gt;
&lt;td&gt;ActivityNet1.3&lt;/td&gt;
&lt;td&gt;AUC=67.19%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleVideo/models/ets"&gt;ETS&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;视频摘要生成领域的基准模型&lt;/td&gt;
&lt;td&gt;ActivityNet Captions&lt;/td&gt;
&lt;td&gt;METEOR：10.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/PaddleVideo/models/tall"&gt;TALL&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;视频Grounding方向的BaseLine模型&lt;/td&gt;
&lt;td&gt;TACoS&lt;/td&gt;
&lt;td&gt;R1@IOU5=0.13&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-paddlenlp" class="anchor" aria-hidden="true" href="#paddlenlp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PaddleNLP&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP"&gt;&lt;strong&gt;PaddleNLP&lt;/strong&gt;&lt;/a&gt; 是基于 PaddlePaddle 深度学习框架开发的自然语言处理 (NLP) 工具，算法，模型和数据的开源项目。百度在 NLP 领域十几年的深厚积淀为 PaddleNLP 提供了强大的核心动力。使用 PaddleNLP，您可以得到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;丰富而全面的 NLP 任务支持：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;PaddleNLP 为您提供了多粒度，多场景的应用支持。涵盖了从&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/lexical_analysis"&gt;分词&lt;/a&gt;，&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/lexical_analysis"&gt;词性标注&lt;/a&gt;，&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/lexical_analysis"&gt;命名实体识别&lt;/a&gt;等 NLP 基础技术，到&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/sentiment_classification"&gt;文本分类&lt;/a&gt;，&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/similarity_net"&gt;文本相似度计算&lt;/a&gt;，&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleLARK"&gt;语义表示&lt;/a&gt;，&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleTextGEN"&gt;文本生成&lt;/a&gt;等 NLP 核心技术。同时，PaddleNLP 还提供了针对常见 NLP 大型应用系统（如&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleMRC"&gt;阅读理解&lt;/a&gt;，&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleDialogue"&gt;对话系统&lt;/a&gt;，&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleMT"&gt;机器翻译系统&lt;/a&gt;等）的特定核心技术和工具组件，模型和预训练参数等，让您在 NLP 领域畅通无阻。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;稳定可靠的 NLP 模型和强大的预训练参数：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;PaddleNLP集成了百度内部广泛使用的 NLP 工具模型，为您提供了稳定可靠的 NLP 算法解决方案。基于百亿级数据的预训练参数和丰富的预训练模型，助您轻松提高模型效果，为您的 NLP 业务注入强大动力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;持续改进和技术支持，零基础搭建 NLP 应用：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;PaddleNLP 为您提供持续的技术支持和模型算法更新，为您的 NLP 业务保驾护航。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-nlp-基础技术" class="anchor" aria-hidden="true" href="#nlp-基础技术"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP 基础技术&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;任务类型&lt;/th&gt;
&lt;th&gt;目录&lt;/th&gt;
&lt;th&gt;简介&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;中文词法分析&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/lexical_analysis"&gt;LAC(Lexical Analysis of Chinese)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;百度自主研发中文特色模型词法分析任务，集成了中文分词、词性标注和命名实体识别任务。输入是一个字符串，而输出是句子中的词边界和词性、实体类别。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;词向量&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/word2vec"&gt;Word2vec&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;提供单机多卡，多机等分布式训练中文词向量能力，支持主流词向量模型（skip-gram，cbow等），可以快速使用自定义数据训练词向量模型。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;语言模型&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/language_model"&gt;Language_model&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;给定一个输入词序列（中文需要先分词、英文需要先 tokenize），计算其生成概率。 语言模型的评价指标 PPL(困惑度)，用于表示模型生成句子的流利程度。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-nlp-核心技术" class="anchor" aria-hidden="true" href="#nlp-核心技术"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP 核心技术&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-语义表示" class="anchor" aria-hidden="true" href="#语义表示"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;语义表示&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleLARK"&gt;PaddleLARK&lt;/a&gt; (Paddle LAngauge Representation ToolKit) 是传统语言模型的进一步发展，通过在大规模语料上训练得到的通用的语义表示模型，可以助益其他自然语言处理任务，是通用预训练 + 特定任务精调范式的体现。PaddleLARK 集成了 ELMO，BERT，ERNIE 1.0，ERNIE 2.0，XLNet 等热门中英文预训练模型。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;th&gt;简介&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/ERNIE"&gt;ERNIE&lt;/a&gt;(Enhanced Representation from kNowledge IntEgration)&lt;/td&gt;
&lt;td&gt;百度自研的语义表示模型，通过建模海量数据中的词、实体及实体关系，学习真实世界的语义知识。相较于 BERT 学习原始语言信号，ERNIE 直接对先验语义知识单元进行建模，增强了模型语义表示能力。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleLARK/BERT"&gt;BERT&lt;/a&gt;(Bidirectional Encoder Representation from Transformers)&lt;/td&gt;
&lt;td&gt;一个迁移能力很强的通用语义表示模型， 以 Transformer 为网络基本组件，以双向 Masked Language Model和 Next Sentence Prediction 为训练目标，通过预训练得到通用语义表示，再结合简单的输出层，应用到下游的 NLP 任务，在多个任务上取得了 SOTA 的结果。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleLARK/XLNet"&gt;XLNet&lt;/a&gt;(XLNet: Generalized Autoregressive Pretraining for Language Understanding)&lt;/td&gt;
&lt;td&gt;重要的语义表示模型之一，引入 Transformer-XL 为骨架，以 Permutation Language Modeling 为优化目标，在若干下游任务上优于 BERT 的性能。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleLARK/ELMo"&gt;ELMo&lt;/a&gt;(Embeddings from Language Models)&lt;/td&gt;
&lt;td&gt;重要的通用语义表示模型之一，以双向 LSTM 为网路基本组件，以 Language Model 为训练目标，通过预训练得到通用的语义表示，将通用的语义表示作为 Feature 迁移到下游 NLP 任务中，会显著提升下游任务的模型性能。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-文本相似度计算" class="anchor" aria-hidden="true" href="#文本相似度计算"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;文本相似度计算&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/similarity_net"&gt;SimNet&lt;/a&gt; (Similarity Net) 是一个计算短文本相似度的框架，主要包括 BOW、CNN、RNN、MMDNN 等核心网络结构形式。SimNet 框架在百度各产品上广泛应用，提供语义相似度计算训练和预测框架，适用于信息检索、新闻推荐、智能客服等多个应用场景，帮助企业解决语义匹配问题。&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-文本生成" class="anchor" aria-hidden="true" href="#文本生成"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;文本生成&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleTextGEN"&gt;PaddleTextGEN&lt;/a&gt; (Paddle Text Generation) ,一个基于 PaddlePaddle 的文本生成框架，提供了一些列经典文本生成模型案例，如 vanilla seq2seq，seq2seq with attention，variational seq2seq 模型等。&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-nlp-系统应用" class="anchor" aria-hidden="true" href="#nlp-系统应用"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NLP 系统应用&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-情感分析" class="anchor" aria-hidden="true" href="#情感分析"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;情感分析&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;th&gt;简介&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/sentiment_classification"&gt;Senta&lt;/a&gt; (Sentiment Classification，简称Senta)&lt;/td&gt;
&lt;td&gt;面向&lt;strong&gt;通用场景&lt;/strong&gt;的情感分类模型，针对带有主观描述的中文文本，可自动判断该文本的情感极性类别。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/emotion_detection"&gt;EmotionDetection&lt;/a&gt; (Emotion Detection，简称EmoTect)&lt;/td&gt;
&lt;td&gt;专注于识别&lt;strong&gt;人机对话场景&lt;/strong&gt;中用户的情绪，针对智能对话场景中的用户文本，自动判断该文本的情绪类别。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-阅读理解" class="anchor" aria-hidden="true" href="#阅读理解"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;阅读理解&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleMRC"&gt;PaddleMRC&lt;/a&gt; (Paddle Machine Reading Comprehension)，集合了百度在阅读理解领域相关的模型，工具，开源数据集等一系列工作。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;th&gt;简介&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/Research/ACL2018-DuReader"&gt;DuReader&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;包含百度开源的基于真实搜索用户行为的中文大规模阅读理解数据集以及基线模型。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/Research/ACL2019-KTNET"&gt;KT-Net&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;结合知识的阅读理解模型，Squad 曾排名第一。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/Research/MRQA2019-D-NET"&gt;D-Net&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;阅读理解十项全能模型，在 EMNLP2019 国际阅读理解大赛夺得 10 项冠军。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;&lt;a id="user-content-机器翻译" class="anchor" aria-hidden="true" href="#机器翻译"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;机器翻译&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleMT"&gt;PaddleMT&lt;/a&gt; ，全称为Paddle Machine Translation，基于Transformer的经典机器翻译模型，基于论文 &lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Attention Is All You Need&lt;/a&gt;。&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-对话系统" class="anchor" aria-hidden="true" href="#对话系统"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;对话系统&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleDialogue"&gt;PaddleDialogue&lt;/a&gt; 包含对话系统方向的模型、数据集和工具。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;th&gt;简介&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleDialogue/dialogue_general_understanding"&gt;DGU&lt;/a&gt; (Dialogue General Understanding，通用对话理解模型)&lt;/td&gt;
&lt;td&gt;覆盖了包括&lt;strong&gt;检索式聊天系统&lt;/strong&gt;中 context-response matching 任务和&lt;strong&gt;任务完成型对话系统&lt;/strong&gt;中&lt;strong&gt;意图识别&lt;/strong&gt;，&lt;strong&gt;槽位解析&lt;/strong&gt;，&lt;strong&gt;状态追踪&lt;/strong&gt;等常见对话系统任务，在 6 项国际公开数据集中都获得了最佳效果。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/PaddleDialogue/auto_dialogue_evaluation"&gt;ADEM&lt;/a&gt; (Auto Dialogue Evaluation Model)&lt;/td&gt;
&lt;td&gt;评估开放领域对话系统的回复质量，能够帮助企业或个人快速评估对话系统的回复质量，减少人工评估成本。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/Research/ACL2019-DuConv"&gt;Proactive Conversation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;包含百度开源的知识驱动的开放领域对话数据集 &lt;a href="https://ai.baidu.com/broad/subordinate?dataset=duconv" rel="nofollow"&gt;DuConv&lt;/a&gt;，以及基线模型。对应论文 &lt;a href="https://arxiv.org/abs/1906.05572" rel="nofollow"&gt;Proactive Human-Machine Conversation with Explicit Conversation Goals&lt;/a&gt; 发表于 ACL2019。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/Research/ACL2018-DAM"&gt;DAM&lt;/a&gt;（Deep Attention Matching Network，深度注意力机制模型）&lt;/td&gt;
&lt;td&gt;开放领域多轮对话匹配模型，对应论文 &lt;a href="https://aclweb.org/anthology/P18-1103/" rel="nofollow"&gt;Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network&lt;/a&gt; 发表于 ACL2018。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;百度最新前沿工作开源，请参考 &lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/Research"&gt;Research&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-paddlerec" class="anchor" aria-hidden="true" href="#paddlerec"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PaddleRec&lt;/h2&gt;
&lt;p&gt;个性化推荐，在当前的互联网服务中正在发挥越来越大的作用，目前大部分电子商务系统、社交网络，广告推荐，搜索引擎，都不同程度的使用了各种形式的个性化推荐技术，帮助用户快速找到他们想要的信息。&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleRec"&gt;PaddleRec&lt;/a&gt; 包含的模型如下。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;模型&lt;/th&gt;
&lt;th align="left"&gt;应用场景&lt;/th&gt;
&lt;th align="left"&gt;简介&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/gru4rec"&gt;GRU4Rec&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;Session-based 推荐, 图网络推荐&lt;/td&gt;
&lt;td align="left"&gt;首次将 RNN（GRU）运用于 session-based 推荐，核心思想是在一个 session 中，用户点击一系列item的行为看做一个序列，用来训练 RNN 模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/tagspace"&gt;TagSpace&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;标签推荐&lt;/td&gt;
&lt;td align="left"&gt;Tagspace 模型学习文本及标签的 embedding 表示，应用于工业级的标签推荐，具体应用场景有 feed 新闻标签推荐。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/ssr"&gt;SequenceSemanticRetrieval&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;召回&lt;/td&gt;
&lt;td align="left"&gt;解决了 GRU4Rec 模型无法预测训练数据集中不存在的项目，比如新闻推荐的问题。它由两个部分组成：一个是匹配模型部分，另一个是检索部分&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/word2vec"&gt;Word2Vec&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;词向量&lt;/td&gt;
&lt;td align="left"&gt;训练得到词的向量表示、广泛应用于 NLP 、推荐等任务场景。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/multiview_simnet"&gt;Multiview-Simnet&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;排序&lt;/td&gt;
&lt;td align="left"&gt;多视角Simnet模型是可以融合用户以及推荐项目的多个视角的特征并进行个性化匹配学习的一体化模型。这类模型在很多工业化的场景中都会被使用到，比如百度的 Feed 产品中&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/gnn"&gt;GraphNeuralNetwork&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;召回&lt;/td&gt;
&lt;td align="left"&gt;SR-GNN，全称为 Session-based Recommendations with Graph Neural Network（GNN）。使用 GNN 进行会话序列建模。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/din"&gt;DeepInterestNetwork&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;排序&lt;/td&gt;
&lt;td align="left"&gt;DIN，全称为 Deep Interest Network。特点为对历史序列建模的过程中结合了预估目标的信息。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/ctr/deepfm"&gt;DeepFM&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;推荐系统&lt;/td&gt;
&lt;td align="left"&gt;DeepFM，全称 Factorization-Machine based Neural Network。经典的 CTR 推荐算法，网络由DNN和FM两部分组成。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/ctr/dcn"&gt;DCN&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;推荐系统&lt;/td&gt;
&lt;td align="left"&gt;全称 Deep &amp;amp; Cross Network。提出一种新的交叉网络（cross network），在每个层上明确地应用特征交叉。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleRec/ctr/xdeepfm"&gt;XDeepFM&lt;/a&gt;&lt;/td&gt;
&lt;td align="left"&gt;推荐系统&lt;/td&gt;
&lt;td align="left"&gt;xDeepFM，全称 extreme Factorization Machine。对 DeepFM 和 DCN 的改进，提出 CIN（Compressed Interaction Network），使用 vector-wise 等级的显示特征交叉。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-paddlespeech" class="anchor" aria-hidden="true" href="#paddlespeech"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PaddleSpeech&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleSpeech"&gt;PaddleSpeech&lt;/a&gt; 包含语音识别和语音合成相关的模型。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型&lt;/th&gt;
&lt;th&gt;简介&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/blob/develop/PaddleSpeech/DeepASR/README_cn.md"&gt;DeepASR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;利用 PaddlePaddle 框架完成语音识别中声学模型的配置和训练，并集成 Kaldi 的解码器。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/DeepSpeech"&gt;DeepSpeech2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;一个采用 PaddlePaddle 平台的端到端自动语音识别（ASR）引擎的开源项目，具体原理请参考论文 &lt;a href="https://arxiv.org/abs/1512.02595" rel="nofollow"&gt;Deep Speech 2: End-to-End Speech Recognition in English and Mandarin&lt;/a&gt;。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleSpeech/DeepVoice3"&gt;DeepVoice3&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;百度研发的基于卷积神经网络的端到端语音合成模型，对应论文 &lt;a href="https://arxiv.org/abs/1710.07654" rel="nofollow"&gt;Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning&lt;/a&gt;， 基于 PaddlePaddle 动态图实现。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-paddleslim" class="anchor" aria-hidden="true" href="#paddleslim"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PaddleSlim&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleSlim"&gt;PaddleSlim&lt;/a&gt; 是 PaddlePaddle 框架的一个子模块，主要用于压缩图像领域模型。在 PaddleSlim 中，不仅实现了目前主流的网络剪枝、量化、蒸馏三种压缩策略，还实现了超参数搜索和小模型网络结构搜索功能。在后续版本中，会添加更多的压缩策略，以及完善对 NLP 领域模型的支持。&lt;/p&gt;
&lt;p&gt;PaddleSlim 模型压缩工具库的实验结果和模型库见 &lt;a href="https://github.com/PaddlePaddle/models/blob/develop/PaddleSlim/docs/model_zoo.md"&gt;详细实验结果与ModelZoo&lt;/a&gt;。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-基于动态图实现的模型" class="anchor" aria-hidden="true" href="#基于动态图实现的模型"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;基于动态图实现的模型&lt;/h2&gt;
&lt;p&gt;自 PaddlePaddle fluid 1.5 版本正式支持动态图模式以来，模型库新增若干基于动态图实现的模型，请参考 &lt;a href="https://github.com/PaddlePaddle/models/blob/develop/dygraph/"&gt;dygraph&lt;/a&gt;，这些模型可以作为了解和使用 PaddlePaddle 动态图模式的示例。目前 PaddlePaddle 的动态图功能正在活跃开发中，API 可能发生变动，欢迎用户试用并给我们反馈。&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-其他模型" class="anchor" aria-hidden="true" href="#其他模型"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;其他模型&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型名称&lt;/th&gt;
&lt;th&gt;模型简介&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/blob/develop/legacy/PaddleRL/DeepQNetwork/README_cn.md"&gt;DQN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;value-based 强化学习算法，第一个成功地将深度学习和强化学习结合起来的模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/blob/develop/legacy/PaddleRL/DeepQNetwork/README_cn.md"&gt;DoubleDQN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;将 Double Q 的想法应用在 DQN 上，解决过优化问题&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/blob/develop/legacy/PaddleRL/DeepQNetwork/README_cn.md"&gt;DuelingDQN&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;改进了 DQN 模型，提高了模型的性能&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleST/Research/CIKM2019-MONOPOLY"&gt;CIKM2019-MONOPOLY&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Monopoly 是一个实用的 POI 商业智能算法，能够利用少量的房产价格，对大量其他的固定资产进行价值估计。 该算法全面适配 MapReduce 的分布式计算框架，能够快速用于工业部署。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-快速下载模型库" class="anchor" aria-hidden="true" href="#快速下载模型库"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;快速下载模型库&lt;/h2&gt;
&lt;p&gt;由于 github 在国内的下载速度不稳定，我们提供了 models 各版本压缩包的百度云下载地址，以便用户更快速地获取代码。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;版本号&lt;/th&gt;
&lt;th&gt;tar包&lt;/th&gt;
&lt;th&gt;zip包&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;models 1.5.1&lt;/td&gt;
&lt;td&gt;&lt;a href="https://paddlepaddle-modles.bj.bcebos.com/models-1.5.1.tar.gz" rel="nofollow"&gt;https://paddlepaddle-modles.bj.bcebos.com/models-1.5.1.tar.gz&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://paddlepaddle-modles.bj.bcebos.com/models-1.5.1.zip" rel="nofollow"&gt;https://paddlepaddle-modles.bj.bcebos.com/models-1.5.1.zip&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;models 1.5&lt;/td&gt;
&lt;td&gt;&lt;a href="https://paddlepaddle-modles.bj.bcebos.com/models-1.5.tar.gz" rel="nofollow"&gt;https://paddlepaddle-modles.bj.bcebos.com/models-1.5.tar.gz&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://paddlepaddle-modles.bj.bcebos.com/models-1.5.zip" rel="nofollow"&gt;https://paddlepaddle-modles.bj.bcebos.com/models-1.5.zip&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;models 1.4&lt;/td&gt;
&lt;td&gt;&lt;a href="https://paddlepaddle-modles.bj.bcebos.com/models-1.4.tar.gz" rel="nofollow"&gt;https://paddlepaddle-modles.bj.bcebos.com/models-1.4.tar.gz&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://paddlepaddle-modles.bj.bcebos.com/models-1.4.zip" rel="nofollow"&gt;https://paddlepaddle-modles.bj.bcebos.com/models-1.4.zip&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;models 1.3&lt;/td&gt;
&lt;td&gt;&lt;a href="https://paddlepaddle-modles.bj.bcebos.com/models-1.3.tar.gz" rel="nofollow"&gt;https://paddlepaddle-modles.bj.bcebos.com/models-1.3.tar.gz&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://paddlepaddle-modles.bj.bcebos.com/models-1.3.zip" rel="nofollow"&gt;https://paddlepaddle-modles.bj.bcebos.com/models-1.3.zip&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This tutorial is contributed by &lt;a href="https://github.com/PaddlePaddle/Paddle"&gt;PaddlePaddle&lt;/a&gt; and licensed under the &lt;a href="LICENSE"&gt;Apache-2.0 license&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-许可证书" class="anchor" aria-hidden="true" href="#许可证书"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;许可证书&lt;/h2&gt;
&lt;p&gt;此向导由&lt;a href="https://github.com/PaddlePaddle/Paddle"&gt;PaddlePaddle&lt;/a&gt;贡献，受&lt;a href="LICENSE"&gt;Apache-2.0 license&lt;/a&gt;许可认证。&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>PaddlePaddle</author><guid isPermaLink="false">https://github.com/PaddlePaddle/models</guid><pubDate>Mon, 11 Nov 2019 00:24:00 GMT</pubDate></item><item><title>zzh8829/yolov3-tf2 #25 in Python, This month</title><link>https://github.com/zzh8829/yolov3-tf2</link><description>&lt;p&gt;&lt;i&gt;YoloV3 Implemented in Tensorflow 2.0&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-yolov3-implemented-in-tensorflow-20" class="anchor" aria-hidden="true" href="#yolov3-implemented-in-tensorflow-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;YoloV3 Implemented in TensorFlow 2.0&lt;/h1&gt;
&lt;p&gt;This repo provides a clean implementation of YoloV3 in TensorFlow 2.0 using all the best practices.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-key-features" class="anchor" aria-hidden="true" href="#key-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Key Features&lt;/h2&gt;
&lt;ul class="contains-task-list"&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; TensorFlow 2.0&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; &lt;code&gt;yolov3&lt;/code&gt; with pre-trained Weights&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; &lt;code&gt;yolov3-tiny&lt;/code&gt; with pre-trained Weights&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Inference example&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Transfer learning example&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Eager mode training with &lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Graph mode training with &lt;code&gt;model.fit&lt;/code&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Functional model with &lt;code&gt;tf.keras.layers&lt;/code&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Input pipeline using &lt;code&gt;tf.data&lt;/code&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Tensorflow Serving&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Vectorized transformations&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; GPU accelerated&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Fully integrated with &lt;code&gt;absl-py&lt;/code&gt; from &lt;a href="https://abseil.io" rel="nofollow"&gt;abseil.io&lt;/a&gt;&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Clean implementation&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Following the best practices&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; MIT License&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zzh8829/yolov3-tf2/master/data/meme_out.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/zzh8829/yolov3-tf2/master/data/meme_out.jpg" alt="demo" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/zzh8829/yolov3-tf2/master/data/street_out.jpg"&gt;&lt;img src="https://raw.githubusercontent.com/zzh8829/yolov3-tf2/master/data/street_out.jpg" alt="demo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-conda-recommended" class="anchor" aria-hidden="true" href="#conda-recommended"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conda (Recommended)&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Tensorflow CPU&lt;/span&gt;
conda env create -f conda-cpu.yml
conda activate yolov3-tf2-cpu

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Tensorflow GPU&lt;/span&gt;
conda env create -f conda-gpu.yml
conda activate yolov3-tf2-gpu&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;&lt;a id="user-content-pip" class="anchor" aria-hidden="true" href="#pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pip&lt;/h4&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-nvidia-driver-for-gpu" class="anchor" aria-hidden="true" href="#nvidia-driver-for-gpu"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Nvidia Driver (For GPU)&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Ubuntu 18.04&lt;/span&gt;
sudo apt-add-repository -r ppa:graphics-drivers/ppa
sudo apt install nvidia-driver-430
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Windows/Other&lt;/span&gt;
https://www.nvidia.com/Download/index.aspx&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-convert-pre-trained-darknet-weights" class="anchor" aria-hidden="true" href="#convert-pre-trained-darknet-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convert pre-trained Darknet weights&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; yolov3&lt;/span&gt;
wget https://pjreddie.com/media/files/yolov3.weights -O data/yolov3.weights
python convert.py

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; yolov3-tiny&lt;/span&gt;
wget https://pjreddie.com/media/files/yolov3-tiny.weights -O data/yolov3-tiny.weights
python convert.py --weights ./data/yolov3-tiny.weights --output ./checkpoints/yolov3-tiny.tf --tiny&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-detection" class="anchor" aria-hidden="true" href="#detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Detection&lt;/h3&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; yolov3&lt;/span&gt;
python detect.py --image ./data/meme.jpg

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; yolov3-tiny&lt;/span&gt;
python detect.py --weights ./checkpoints/yolov3-tiny.tf --tiny --image ./data/street.jpg

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; webcam&lt;/span&gt;
python detect_video.py --video 0

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; video file&lt;/span&gt;
python detect_video.py --video path_to_file.mp4 --weights ./checkpoints/yolov3-tiny.tf --tiny&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h3&gt;
&lt;p&gt;You need to generate tfrecord following the TensorFlow Object Detection API.
For example you can use &lt;a href="https://github.com/Microsoft/VoTT"&gt;Microsoft VOTT&lt;/a&gt; to generate such dataset.
You can also use this &lt;a href="https://github.com/tensorflow/models/blob/master/research/object_detection/dataset_tools/create_pascal_tf_record.py"&gt;script&lt;/a&gt; to create the pascal voc dataset.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python train.py --batch_size 8 --dataset &lt;span class="pl-k"&gt;~&lt;/span&gt;/Data/voc2012.tfrecord --val_dataset &lt;span class="pl-k"&gt;~&lt;/span&gt;/Data/voc2012_val.tfrecord --epochs 100 --mode eager_tf --transfer fine_tune

python train.py --batch_size 8 --dataset &lt;span class="pl-k"&gt;~&lt;/span&gt;/Data/voc2012.tfrecord --val_dataset &lt;span class="pl-k"&gt;~&lt;/span&gt;/Data/voc2012_val.tfrecord --epochs 100 --mode fit --transfer none

python train.py --batch_size 8 --dataset &lt;span class="pl-k"&gt;~&lt;/span&gt;/Data/voc2012.tfrecord --val_dataset &lt;span class="pl-k"&gt;~&lt;/span&gt;/Data/voc2012_val.tfrecord --epochs 100 --mode fit --transfer no_output

python train.py --batch_size 8 --dataset &lt;span class="pl-k"&gt;~&lt;/span&gt;/Data/voc2012.tfrecord --val_dataset &lt;span class="pl-k"&gt;~&lt;/span&gt;/Data/voc2012_val.tfrecord --epochs 10 --mode eager_fit --transfer fine_tune --weights ./checkpoints/yolov3-tiny.tf --tiny&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-tensorflow-serving" class="anchor" aria-hidden="true" href="#tensorflow-serving"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tensorflow Serving&lt;/h3&gt;
&lt;p&gt;You can export the model to tf serving&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python export_tfserving.py --output serving/yolov3/1/
# verify tfserving graph
saved_model_cli show --dir serving/yolov3/1/ --tag_set serve --signature_def serving_default
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The inputs are preprocessed images (see &lt;code&gt;dataset.transform_iamges&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;outputs are&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yolo_nms_0: bounding boxes
yolo_nms_1: scores
yolo_nms_2: classes
yolo_nms_3: numbers of valid detections
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-benchmark-no-training-yet" class="anchor" aria-hidden="true" href="#benchmark-no-training-yet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmark (No Training Yet)&lt;/h2&gt;
&lt;p&gt;Numbers are obtained with rough calculations from &lt;code&gt;detect_video.py&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-macbook-pro-13-27ghz-i5" class="anchor" aria-hidden="true" href="#macbook-pro-13-27ghz-i5"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Macbook Pro 13 (2.7GHz i5)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Detection&lt;/th&gt;
&lt;th&gt;416x416&lt;/th&gt;
&lt;th&gt;320x320&lt;/th&gt;
&lt;th&gt;608x608&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;YoloV3&lt;/td&gt;
&lt;td&gt;1000ms&lt;/td&gt;
&lt;td&gt;500ms&lt;/td&gt;
&lt;td&gt;1546ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YoloV3-Tiny&lt;/td&gt;
&lt;td&gt;100ms&lt;/td&gt;
&lt;td&gt;58ms&lt;/td&gt;
&lt;td&gt;208ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-desktop-pc-gtx-970" class="anchor" aria-hidden="true" href="#desktop-pc-gtx-970"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Desktop PC (GTX 970)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Detection&lt;/th&gt;
&lt;th&gt;416x416&lt;/th&gt;
&lt;th&gt;320x320&lt;/th&gt;
&lt;th&gt;608x608&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;YoloV3&lt;/td&gt;
&lt;td&gt;74ms&lt;/td&gt;
&lt;td&gt;57ms&lt;/td&gt;
&lt;td&gt;129ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YoloV3-Tiny&lt;/td&gt;
&lt;td&gt;18ms&lt;/td&gt;
&lt;td&gt;15ms&lt;/td&gt;
&lt;td&gt;28ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-aws-g34xlarge-tesla-m60" class="anchor" aria-hidden="true" href="#aws-g34xlarge-tesla-m60"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;AWS g3.4xlarge (Tesla M60)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Detection&lt;/th&gt;
&lt;th&gt;416x416&lt;/th&gt;
&lt;th&gt;320x320&lt;/th&gt;
&lt;th&gt;608x608&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;YoloV3&lt;/td&gt;
&lt;td&gt;66ms&lt;/td&gt;
&lt;td&gt;50ms&lt;/td&gt;
&lt;td&gt;123ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YoloV3-Tiny&lt;/td&gt;
&lt;td&gt;15ms&lt;/td&gt;
&lt;td&gt;10ms&lt;/td&gt;
&lt;td&gt;24ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Darknet version of YoloV3 at 416x416 takes 29ms on Titan X.
Considering Titan X has about double the benchmark of Tesla M60,
Performance-wise this implementation is pretty comparable.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-implementation-details" class="anchor" aria-hidden="true" href="#implementation-details"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementation Details&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-eager-execution" class="anchor" aria-hidden="true" href="#eager-execution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eager execution&lt;/h3&gt;
&lt;p&gt;Great addition for existing TensorFlow experts.
Not very easy to use without some intermediate understanding of TensorFlow graphs.
It is annoying when you accidentally use incompatible features like tensor.shape[0]
or some sort of python control flow that works fine in eager mode, but
totally breaks down when you try to compile the model to graph.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-modelx-vs-modelpredictx" class="anchor" aria-hidden="true" href="#modelx-vs-modelpredictx"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;model(x) vs. model.predict(x)&lt;/h3&gt;
&lt;p&gt;When calling model(x) directly, we are executing the graph in eager mode. For
&lt;code&gt;model.predict&lt;/code&gt;, tf actually compiles the graph on the first run and then
execute in graph mode. So if you are only running the model once, &lt;code&gt;model(x)&lt;/code&gt; is
faster since there is no compilation needed. Otherwise, &lt;code&gt;model.predict&lt;/code&gt; or
using exported SavedModel graph is much faster (by 2x).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-gradienttape" class="anchor" aria-hidden="true" href="#gradienttape"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GradientTape&lt;/h3&gt;
&lt;p&gt;Extremely useful for debugging purpose, you can set breakpoints anywhere.
You can compile all the keras fitting functionalities with gradient tape using the
&lt;code&gt;run_eagerly&lt;/code&gt; argument in model.compile. From my limited testing, all training methods
including GradientTape, keras.fit, eager or not yeilds similar performance. But graph
mode is still preferred since it's a tiny bit more efficient.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-tffunction" class="anchor" aria-hidden="true" href="#tffunction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;@tf.function&lt;/h3&gt;
&lt;p&gt;@tf.function is very cool. It's like an in-between version of eager and graph.
You can step through the function by disabling tf.function and then gain
performance when you enable it in production. Important note, you should not
pass any non-tensor parameter to @tf.function, it will cause re-compilation
on every call. I am not sure whats the best way other than using globals.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-abslpy-abseil" class="anchor" aria-hidden="true" href="#abslpy-abseil"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;absl.py (abseil)&lt;/h3&gt;
&lt;p&gt;Absolutely amazing. If you don't know already, absl.py is officially used by
internal projects at Google. It standardizes application interface for Python
and many other languages. After using it within Google, I was so excited
to hear abseil going open source. It includes many decades of best practices
learned from creating large size scalable applications. I literally have
nothing bad to say about it, strongly recommend absl.py to everybody.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-loading-pre-trained-darknet-weights" class="anchor" aria-hidden="true" href="#loading-pre-trained-darknet-weights"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Loading pre-trained Darknet weights&lt;/h3&gt;
&lt;p&gt;very hard with pure functional API because the layer ordering is different in
tf.keras and darknet. The clean solution here is creating sub-models in keras.
Keras is not able to save nested model in h5 format properly, TF Checkpoint is
recommended since its offically supported by TensorFlow.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-tfkeraslayersbatchnormalization" class="anchor" aria-hidden="true" href="#tfkeraslayersbatchnormalization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;tf.keras.layers.BatchNormalization&lt;/h3&gt;
&lt;p&gt;It doesn't work very well for transfer learning. There are many articles and
github issues all over the internet. I used a simple hack to make it work nicer
on transfer learning with small batches.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-what-is-the-output-of-transform_targets-" class="anchor" aria-hidden="true" href="#what-is-the-output-of-transform_targets-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is the output of transform_targets ???&lt;/h3&gt;
&lt;p&gt;I know it's very confusion but the output is tuple of shape&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(
  [N, 13, 13, 3, 6],
  [N, 26, 26, 3, 6],
  [N, 52, 52, 3, 6]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where N is the number of labels in batch and the last dimension "6" represents
&lt;code&gt;[x, y, w, h, obj, class]&lt;/code&gt; of the bounding boxes.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-iou-and-score-threshold" class="anchor" aria-hidden="true" href="#iou-and-score-threshold"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;IOU and Score Threshold&lt;/h3&gt;
&lt;p&gt;the default threshold is 0.5 for both IOU and score, you can adjust them
according to your need by setting &lt;code&gt;--yolo_iou_threshold&lt;/code&gt; and
&lt;code&gt;--yolo_score_threshold&lt;/code&gt; flags&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-command-line-args-reference" class="anchor" aria-hidden="true" href="#command-line-args-reference"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Command Line Args Reference&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;convert.py:
  --output: path to output
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./checkpoints/yolov3.tf&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --[no]tiny: yolov3 or yolov3-tiny
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;false&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --weights: path to weights file
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./data/yolov3.weights&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --num_classes: number of classes &lt;span class="pl-k"&gt;in&lt;/span&gt; the model
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;80&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (an integer)

detect.py:
  --classes: path to classes file
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./data/coco.names&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --image: path to input image
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./data/girl.png&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --output: path to output image
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./output.jpg&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --[no]tiny: yolov3 or yolov3-tiny
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;false&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --weights: path to weights file
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./checkpoints/yolov3.tf&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --num_classes: number of classes &lt;span class="pl-k"&gt;in&lt;/span&gt; the model
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;80&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (an integer)

train.py:
  --batch_size: batch size
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;8&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (an integer)
  --classes: path to classes file
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./data/coco.names&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --dataset: path to dataset
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --epochs: number of epochs
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;2&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (an integer)
  --learning_rate: learning rate
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;0.001&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (a number)
  --mode: &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;fit&lt;span class="pl-k"&gt;|&lt;/span&gt;eager_fit&lt;span class="pl-k"&gt;|&lt;/span&gt;eager_tf&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt;: fit: model.fit, eager_fit: model.fit(run_eagerly=True), eager_tf: custom GradientTape
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;fit&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --num_classes: number of classes &lt;span class="pl-k"&gt;in&lt;/span&gt; the model
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;80&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (an integer)
  --size: image size
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;416&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
    (an integer)
  --[no]tiny: yolov3 or yolov3-tiny
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;false&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --transfer: &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;none&lt;span class="pl-k"&gt;|&lt;/span&gt;darknet&lt;span class="pl-k"&gt;|&lt;/span&gt;no_output&lt;span class="pl-k"&gt;|&lt;/span&gt;frozen&lt;span class="pl-k"&gt;|&lt;/span&gt;fine_tune&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt;: none: Training from scratch, darknet: Transfer darknet, no_output: Transfer all but output, frozen: Transfer and freeze all,
    fine_tune: Transfer all and freeze darknet only
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;none&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --val_dataset: path to validation dataset
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
  --weights: path to weights file
    (default: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./checkpoints/yolov3.tf&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-change-log" class="anchor" aria-hidden="true" href="#change-log"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Change Log&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-october-1-2019" class="anchor" aria-hidden="true" href="#october-1-2019"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;October 1, 2019&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Updated to Tensorflow to v2.0.0 Release&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;p&gt;It is pretty much impossible to implement this from the yolov3 paper alone. I had to reference the official (very hard to understand) and many un-official (many minor errors) repos to piece together the complete picture.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/pjreddie/darknet"&gt;https://github.com/pjreddie/darknet&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;official yolov3 implementation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/AlexeyAB"&gt;https://github.com/AlexeyAB&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;explinations of parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/qqwweee/keras-yolo3"&gt;https://github.com/qqwweee/keras-yolo3&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;models&lt;/li&gt;
&lt;li&gt;loss functions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/YunYang1994/tensorflow-yolov3"&gt;https://github.com/YunYang1994/tensorflow-yolov3&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;data transformations&lt;/li&gt;
&lt;li&gt;loss functions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ayooshkathuria/pytorch-yolo-v3"&gt;https://github.com/ayooshkathuria/pytorch-yolo-v3&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/broadinstitute/keras-resnet"&gt;https://github.com/broadinstitute/keras-resnet&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;batch normalization fix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>zzh8829</author><guid isPermaLink="false">https://github.com/zzh8829/yolov3-tf2</guid><pubDate>Mon, 11 Nov 2019 00:25:00 GMT</pubDate></item></channel></rss>