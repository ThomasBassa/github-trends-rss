<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: Python, This month</title><link>https://github.com/trending/python?since=monthly</link><description>The top repositories on GitHub for python, measured monthly</description><pubDate>Tue, 19 Nov 2019 01:07:03 GMT</pubDate><lastBuildDate>Tue, 19 Nov 2019 01:07:03 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>openai/gpt-2 #1 in Python, This month</title><link>https://github.com/openai/gpt-2</link><description>&lt;p&gt;&lt;i&gt;Code for the paper "Language Models are Unsupervised Multitask Learners"&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Archive (code is provided as-is, no updates expected)&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-gpt-2" class="anchor" aria-hidden="true" href="#gpt-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;gpt-2&lt;/h1&gt;
&lt;p&gt;Code and models from the paper &lt;a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" rel="nofollow"&gt;"Language Models are Unsupervised Multitask Learners"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can read about GPT-2 and its staged release in our &lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;original blog post&lt;/a&gt;, &lt;a href="https://openai.com/blog/gpt-2-6-month-follow-up/" rel="nofollow"&gt;6 month follow-up post&lt;/a&gt;, and &lt;a href="https://www.openai.com/blog/gpt-2-1-5b-release/" rel="nofollow"&gt;final post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We have also &lt;a href="https://github.com/openai/gpt-2-output-dataset"&gt;released a dataset&lt;/a&gt; for researchers to study their behaviors.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; &lt;em&gt;Note that our original parameter counts were wrong due to an error (in our previous blog posts and paper).  Thus you may have seen small referred to as 117M and medium referred to as 345M.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;This repository is meant to be a starting point for researchers and engineers to experiment with GPT-2.&lt;/p&gt;
&lt;p&gt;For basic information, see our &lt;a href="./model_card.md"&gt;model card&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-some-caveats" class="anchor" aria-hidden="true" href="#some-caveats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Some caveats&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GPT-2 models' robustness and worst case behaviors are not well-understood.  As with any machine-learned model, carefully evaluate GPT-2 for your use case, especially if used without fine-tuning or in safety-critical applications where reliability is important.&lt;/li&gt;
&lt;li&gt;The dataset our GPT-2 models were trained on contains many texts with &lt;a href="https://twitter.com/TomerUllman/status/1101485289720242177" rel="nofollow"&gt;biases&lt;/a&gt; and factual inaccuracies, and thus GPT-2 models are likely to be biased and inaccurate as well.&lt;/li&gt;
&lt;li&gt;To avoid having samples mistaken as human-written, we recommend clearly labeling samples as synthetic before wide dissemination.  Our models are often incoherent or inaccurate in subtle ways, which takes more than a quick read for a human to notice.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-work-with-us" class="anchor" aria-hidden="true" href="#work-with-us"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Work with us&lt;/h3&gt;
&lt;p&gt;Please &lt;a href="mailto:languagequestions@openai.com"&gt;let us know&lt;/a&gt; if you‚Äôre doing interesting research with or working on applications of GPT-2!  We‚Äôre especially interested in hearing from and potentially working with those who are studying&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Potential malicious use cases and defenses against them (e.g. the detectability of synthetic text)&lt;/li&gt;
&lt;li&gt;The extent of problematic content (e.g. bias) being baked into the models and effective mitigations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h2&gt;
&lt;p&gt;See &lt;a href="./DEVELOPERS.md"&gt;DEVELOPERS.md&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;See &lt;a href="./CONTRIBUTORS.md"&gt;CONTRIBUTORS.md&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;Please use the following bibtex entry:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-future-work" class="anchor" aria-hidden="true" href="#future-work"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Future work&lt;/h2&gt;
&lt;p&gt;We may release code for evaluating the models on various benchmarks.&lt;/p&gt;
&lt;p&gt;We are still considering release of the larger models.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;a href="./LICENSE"&gt;MIT&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>openai</author><guid isPermaLink="false">https://github.com/openai/gpt-2</guid><pubDate>Tue, 19 Nov 2019 00:01:00 GMT</pubDate></item><item><title>swisskyrepo/PayloadsAllTheThings #2 in Python, This month</title><link>https://github.com/swisskyrepo/PayloadsAllTheThings</link><description>&lt;p&gt;&lt;i&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-payloads-all-the-things" class="anchor" aria-hidden="true" href="#payloads-all-the-things"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Payloads All The Things&lt;/h1&gt;
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques !
I &lt;g-emoji class="g-emoji" alias="heart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2764.png"&gt;‚ù§Ô∏è&lt;/g-emoji&gt; pull requests :)&lt;/p&gt;
&lt;p&gt;You can also contribute with a &lt;g-emoji class="g-emoji" alias="beers" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f37b.png"&gt;üçª&lt;/g-emoji&gt; IRL&lt;/p&gt;
&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;README.md - vulnerability description and how to exploit it&lt;/li&gt;
&lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt;
&lt;li&gt;Images - pictures for the README.md&lt;/li&gt;
&lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You might also like the &lt;code&gt;Methodology and Resources&lt;/code&gt; folder :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/"&gt;Methodology and Resources&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Active%20Directory%20Attack.md"&gt;Active Directory Attack.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Persistence.md"&gt;Linux - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Linux%20-%20Privilege%20Escalation.md"&gt;Linux - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Metasploit%20-%20Cheatsheet.md"&gt;Metasploit - Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Methodology%20and%20enumeration.md"&gt;Methodology and enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Pivoting%20Techniques.md"&gt;Network Pivoting Techniques.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Network%20Discovery.md"&gt;Network Discovery.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Reverse%20Shell%20Cheatsheet.md"&gt;Reverse Shell Cheatsheet.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Subdomains%20Enumeration.md"&gt;Subdomains Enumeration.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Download%20and%20Execute.md"&gt;Windows - Download and Execute.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Mimikatz.md"&gt;Windows - Mimikatz.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Persistence.md"&gt;Windows - Persistence.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Post%20Exploitation%20Koadic.md"&gt;Windows - Post Exploitation Koadic.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Privilege%20Escalation.md"&gt;Windows - Privilege Escalation.md&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/Methodology%20and%20Resources/Windows%20-%20Using%20credentials.md"&gt;Windows - Using credentials.md&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CVE%20Exploits"&gt;CVE Exploits&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apache Struts 2 CVE-2013-2251 CVE-2017-5638 CVE-2018-11776_.py&lt;/li&gt;
&lt;li&gt;Apache Struts 2 CVE-2017-9805.py&lt;/li&gt;
&lt;li&gt;Apache Struts 2 CVE-2018-11776.py&lt;/li&gt;
&lt;li&gt;Docker API RCE.py&lt;/li&gt;
&lt;li&gt;Drupalgeddon2 CVE-2018-7600.rb&lt;/li&gt;
&lt;li&gt;Heartbleed CVE-2014-0160.py&lt;/li&gt;
&lt;li&gt;JBoss CVE-2015-7501.py&lt;/li&gt;
&lt;li&gt;Jenkins CVE-2015-8103.py&lt;/li&gt;
&lt;li&gt;Jenkins CVE-2016-0792.py&lt;/li&gt;
&lt;li&gt;Rails CVE-2019-5420.rb&lt;/li&gt;
&lt;li&gt;Shellshock CVE-2014-6271.py&lt;/li&gt;
&lt;li&gt;Tomcat CVE-2017-12617.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2016-3510.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2017-10271.py&lt;/li&gt;
&lt;li&gt;WebLogic CVE-2018-2894.py&lt;/li&gt;
&lt;li&gt;WebSphere CVE-2015-7450.py&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You want more ? Check the &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/BOOKS.md"&gt;Books&lt;/a&gt; and &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/YOUTUBE.md"&gt;Youtube videos&lt;/a&gt; selections.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>swisskyrepo</author><guid isPermaLink="false">https://github.com/swisskyrepo/PayloadsAllTheThings</guid><pubDate>Tue, 19 Nov 2019 00:02:00 GMT</pubDate></item><item><title>CorentinJ/Real-Time-Voice-Cloning #3 in Python, This month</title><link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link><description>&lt;p&gt;&lt;i&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-real-time-voice-cloning" class="anchor" aria-hidden="true" href="#real-time-voice-cloning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Real-Time Voice Cloning&lt;/h1&gt;
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;Transfer Learning from Speaker Verification to
Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. Feel free to check &lt;a href="https://matheo.uliege.be/handle/2268.2/6801" rel="nofollow"&gt;my thesis&lt;/a&gt; if you're curious or if you're looking for info I haven't documented yet (don't hesitate to make an issue for that too). Mostly I would recommend giving a quick look to the figures beyond the introduction.&lt;/p&gt;
&lt;p&gt;SV2TTS is a three-stage deep learning framework that allows to create a numerical representation of a voice from a few seconds of audio, and to use it to condition a text-to-speech model trained to generalize to new voices.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9c33f78be8afe656503da974c478ea2ba2647db7/68747470733a2f2f692e696d6775722e636f6d2f386c46556c677a2e706e67" alt="Toolbox demo" data-canonical-src="https://i.imgur.com/8lFUlgz.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-papers-implemented" class="anchor" aria-hidden="true" href="#papers-implemented"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Papers implemented&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Designation&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Implementation source&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf" rel="nofollow"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf" rel="nofollow"&gt;1802.08435&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;WaveRNN (vocoder)&lt;/td&gt;
&lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1712.05884.pdf" rel="nofollow"&gt;1712.05884&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tacotron 2 (synthesizer)&lt;/td&gt;
&lt;td&gt;Natural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions&lt;/td&gt;
&lt;td&gt;&lt;a href="https://github.com/Rayhane-mamah/Tacotron-2"&gt;Rayhane-mamah/Tacotron-2&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf" rel="nofollow"&gt;1710.10467&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;GE2E (encoder)&lt;/td&gt;
&lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt;
&lt;td&gt;This repo&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-news" class="anchor" aria-hidden="true" href="#news"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;News&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;13/11/19&lt;/strong&gt;: I'm sorry that I can't maintain this repo as much as I wish I could. I'm working full time on improving voice cloning techniques and I don't have the time to share my improvements here. Plus this repo relies on a lot of old tensorflow code and it's hard to work with. If you're a researcher, then this repo might be of use to you. &lt;strong&gt;If you just want to clone your voice&lt;/strong&gt;, do check our demo on &lt;a href="https://www.resemble.ai/" rel="nofollow"&gt;Resemble.AI&lt;/a&gt; - it can run for free but it will be a bit slower, and it will give much better results than this repo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;20/08/19:&lt;/strong&gt; I'm working on &lt;a href="https://github.com/resemble-ai/Resemblyzer"&gt;resemblyzer&lt;/a&gt;, an independent package for the voice encoder. You can use your trained encoder models from this repo with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;06/07/19:&lt;/strong&gt; Need to run within a docker container on a remote server? See &lt;a href="https://sean.lane.sh/posts/2019/07/Running-the-Real-Time-Voice-Cloning-project-in-Docker/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;25/06/19:&lt;/strong&gt; Experimental support for low-memory GPUs (~2gb) added for the synthesizer. Pass &lt;code&gt;--low_mem&lt;/code&gt; to &lt;code&gt;demo_cli.py&lt;/code&gt; or &lt;code&gt;demo_toolbox.py&lt;/code&gt; to enable it. It adds a big overhead, so it's not recommended if you have enough VRAM.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick start&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h3&gt;
&lt;p&gt;You will need the following whether you plan to use the toolbox only or to retrain the models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Python 3.7&lt;/strong&gt;. Python 3.6 might work too, but I wouldn't go lower because I make extensive use of pathlib.&lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to install the necessary packages. Additionally you will need &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;PyTorch&lt;/a&gt; (&amp;gt;=1.0.1).&lt;/p&gt;
&lt;p&gt;A GPU is mandatory, but you don't necessarily need a high tier GPU if you only want to use the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pretrained-models" class="anchor" aria-hidden="true" href="#pretrained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pretrained models&lt;/h3&gt;
&lt;p&gt;Download the latest &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-preliminary" class="anchor" aria-hidden="true" href="#preliminary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preliminary&lt;/h3&gt;
&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If all tests pass, you're good to go.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Datasets&lt;/h3&gt;
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="http://www.openslr.org/resources/12/train-clean-100.tar.gz" rel="nofollow"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-toolbox" class="anchor" aria-hidden="true" href="#toolbox"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Toolbox&lt;/h3&gt;
&lt;p&gt;You can then try the toolbox:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br&gt;
or&lt;br&gt;
&lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590"&gt;this issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wiki" class="anchor" aria-hidden="true" href="#wiki"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wiki&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How it all works&lt;/strong&gt; (WIP - &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/How-it-all-works"&gt;stub&lt;/a&gt;, you might be better off reading my thesis until it's done)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training"&gt;&lt;strong&gt;Training models yourself&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training with other data/languages&lt;/strong&gt; (WIP - see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/30#issuecomment-507864097"&gt;here&lt;/a&gt; for now)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/TODO-&amp;amp;-planned-features"&gt;&lt;strong&gt;TODO and planned features&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributions--issues" class="anchor" aria-hidden="true" href="#contributions--issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributions &amp;amp; Issues&lt;/h2&gt;
&lt;p&gt;I'm working full-time as of June 2019. I don't have time to maintain this repo nor reply to issues. Sorry.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>CorentinJ</author><guid isPermaLink="false">https://github.com/CorentinJ/Real-Time-Voice-Cloning</guid><pubDate>Tue, 19 Nov 2019 00:03:00 GMT</pubDate></item><item><title>streamlit/streamlit #4 in Python, This month</title><link>https://github.com/streamlit/streamlit</link><description>&lt;p&gt;&lt;i&gt;Streamlit ‚Äî The fastest way to build custom ML tools&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-welcome-to-streamlit-wave" class="anchor" aria-hidden="true" href="#welcome-to-streamlit-wave"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Welcome to Streamlit &lt;g-emoji class="g-emoji" alias="wave" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png"&gt;üëã&lt;/g-emoji&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;The fastest way to build custom ML tools.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Streamlit lets you create apps for your machine learning projects with deceptively simple Python scripts. It supports hot-reloading, so your app updates live as you edit and save your file. No need to mess with HTTP requests, HTML, JavaScript, etc. All you need is your favorite editor and a browser. Take a look at Streamlit in action:&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/5ae1dcfd188be26bbb0648fb62e9d6d593dbb6f5/68747470733a2f2f617773312e646973636f757273652d63646e2e636f6d2f7374616e6461726431302f75706c6f6164732f73747265616d6c69742f6f726967696e616c2f31582f323932653938356637663735656637626566386332376235383939663731663736636435373765302e676966"&gt;&lt;img src="https://camo.githubusercontent.com/5ae1dcfd188be26bbb0648fb62e9d6d593dbb6f5/68747470733a2f2f617773312e646973636f757273652d63646e2e636f6d2f7374616e6461726431302f75706c6f6164732f73747265616d6c69742f6f726967696e616c2f31582f323932653938356637663735656637626566386332376235383939663731663736636435373765302e676966" alt="Example of live coding a dashboard in Streamlit|635x380" data-canonical-src="https://aws1.discourse-cdn.com/standard10/uploads/streamlit/original/1X/292e985f7f75ef7bef8c27b5899f71f76cd577e0.gif" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Check out our &lt;a href="https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace" rel="nofollow"&gt;launch blog post&lt;/a&gt;!!&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install streamlit
streamlit hello&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-example" class="anchor" aria-hidden="true" href="#example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example&lt;/h2&gt;
&lt;p&gt;Streamlit lets you build interactive apps ridiculously easily:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; streamlit &lt;span class="pl-k"&gt;as&lt;/span&gt; st

x &lt;span class="pl-k"&gt;=&lt;/span&gt; st.slider(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Select a value&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
st.write(x, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;squared is&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, x &lt;span class="pl-k"&gt;*&lt;/span&gt; x)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/1e18efff3f06946e9d1559712cea0cb76364f004/68747470733a2f2f73747265616d6c69742d64656d6f2d646174612e73332d75732d776573742d322e616d617a6f6e6177732e636f6d2f737175617265642d696d6167652d666f722d6769746875622d726561646d652d322e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/1e18efff3f06946e9d1559712cea0cb76364f004/68747470733a2f2f73747265616d6c69742d64656d6f2d646174612e73332d75732d776573742d322e616d617a6f6e6177732e636f6d2f737175617265642d696d6167652d666f722d6769746875622d726561646d652d322e706e67" width="490/" data-canonical-src="https://streamlit-demo-data.s3-us-west-2.amazonaws.com/squared-image-for-github-readme-2.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-a-bigger-example" class="anchor" aria-hidden="true" href="#a-bigger-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A Bigger Example&lt;/h2&gt;
&lt;p&gt;Despite its simplicity Streamlit lets you build incredibly rich and powerful tools. &lt;a href="https://github.com/streamlit/demo-self-driving"&gt;This demo project&lt;/a&gt; lets you browse the entire &lt;a href="https://github.com/udacity/self-driving-car"&gt;Udacity self-driving-car dataset&lt;/a&gt; and run inference in real time using the &lt;a href="https://pjreddie.com/darknet/yolo" rel="nofollow"&gt;YOLO object detection net&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/streamlit/demo-self-driving/master/av_final_optimized.gif"&gt;&lt;img src="https://raw.githubusercontent.com/streamlit/demo-self-driving/master/av_final_optimized.gif" alt="Making-of Animation" title="Making-of Animation" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The complete demo is implemented in less than 300 lines of Python. In fact, the app contains &lt;a href="https://github.com/streamlit/demo-self-driving/blob/master/app.py"&gt;only 23 Streamlit calls&lt;/a&gt; which illustrates all the major building blocks of Streamlit. You can try it right now with:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install --upgrade streamlit opencv-python
streamlit run https://raw.githubusercontent.com/streamlit/demo-self-driving/master/app.py&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-more-information" class="anchor" aria-hidden="true" href="#more-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;More Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Our &lt;a href="https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace" rel="nofollow"&gt;launch post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Our lovely &lt;a href="https://discuss.streamlit.io/" rel="nofollow"&gt;community&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Streamlit &lt;a href="https://streamlit.io/docs" rel="nofollow"&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;More &lt;a href="https://github.com/streamlit/"&gt;demo projects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you would like to contribute, see &lt;a href="https://github.com/streamlit/streamlit/wiki/Contributing"&gt;instructions here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-streamlit-for-teams" class="anchor" aria-hidden="true" href="#streamlit-for-teams"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Streamlit for Teams&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://streamlit.io/forteams/" rel="nofollow"&gt;Streamlit for Teams&lt;/a&gt; is our enterprise edition, with single-click deploy, authentication, web editing, versioning, and more. Please contact us if you would like to learn more.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Streamlit is completely free and open source and licensed under the &lt;a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"&gt;Apache 2.0&lt;/a&gt; license.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>streamlit</author><guid isPermaLink="false">https://github.com/streamlit/streamlit</guid><pubDate>Tue, 19 Nov 2019 00:04:00 GMT</pubDate></item><item><title>apachecn/AiLearning #5 in Python, This month</title><link>https://github.com/apachecn/AiLearning</link><description>&lt;p&gt;&lt;i&gt;AiLearning: Êú∫Âô®Â≠¶‰π† - MachineLearning - ML„ÄÅÊ∑±Â∫¶Â≠¶‰π† - DeepLearning - DL„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ NLP&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;a href="https://www.apachecn.org" rel="nofollow"&gt;
        &lt;img width="200" src="https://camo.githubusercontent.com/9d35c24a9d070c56093b5598ef22afae12a2f45b/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f6c6f676f2e6a7067" data-canonical-src="http://data.apachecn.org/img/logo.jpg" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;br&gt;
    &lt;a href="https://www.apachecn.org/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/63fa00e49cf2df161cf6022c501b145d225bc335/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2533452d484f4d452d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/%3E-HOME-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="http://home.apachecn.org/about/" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/971440c5a29c84e984688b2ebe165dc27aa8cf61/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2533452d41424f55542d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/%3E-ABOUT-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="mailto:apache@163.com"&gt;&lt;img src="https://camo.githubusercontent.com/f1fab6e562c98b86b95a7c44eae041e04022ec3e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2533452d456d61696c2d677265656e2e737667" data-canonical-src="https://img.shields.io/badge/%3E-Email-green.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h1 align="center"&gt;&lt;a id="user-content-ai-learning" class="anchor" aria-hidden="true" href="#ai-learning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://github.com/apachecn/AiLearning"&gt;AI learning&lt;/a&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-ÁªÑÁªá‰ªãÁªç" class="anchor" aria-hidden="true" href="#ÁªÑÁªá‰ªãÁªç"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÁªÑÁªá‰ªãÁªç&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Âêà‰Ωúor‰æµÊùÉÔºåËØ∑ËÅîÁ≥ª: &lt;code&gt;apachecn@163.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Êàë‰ª¨‰∏çÊòØ Apache ÁöÑÂÆòÊñπÁªÑÁªá/Êú∫ÊûÑ/Âõ¢‰ΩìÔºåÂè™ÊòØ Apache ÊäÄÊúØÊ†àÔºà‰ª•Âèä AIÔºâÁöÑÁà±Â•ΩËÄÖÔºÅ&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApacheCN - Â≠¶‰π†Êú∫Âô®Â≠¶‰π†Áæ§„Äê629470233„Äë&lt;a href="//shang.qq.com/wpa/qunwpa?idkey=30e5f1123a79867570f665aa3a483ca404b1c3f77737bc01ec520ed5f078ddef" rel="nofollow"&gt;&lt;img border="0" src="https://camo.githubusercontent.com/6ef3c468024fa0a3ac83d0084d8d0847c6f57769/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f6c6f676f2f417061636865434e2d67726f75702e706e67" alt="ApacheCN - Â≠¶‰π†Êú∫Âô®Â≠¶‰π†Áæ§[629470233]" title="ApacheCN - Â≠¶‰π†Êú∫Âô®Â≠¶‰π†Áæ§[629470233]" data-canonical-src="http://data.apachecn.org/img/logo/ApacheCN-group.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Ê¨¢Ëøé‰ªª‰Ωï‰∫∫ÂèÇ‰∏éÂíåÂÆåÂñÑÔºö‰∏Ä‰∏™‰∫∫ÂèØ‰ª•Ëµ∞ÁöÑÂæàÂø´Ôºå‰ΩÜÊòØ‰∏ÄÁæ§‰∫∫Âç¥ÂèØ‰ª•Ëµ∞ÁöÑÊõ¥Ëøú&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;a id="user-content-Ë∑ØÁ∫øÂõæ" class="anchor" aria-hidden="true" href="#Ë∑ØÁ∫øÂõæ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ë∑ØÁ∫øÂõæ&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;ÂÖ•Èó®Âè™Áúã: Ê≠•È™§ 1 =&amp;gt; 2 =&amp;gt; 3Ôºå‰Ω†ÂèØ‰ª•ÂΩìÂ§ßÁâõÔºÅ&lt;/li&gt;
&lt;li&gt;‰∏≠Á∫ßË°•ÂÖÖ - ËµÑÊñôÂ∫ì: &lt;a href="https://github.com/apachecn/ai-roadmap"&gt;https://github.com/apachecn/ai-roadmap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-1Êú∫Âô®Â≠¶‰π†---Âü∫Á°Ä" class="anchor" aria-hidden="true" href="#1Êú∫Âô®Â≠¶‰π†---Âü∫Á°Ä"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.Êú∫Âô®Â≠¶‰π† - Âü∫Á°Ä&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-Âü∫Êú¨‰ªãÁªç" class="anchor" aria-hidden="true" href="#Âü∫Êú¨‰ªãÁªç"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Âü∫Êú¨‰ªãÁªç&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ËµÑÊñôÊù•Ê∫ê: Machine Learning in Action(Êú∫Âô®Â≠¶‰π†ÂÆûÊàò-‰∏™‰∫∫Á¨îËÆ∞)&lt;/li&gt;
&lt;li&gt;Áªü‰∏ÄÊï∞ÊçÆÂú∞ÂùÄ: &lt;a href="https://github.com/apachecn/data"&gt;https://github.com/apachecn/data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;‰π¶Á±ç‰∏ãËΩΩÂú∞ÂùÄ: &lt;a href="https://github.com/apachecn/data/tree/master/book"&gt;https://github.com/apachecn/data/tree/master/book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Êú∫Âô®Â≠¶‰π†‰∏ãËΩΩÂú∞ÂùÄ: &lt;a href="https://github.com/apachecn/data/tree/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"&gt;https://github.com/apachecn/data/tree/master/Êú∫Âô®Â≠¶‰π†&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ê∑±Â∫¶Â≠¶‰π†Êï∞ÊçÆÂú∞ÂùÄ: &lt;a href="https://github.com/apachecn/data/tree/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"&gt;https://github.com/apachecn/data/tree/master/Ê∑±Â∫¶Â≠¶‰π†&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Êé®ËçêÁ≥ªÁªüÊï∞ÊçÆÂú∞ÂùÄ: &lt;a href="https://github.com/apachecn/data/tree/master/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"&gt;https://github.com/apachecn/data/tree/master/Êé®ËçêÁ≥ªÁªü&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ËßÜÈ¢ëÁΩëÁ´ôÔºö‰ºòÈÖ∑ Ôºèbilibili / Acfun / ÁΩëÊòì‰∫ëËØæÂ†ÇÔºåÂèØÁõ¥Êé•Âú®Á∫øÊí≠Êîæ„ÄÇÔºàÊúÄ‰∏ãÊñπÊúâÁõ∏Â∫îÈìæÊé•Ôºâ&lt;/li&gt;
&lt;li&gt;-- Êé®Ëçê &lt;a href="https://github.com/RedstoneWill"&gt;Á∫¢Ëâ≤Áü≥Â§¥&lt;/a&gt;: &lt;a href="https://github.com/apachecn/ntu-hsuantienlin-ml"&gt;Âè∞ÊπæÂ§ßÂ≠¶ÊûóËΩ©Áî∞Êú∫Âô®Â≠¶‰π†Á¨îËÆ∞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;-- Êé®Ëçê &lt;a href="https://feisky.xyz/machine-learning" rel="nofollow"&gt;Êú∫Âô®Â≠¶‰π†Á¨îËÆ∞&lt;/a&gt;: &lt;a href="https://feisky.xyz/machine-learning" rel="nofollow"&gt;https://feisky.xyz/machine-learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-Â≠¶‰π†ÊñáÊ°£" class="anchor" aria-hidden="true" href="#Â≠¶‰π†ÊñáÊ°£"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Â≠¶‰π†ÊñáÊ°£&lt;/h3&gt;
&lt;table&gt;
  &lt;tbody&gt;&lt;tr&gt;
    &lt;th&gt;Ê®°Âùó&lt;/th&gt;
    &lt;th&gt;Á´†ËäÇ&lt;/th&gt;
    &lt;th&gt;Á±ªÂûã&lt;/th&gt;
    &lt;th&gt;Ë¥üË¥£‰∫∫(GitHub)&lt;/th&gt;
    &lt;th&gt;QQ&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/1.Êú∫Âô®Â≠¶‰π†Âü∫Á°Ä.md"&gt; Á¨¨ 1 Á´†: Êú∫Âô®Â≠¶‰π†Âü∫Á°Ä&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;‰ªãÁªç&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/ElmaDavies"&gt;@ÊØõÁ∫¢Âä®&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;1306014226&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/2.k-ËøëÈÇªÁÆóÊ≥ï.md"&gt;Á¨¨ 2 Á´†: KNN ËøëÈÇªÁÆóÊ≥ï&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;ÂàÜÁ±ª&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/youyj521"&gt;@Â∞§Ê∞∏Ê±ü&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;279393323&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/3.ÂÜ≥Á≠ñÊ†ë.md"&gt;Á¨¨ 3 Á´†: ÂÜ≥Á≠ñÊ†ë&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;ÂàÜÁ±ª&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/jingwangfei"&gt;@ÊôØÊ∂õ&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;844300439&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/4.Êú¥Á¥†Ë¥ùÂè∂ÊñØ.md"&gt;Á¨¨ 4 Á´†: Êú¥Á¥†Ë¥ùÂè∂ÊñØ&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;ÂàÜÁ±ª&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/wnma3mz"&gt;@wnma3mz&lt;/a&gt;&lt;br&gt;&lt;a href="https://github.com/kailian"&gt;@ÂàÜÊûê&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;1003324213&lt;br&gt;244970749&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/5.LogisticÂõûÂΩí.md"&gt;Á¨¨ 5 Á´†: LogisticÂõûÂΩí&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;ÂàÜÁ±ª&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/DataMonk2017"&gt;@ÂæÆÂÖâÂêåÂ∞ò&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;529925688&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/6.ÊîØÊåÅÂêëÈáèÊú∫.md"&gt;Á¨¨ 6 Á´†: SVM ÊîØÊåÅÂêëÈáèÊú∫&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;ÂàÜÁ±ª&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/VPrincekin"&gt;@ÁéãÂæ∑Á∫¢&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;934969547&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;ÁΩë‰∏äÁªÑÂêàÂÜÖÂÆπ&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/7.ÈõÜÊàêÊñπÊ≥ï-ÈöèÊú∫Ê£ÆÊûóÂíåAdaBoost.md"&gt;Á¨¨ 7 Á´†: ÈõÜÊàêÊñπÊ≥ïÔºàÈöèÊú∫Ê£ÆÊûóÂíå AdaBoostÔºâ&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;ÂàÜÁ±ª&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/jiangzhonglian"&gt;@ÁâáÂàª&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;529815144&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/8.ÂõûÂΩí.md"&gt;Á¨¨ 8 Á´†: ÂõûÂΩí&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;ÂõûÂΩí&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/DataMonk2017"&gt;@ÂæÆÂÖâÂêåÂ∞ò&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;529925688&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/9.Ê†ëÂõûÂΩí.md"&gt;Á¨¨ 9 Á´†: Ê†ëÂõûÂΩí&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;ÂõûÂΩí&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/DataMonk2017"&gt;@ÂæÆÂÖâÂêåÂ∞ò&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;529925688&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/10.k-meansËÅöÁ±ª.md"&gt;Á¨¨ 10 Á´†: K-Means ËÅöÁ±ª&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;ËÅöÁ±ª&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/xuzhaoqing"&gt;@ÂæêÊò≠Ê∏Ö&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;827106588&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/11.‰ΩøÁî®AprioriÁÆóÊ≥ïËøõË°åÂÖ≥ËÅîÂàÜÊûê.md"&gt;Á¨¨ 11 Á´†: Âà©Áî® Apriori ÁÆóÊ≥ïËøõË°åÂÖ≥ËÅîÂàÜÊûê&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;È¢ëÁπÅÈ°πÈõÜ&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/WindZQ"&gt;@ÂàòÊµ∑È£û&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;1049498972&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/12.‰ΩøÁî®FP-growthÁÆóÊ≥ïÊù•È´òÊïàÂèëÁé∞È¢ëÁπÅÈ°πÈõÜ.md"&gt;Á¨¨ 12 Á´†: FP-growth È´òÊïàÂèëÁé∞È¢ëÁπÅÈ°πÈõÜ&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;È¢ëÁπÅÈ°πÈõÜ&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/mikechengwei"&gt;@Á®ãÂ®Å&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;842725815&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/13.Âà©Áî®PCAÊù•ÁÆÄÂåñÊï∞ÊçÆ.md"&gt;Á¨¨ 13 Á´†: Âà©Áî® PCA Êù•ÁÆÄÂåñÊï∞ÊçÆ&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Â∑•ÂÖ∑&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/lljuan330"&gt;@ÂªñÁ´ãÂ®ü&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;835670618&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/14.Âà©Áî®SVDÁÆÄÂåñÊï∞ÊçÆ.md"&gt;Á¨¨ 14 Á´†: Âà©Áî® SVD Êù•ÁÆÄÂåñÊï∞ÊçÆ&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Â∑•ÂÖ∑&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/marsjhao"&gt;@Âº†‰øäÁöì&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;714974242&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Êú∫Âô®Â≠¶‰π†ÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/15.Â§ßÊï∞ÊçÆ‰∏éMapReduce.md"&gt;Á¨¨ 15 Á´†: Â§ßÊï∞ÊçÆ‰∏é MapReduce&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Â∑•ÂÖ∑&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/wnma3mz"&gt;@wnma3mz&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;1003324213&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MlÈ°πÁõÆÂÆûÊàò&lt;/td&gt;
    &lt;td&gt;&lt;a href="docs/ml/16.Êé®ËçêÁ≥ªÁªü.md"&gt;Á¨¨ 16 Á´†: Êé®ËçêÁ≥ªÁªüÔºàÂ∑≤ËøÅÁßªÔºâ&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;È°πÁõÆ&lt;/td&gt;
    &lt;td&gt;&lt;a href="https://github.com/apachecn/RecommenderSystems"&gt;Êé®ËçêÁ≥ªÁªüÔºàËøÅÁßªÂêéÂú∞ÂùÄÔºâ&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Á¨¨‰∏ÄÊúüÁöÑÊÄªÁªì&lt;/td&gt;
    &lt;td&gt;&lt;a href="report/2017-04-08_Á¨¨‰∏ÄÊúüÁöÑÊÄªÁªì.md"&gt;2017-04-08: Á¨¨‰∏ÄÊúüÁöÑÊÄªÁªì&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;ÊÄªÁªì&lt;/td&gt;
    &lt;td&gt;ÊÄªÁªì&lt;/td&gt;
    &lt;td&gt;529815144&lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-ÁΩëÁ´ôËßÜÈ¢ë" class="anchor" aria-hidden="true" href="#ÁΩëÁ´ôËßÜÈ¢ë"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÁΩëÁ´ôËßÜÈ¢ë&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://www.zhihu.com/question/20691338/answer/248678328" rel="nofollow"&gt;Áü•‰πéÈóÆÁ≠î-ÁàÜÁÇ∏Âï¶-Êú∫Âô®Â≠¶‰π†ËØ•ÊÄé‰πàÂÖ•Èó®Ôºü&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ÂΩìÁÑ∂ÊàëÁü•ÈÅìÔºåÁ¨¨‰∏ÄÂè•Â∞±‰ºöË¢´ÂêêÊßΩÔºåÂõ†‰∏∫ÁßëÁè≠Âá∫Ë∫´ÁöÑ‰∫∫Ôºå‰∏çÂ±ëÁöÑÂêê‰∫Ü‰∏ÄÂè£ÂîæÊ≤´ÔºåËØ¥ÂÇªXÔºåËøòËØÑËÆ∫ Andrew Ng ÁöÑËßÜÈ¢ë„ÄÇ„ÄÇ&lt;/p&gt;
&lt;p&gt;ÊàëËøòÁü•ÈÅìËøòÊúâ‰∏ÄÈÉ®ÂàÜ‰∫∫ÔºåÁúã Andrew Ng ÁöÑËßÜÈ¢ëÂ∞±ÊòØÁúã‰∏çÊáÇÔºåÈÇ£Á•ûÁßòÁöÑÊï∞Â≠¶Êé®ÂØºÔºåÈÇ£Ëø∑‰πãÂæÆÁ¨ëÁöÑËã±ÊñáÁâàÁöÑÊïôÂ≠¶ÔºåÊàë‰ΩïÂ∞ùÂèà‰∏çÊòØËøôÊ†∑Ëµ∞ËøáÊù•ÁöÑÔºüÔºü ÊàëÁöÑÂøÉÂèØËÉΩÊØî‰Ω†‰ª¨ÈÉΩÁóõÔºåÂõ†‰∏∫ÊàëÂú®ÁΩë‰∏äÊî∂ËóèËøá‰∏ä10ÈÉ®„ÄäÊú∫Âô®Â≠¶‰π†„ÄãÁõ∏ÂÖ≥ËßÜÈ¢ëÔºåÂ§ñÂä†ÂõΩÂÜÖÊú¨ÂúüÈ£éÊ†ºÁöÑÊïôÁ®ãÔºö7Êúà+Â∞èË±° Á≠âÁ≠âÔºåÊàëÈÉΩÂæàÈöæÂéªÂê¨ÊáÇÔºåÁõ¥Âà∞Êúâ‰∏ÄÂ§©ÔºåË¢´‰∏Ä‰∏™ÁôæÂ∫¶ÁöÑÈ´òÁ∫ßÁÆóÊ≥ïÂàÜÊûêÂ∏àÊé®ËçêËØ¥Ôºö„ÄäÊú∫Âô®Â≠¶‰π†ÂÆûÊàò„ÄãËøò‰∏çÈîôÔºåÈÄö‰øóÊòìÊáÇÔºå‰Ω†ÂéªËØïËØïÔºüÔºü&lt;/p&gt;
&lt;p&gt;ÊàëËØï‰∫ÜËØïÔºåËøòÂ•ΩÊàëÁöÑPythonÂü∫Á°ÄÂíåË∞ÉËØïËÉΩÂäõËøò‰∏çÈîôÔºåÂü∫Êú¨‰∏ä‰ª£Á†ÅÈÉΩË∞ÉËØïËøá‰∏ÄÈÅçÔºåÂæàÂ§öÈ´òÂ§ß‰∏äÁöÑ "ÁêÜËÆ∫+Êé®ÂØº"ÔºåÂú®ÊàëÁúº‰∏≠ÂèòÊàê‰∫ÜÂá†‰∏™ "Âä†Âáè‰πòÈô§+Âæ™ÁéØ"ÔºåÊàëÊÉ≥Ëøô‰∏çÂ∞±ÊòØÂÉèÊàëËøôÊ†∑ÁöÑÁ®ãÂ∫èÂëòÊÉ≥Ë¶ÅÁöÑÂÖ•Èó®ÊïôÁ®ã‰πàÔºü&lt;/p&gt;
&lt;p&gt;ÂæàÂ§öÁ®ãÂ∫èÂëòËØ¥Êú∫Âô®Â≠¶‰π† TM Â§™ÈöæÂ≠¶‰∫ÜÔºåÊòØÁöÑÔºåÁúü TM ÈöæÂ≠¶ÔºåÊàëÊÉ≥ÊúÄÈöæÁöÑÊòØÔºöÊ≤°Êúâ‰∏ÄÊú¨ÂÉè„ÄäÊú∫Âô®Â≠¶‰π†ÂÆûÊàò„ÄãÈÇ£Ê†∑ÁöÑ‰ΩúËÄÖÊÑøÊÑè‰ª•Á®ãÂ∫èÂëò Coding ËßíÂ∫¶ÂéªÁªôÂ§ßÂÆ∂ËÆ≤Ëß£ÔºÅÔºÅ&lt;/p&gt;
&lt;p&gt;ÊúÄËøëÂá†Â§©ÔºåGitHub Ê∂®‰∫Ü 300È¢ó starÔºåÂä†Áæ§ÁöÑ200‰∫∫Ôºå Áé∞Âú®ËøòÂú®‰∏çÊñ≠ÁöÑÂ¢ûÂä†++ÔºåÊàëÊÉ≥Â§ßÂÆ∂ÂèØËÉΩÈÉΩÊòØÊÑüÂêåË∫´ÂèóÂêßÔºÅ&lt;/p&gt;
&lt;p&gt;ÂæàÂ§öÊÉ≥ÂÖ•Èó®Êñ∞ÊâãÂ∞±ÊòØË¢´ÂøΩÊÇ†ÁùÄÊî∂ËóèÊî∂ËóèÂÜçÊî∂ËóèÔºå‰ΩÜÊòØÊúÄÂêéËøòÊòØ‰ªÄ‰πàÈÉΩÊ≤°ÊúâÂ≠¶Âà∞Ôºå‰πüÂ∞±ÊòØ"ËµÑÊ∫êÊî∂ËóèÂÆ∂"Ôºå‰πüËÆ∏Êñ∞ÊâãË¶ÅÁöÑÂ∞±ÊòØ &lt;a href="https://docs.apachecn.org/map" rel="nofollow"&gt;MachineLearning(Êú∫Âô®Â≠¶‰π†) Â≠¶‰π†Ë∑ØÁ∫øÂõæ&lt;/a&gt;„ÄÇÊ≤°ÈîôÔºåÊàëÂèØ‰ª•Áªô‰Ω†‰ª¨ÁöÑ‰∏Ä‰ªΩÔºåÂõ†‰∏∫Êàë‰ª¨ËøòÈÄöËøáËßÜÈ¢ëËÆ∞ÂΩï‰∏ãÊù•Êàë‰ª¨ÁöÑÂ≠¶‰π†ËøáÁ®ã„ÄÇÊ∞¥Âπ≥ÂΩìÁÑ∂‰πüÊúâÈôêÔºå‰∏çËøáÂØπ‰∫éÊñ∞ÊâãÂÖ•Èó®ÔºåÁªùÂØπÊ≤°ÈóÆÈ¢òÔºåÂ¶ÇÊûú‰Ω†Ëøò‰∏ç‰ºöÔºåÈÇ£ÁÆóÊàëËæìÔºÅÔºÅ&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ËßÜÈ¢ëÊÄé‰πàÁúãÔºü&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/4aeb51811bbc73e4715ced3ebc64ab26670bdc67/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434e2d4d4c2d62696c6962696c692d636f6d706172652e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/4aeb51811bbc73e4715ced3ebc64ab26670bdc67/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434e2d4d4c2d62696c6962696c692d636f6d706172652e6a7067" alt="" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCN-ML-bilibili-compare.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ÁêÜËÆ∫ÁßëÁè≠Âá∫Ë∫´-Âª∫ËÆÆÂéªÂ≠¶‰π† Andrew Ng ÁöÑËßÜÈ¢ëÔºàNg ÁöÑËßÜÈ¢ëÁªùÂØπÊòØÊùÉÂ®ÅÔºåËøô‰∏™ÊØãÂ∫∏ÁΩÆÁñëÔºâ&lt;/li&gt;
&lt;li&gt;ÁºñÁ†ÅËÉΩÂäõÂº∫ - Âª∫ËÆÆÁúãÊàë‰ª¨ÁöÑ&lt;a href="https://space.bilibili.com/97678687/#!/channel/detail?cid=22486" rel="nofollow"&gt;„ÄäÊú∫Âô®Â≠¶‰π†ÂÆûÊàò-ÊïôÂ≠¶Áâà„Äã&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ÁºñÁ†ÅËÉΩÂäõÂº± - Âª∫ËÆÆÁúãÊàë‰ª¨ÁöÑ&lt;a href="https://space.bilibili.com/97678687/#!/channel/detail?cid=13045" rel="nofollow"&gt;„ÄäÊú∫Âô®Â≠¶‰π†ÂÆûÊàò-ËÆ®ËÆ∫Áâà„Äã&lt;/a&gt;Ôºå‰∏çËøáÂú®ÁúãÁêÜËÆ∫ÁöÑÊó∂ÂÄôÔºåÁúã ÊïôÂ≠¶Áâà-ÁêÜËÆ∫ÈÉ®ÂàÜÔºõËÆ®ËÆ∫ÁâàÁöÑÂ∫üËØùÂ§™Â§öÔºå‰∏çËøáÂú®ËÆ≤Ëß£‰ª£Á†ÅÁöÑÊó∂ÂÄôÊòØ‰∏ÄË°å‰∏ÄË°åËÆ≤Ëß£ÁöÑÔºõÊâÄ‰ª•ÔºåÊ†πÊçÆËá™Â∑±ÁöÑÈúÄÊ±ÇÔºåËá™Áî±ÁöÑÁªÑÂêà„ÄÇ&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;„ÄêÂÖçË¥π„ÄëÊï∞Â≠¶ÊïôÂ≠¶ËßÜÈ¢ë - ÂèØÊ±óÂ≠¶Èô¢ ÂÖ•Èó®ÁØá&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;@‰∫éÊåØÊ¢ì&lt;/a&gt; Êé®Ëçê: ÂèØÊ±óÂ≠¶Èô¢-ÁΩëÊòìÂÖ¨ÂºÄËØæ&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Ê¶ÇÁéá&lt;/th&gt;
&lt;th&gt;ÁªüËÆ°&lt;/th&gt;
&lt;th&gt;Á∫øÊÄß‰ª£Êï∞&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://open.163.com/special/Khan/probability.html" rel="nofollow"&gt;ÂèØÊ±óÂ≠¶Èô¢(Ê¶ÇÁéá)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://open.163.com/special/Khan/khstatistics.html" rel="nofollow"&gt;ÂèØÊ±óÂ≠¶Èô¢(ÁªüËÆ°Â≠¶)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://open.163.com/special/Khan/linearalgebra.html" rel="nofollow"&gt;ÂèØÊ±óÂ≠¶Èô¢(Á∫øÊÄß‰ª£Êï∞)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;Êú∫Âô®Â≠¶‰π†ËßÜÈ¢ë - ApacheCN ÊïôÂ≠¶Áâà&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;AcFun&lt;/td&gt;
&lt;td&gt;BÁ´ô&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a title="AcFunÔºàÊú∫Âô®Â≠¶‰π†ËßÜÈ¢ëÔºâ" href="http://www.acfun.cn/u/12540256.aspx#page=1" rel="nofollow"&gt;&lt;img width="290" src="https://camo.githubusercontent.com/122aa82278121b78486f6cc20b3813851832d1b0/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434e2d4d4c2d416346756e2e6a7067" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCN-ML-AcFun.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a title="bilibiliÔºàÊú∫Âô®Â≠¶‰π†ËßÜÈ¢ëÔºâ" href="https://space.bilibili.com/97678687/#!/channel/index" rel="nofollow"&gt;&lt;img width="290" src="https://camo.githubusercontent.com/c04107f0478a7e3e15b5103004d534af8ec36afd/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434e2d4d4c2d62696c6962696c692e6a7067" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCN-ML-bilibili.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;‰ºòÈÖ∑&lt;/td&gt;
&lt;td&gt;ÁΩëÊòì‰∫ëËØæÂ†Ç&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a title="YouKuÔºàÊú∫Âô®Â≠¶‰π†ËßÜÈ¢ëÔºâ" href="http://i.youku.com/apachecn" rel="nofollow"&gt;&lt;img width="290" src="https://camo.githubusercontent.com/ff306cb8a0eac3f6a9a59f01e73b538b0be8588e/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434d2d4d4c2d796f756b752e6a7067" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCM-ML-youku.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a title="WangYiYunKeTangÔºàÊú∫Âô®Â≠¶‰π†ËßÜÈ¢ëÔºâ" href="http://study.163.com/course/courseMain.htm?courseId=1004582003" rel="nofollow"&gt;&lt;img width="290" src="https://camo.githubusercontent.com/615d94638b0fefc25253ca5a4147a5153e35379f/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f4d61696e506167652f417061636865434d2d4d4c2d57616e67596959756e4b6554616e672e706e67" data-canonical-src="http://data.apachecn.org/img/AiLearning/MainPage/ApacheCM-ML-WangYiYunKeTang.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;„ÄêÂÖçË¥π„ÄëÊú∫Âô®/Ê∑±Â∫¶Â≠¶‰π†ËßÜÈ¢ë - Âê¥ÊÅ©Ëææ&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Êú∫Âô®Â≠¶‰π†&lt;/th&gt;
&lt;th&gt;Ê∑±Â∫¶Â≠¶‰π†&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="http://study.163.com/course/courseMain.htm?courseId=1004570029" rel="nofollow"&gt;Âê¥ÊÅ©ËææÊú∫Âô®Â≠¶‰π†&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="http://mooc.study.163.com/course/2001281002?tid=2001392029" rel="nofollow"&gt;Á•ûÁªèÁΩëÁªúÂíåÊ∑±Â∫¶Â≠¶‰π†&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-2Ê∑±Â∫¶Â≠¶‰π†" class="anchor" aria-hidden="true" href="#2Ê∑±Â∫¶Â≠¶‰π†"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.Ê∑±Â∫¶Â≠¶‰π†&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-ÂÖ•Èó®Âü∫Á°Ä" class="anchor" aria-hidden="true" href="#ÂÖ•Èó®Âü∫Á°Ä"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÂÖ•Èó®Âü∫Á°Ä&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="/docs/dl/%E5%8F%8D%E5%90%91%E4%BC%A0%E9%80%92.md"&gt;ÂèçÂêë‰º†ÈÄí&lt;/a&gt;: &lt;a href="https://www.cnblogs.com/charlotte77/p/5629865.html" rel="nofollow"&gt;https://www.cnblogs.com/charlotte77/p/5629865.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/docs/dl/CNN%E5%8E%9F%E7%90%86.md"&gt;CNNÂéüÁêÜ&lt;/a&gt;: &lt;a href="http://www.cnblogs.com/charlotte77/p/7759802.html" rel="nofollow"&gt;http://www.cnblogs.com/charlotte77/p/7759802.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/docs/dl/RNN%E5%8E%9F%E7%90%86.md"&gt;RNNÂéüÁêÜ&lt;/a&gt;: &lt;a href="https://blog.csdn.net/qq_39422642/article/details/78676567" rel="nofollow"&gt;https://blog.csdn.net/qq_39422642/article/details/78676567&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/docs/dl/LSTM%E5%8E%9F%E7%90%86.md"&gt;LSTMÂéüÁêÜ&lt;/a&gt;: &lt;a href="https://blog.csdn.net/weixin_42111770/article/details/80900575" rel="nofollow"&gt;https://blog.csdn.net/weixin_42111770/article/details/80900575&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-pytorch---ÊïôÁ®ã" class="anchor" aria-hidden="true" href="#pytorch---ÊïôÁ®ã"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pytorch - ÊïôÁ®ã&lt;/h3&gt;
&lt;p&gt;-- ÂæÖÊõ¥Êñ∞&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-tensorflow-20---ÊïôÁ®ã" class="anchor" aria-hidden="true" href="#tensorflow-20---ÊïôÁ®ã"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;TensorFlow 2.0 - ÊïôÁ®ã&lt;/h3&gt;
&lt;p&gt;-- ÂæÖÊõ¥Êñ∞&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ÁõÆÂΩïÁªìÊûÑ:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97.md"&gt;ÂÆâË£ÖÊåáÂçó&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/Keras%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.md"&gt;Kears Âø´ÈÄüÂÖ•Èó®&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE_1_%E7%94%B5%E5%BD%B1%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB.md"&gt;ÂÆûÊàòÈ°πÁõÆ 1 ÁîµÂΩ±ÊÉÖÊÑüÂàÜÁ±ª&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE_2_%E6%B1%BD%E8%BD%A6%E7%87%83%E6%B2%B9%E6%95%88%E7%8E%87.md"&gt;ÂÆûÊàòÈ°πÁõÆ 2 Ê±ΩËΩ¶ÁáÉÊ≤πÊïàÁéá&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/TensorFlow2.x/%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE%E4%BC%98%E5%8C%96_%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88.md"&gt;ÂÆûÊàòÈ°πÁõÆ ‰ºòÂåñ: ËøáÊãüÂêàÔºåÊ¨†ÊãüÂêà&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-3Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ" class="anchor" aria-hidden="true" href="#3Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3.Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ&lt;/h2&gt;
&lt;p&gt;Â≠¶‰π†ËøáÁ®ã‰∏≠-ÂÜÖÂøÉÂ§çÊùÇÁöÑÂèòÂåñÔºÅÔºÅÔºÅ&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;Ëá™‰ªéÂ≠¶‰π†&lt;span class="pl-c1"&gt;NLP&lt;/span&gt;‰ª•ÂêéÔºåÊâçÂèëÁé∞ÂõΩÂÜÖ‰∏éÂõΩÂ§ñÁöÑÂÖ∏ÂûãÂå∫Âà´:
&lt;span class="pl-c1"&gt;1&lt;/span&gt;. ÂØπËµÑÊ∫êÁöÑÊÄÅÂ∫¶ÊòØÂÆåÂÖ®Áõ∏ÂèçÁöÑ:
  &lt;span class="pl-c1"&gt;1&lt;/span&gt;) ÂõΩÂÜÖÔºöÂ∞±Â•ΩÂÉè‰∏∫‰∫ÜÂêçÊ∞îÔºå‰∏æÂäûÂ∑•‰ΩúË£ÖÈÄºÁöÑ‰ºöËÆÆÔºåÂ∞±ÊòØÊ≤°ÊúâÂπ≤Ë¥ßÔºåÂÖ®ÈÉ®ÈÉΩÊòØË±°ÂæÅÊÄßÁöÑ&lt;span class="pl-c1"&gt;PPT&lt;/span&gt;‰ªãÁªçÔºå‰∏çÊòØÈíàÂØπÂú®ÂÅöÁöÑÂêÑ‰Ωç
  &lt;span class="pl-c1"&gt;2&lt;/span&gt;ÔºâÂõΩÂ§ñÔºöÂ∞±Â•ΩÂÉèÊòØ‰∏∫‰∫ÜÊé®Âä®nlpËøõÊ≠•‰∏ÄÊ†∑ÔºåÂàÜ‰∫´ËÄÖÂêÑÁßçÂπ≤Ë¥ßËµÑÊñôÂíåÂÖ∑‰ΩìÁöÑÂÆûÁé∞„ÄÇÔºàÁâπÂà´ÊòØ: pythonËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºâ
&lt;span class="pl-c1"&gt;2&lt;/span&gt;. ËÆ∫ÊñáÁöÑÂÆûÁé∞Ôºö
  &lt;span class="pl-c1"&gt;1&lt;/span&gt;) ÂêÑÁßçÈ´òÂ§ß‰∏äÁöÑËÆ∫ÊñáÂÆûÁé∞ÔºåÂç¥ËøòÊòØÊ≤°ÁúãÂà∞‰∏Ä‰∏™ÂÉèÊ†∑ÁöÑGitHubÈ°πÁõÆÔºÅÔºàÂèØËÉΩÊàëÁöÑÊêúÁ¥¢ËÉΩÂäõÂ∑Æ‰∫ÜÁÇπÔºå‰∏ÄÁõ¥Ê≤°ÊâæÂà∞Ôºâ
  &lt;span class="pl-c1"&gt;2&lt;/span&gt;ÔºâÂõΩÂ§ñÂ∞±‰∏ç‰∏æ‰æã‰∫ÜÔºåÊàëÁúã‰∏çÊáÇÔºÅ
&lt;span class="pl-c1"&gt;3&lt;/span&gt;. ÂºÄÊ∫êÁöÑÊ°ÜÊû∂
  &lt;span class="pl-c1"&gt;1&lt;/span&gt;ÔºâÂõΩÂ§ñÁöÑÂºÄÊ∫êÊ°ÜÊû∂Ôºö tensorflow&lt;span class="pl-k"&gt;/&lt;/span&gt;pytorch ÊñáÊ°£&lt;span class="pl-k"&gt;+&lt;/span&gt;ÊïôÁ®ã&lt;span class="pl-k"&gt;+&lt;/span&gt;ËßÜÈ¢ëÔºàÂÆòÊñπÊèê‰æõÔºâ
  &lt;span class="pl-c1"&gt;2&lt;/span&gt;) ÂõΩÂÜÖÁöÑÂºÄÊ∫êÊ°ÜÊû∂: È¢ùÈ¢ùÔºåËøòÁúü‰∏æ‰æã‰∏çÂá∫Êù•ÔºÅ‰ΩÜÊòØÁâõÈÄºÂêπÂæó‰∏çÊØîÂõΩÂ§ñÂ∑ÆÔºÅÔºàMXNetËôΩÁÑ∂Êúâ‰ºóÂ§öÂõΩ‰∫∫ÂèÇ‰∏éÂºÄÂèëÔºå‰ΩÜ‰∏çËÉΩÁÆóÊòØÂõΩÂÜÖÂºÄÊ∫êÊ°ÜÊû∂„ÄÇÂü∫‰∫éMXNetÁöÑÂä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†(http:&lt;span class="pl-k"&gt;//&lt;/span&gt;zh.d2l.ai &lt;span class="pl-k"&gt;&amp;amp;&lt;/span&gt; https:&lt;span class="pl-k"&gt;//&lt;/span&gt;discuss.gluon.ai&lt;span class="pl-k"&gt;/&lt;/span&gt;t&lt;span class="pl-k"&gt;/&lt;/span&gt;topic&lt;span class="pl-k"&gt;/&lt;/span&gt;&lt;span class="pl-c1"&gt;753&lt;/span&gt;)‰∏≠ÊñáÊïôÁ®ã,Â∑≤ÁªèÁî±Ê≤êÁ•û(ÊùéÊ≤ê)‰ª•ÂèäÈòøÊñØÈ°ø¬∑Âº†ËÆ≤ÊéàÂΩïÂà∂ÔºåÂÖ¨ÂºÄÂèëÂ∏É(ÊñáÊ°£&lt;span class="pl-k"&gt;+&lt;/span&gt;Á¨¨‰∏ÄÂ≠£ÊïôÁ®ã&lt;span class="pl-k"&gt;+&lt;/span&gt;ËßÜÈ¢ëÔºâ„ÄÇ)
ÊØè‰∏ÄÊ¨°Ê∑±ÂÖ•ÈÉΩË¶ÅÂéªÁøªÂ¢ôÔºåÊØè‰∏ÄÊ¨°Ê∑±ÂÖ•ÈÉΩË¶ÅGoogleÔºåÊØè‰∏ÄÊ¨°ÁúãÁùÄÂõΩÂÜÖÁöÑËØ¥ÔºöÂìàÂ∑•Â§ß„ÄÅËÆØÈ£û„ÄÅ‰∏≠ÁßëÂ§ß„ÄÅÁôæÂ∫¶„ÄÅÈòøÈáåÂ§öÁâõÈÄºÔºå‰ΩÜÊòØËµÑÊñôËøòÊòØÂæóÂõΩÂ§ñÂéªÊâæÔºÅ
ÊúâÊó∂ÂÄôÁúüÁöÑÊå∫ÊÅ®ÁöÑÔºÅÁúüÁöÑÊúâÁÇπÁûß‰∏çËµ∑Ëá™Â∑±ÂõΩÂÜÖÁöÑÊäÄÊúØÁéØÂ¢ÉÔºÅ

ÂΩìÁÑ∂Ë∞¢Ë∞¢ÂõΩÂÜÖÂæàÂ§öÂçöÂÆ¢Â§ß‰Ω¨ÔºåÁâπÂà´ÊòØ‰∏Ä‰∫õÂÖ•Èó®ÁöÑDemoÂíåÂü∫Êú¨Ê¶ÇÂøµ„ÄÇ„ÄêÊ∑±ÂÖ•ÁöÑÊ∞¥Âπ≥ÊúâÈôêÔºåÊ≤°ÁúãÊáÇ„Äë&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/6535fd936f9a2b1db5392b626320ce725854f675/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f6e6c702f46393435383146363443323141313039344134373333393744464134324639432e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/6535fd936f9a2b1db5392b626320ce725854f675/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f6e6c702f46393435383146363443323141313039344134373333393744464134324639432e6a7067" alt="" data-canonical-src="http://data.apachecn.org/img/AiLearning/nlp/F94581F64C21A1094A473397DFA42F9C.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;„ÄêÂÖ•Èó®È°ªÁü•„ÄëÂøÖÈ°ª‰∫ÜËß£&lt;/strong&gt;: &lt;a href="https://github.com/apachecn/AiLearning/tree/master/docs/nlp"&gt;https://github.com/apachecn/AiLearning/tree/master/docs/nlp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;„ÄêÂÖ•Èó®ÊïôÁ®ã„ÄëÂº∫ÁÉàÊé®Ëçê: PyTorch Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ&lt;/strong&gt;: &lt;a href="https://github.com/apachecn/NLP-with-PyTorch"&gt;https://github.com/apachecn/NLP-with-PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Python Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ Á¨¨‰∫åÁâà: &lt;a href="https://usyiyi.github.io/nlp-py-2e-zh" rel="nofollow"&gt;https://usyiyi.github.io/nlp-py-2e-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Êé®Ëçê‰∏Ä‰∏™&lt;a href="https://github.com/liuhuanyong"&gt;liuhuanyongÂ§ß‰Ω¨&lt;/a&gt;Êï¥ÁêÜÁöÑnlpÂÖ®Èù¢Áü•ËØÜ‰ΩìÁ≥ª: &lt;a href="https://liuhuanyong.github.io" rel="nofollow"&gt;https://liuhuanyong.github.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ÂºÄÊ∫ê - ËØçÂêëÈáèÂ∫ìÈõÜÂêà:
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Embedding/Chinese-Word-Vectors"&gt;https://github.com/Embedding/Chinese-Word-Vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/brightmart/nlp_chinese_corpus"&gt;https://github.com/brightmart/nlp_chinese_corpus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/codemayq/chinese_chatbot_corpus"&gt;https://github.com/codemayq/chinese_chatbot_corpus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/candlewill/Dialog_Corpus"&gt;https://github.com/candlewill/Dialog_Corpus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-1‰ΩøÁî®Âú∫ÊôØ-ÁôæÂ∫¶ÂÖ¨ÂºÄËØæ" class="anchor" aria-hidden="true" href="#1‰ΩøÁî®Âú∫ÊôØ-ÁôæÂ∫¶ÂÖ¨ÂºÄËØæ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.‰ΩøÁî®Âú∫ÊôØ ÔºàÁôæÂ∫¶ÂÖ¨ÂºÄËØæÔºâ&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Á¨¨‰∏ÄÈÉ®ÂàÜ ÂÖ•Èó®‰ªãÁªç&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;1.) &lt;a href="/docs/nlp/1.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D.md"&gt;Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂÖ•Èó®‰ªãÁªç&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Á¨¨‰∫åÈÉ®ÂàÜ Êú∫Âô®ÁøªËØë&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;2.) &lt;a href="/docs/nlp/2.%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91.md"&gt;Êú∫Âô®ÁøªËØë&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Á¨¨‰∏âÈÉ®ÂàÜ ÁØáÁ´†ÂàÜÊûê&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;3.1.) &lt;a href="/docs/nlp/3.1.%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90-%E5%86%85%E5%AE%B9%E6%A6%82%E8%BF%B0.md"&gt;ÁØáÁ´†ÂàÜÊûê-ÂÜÖÂÆπÊ¶ÇËø∞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.2.) &lt;a href="/docs/nlp/3.2.%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90-%E5%86%85%E5%AE%B9%E6%A0%87%E7%AD%BE.md"&gt;ÁØáÁ´†ÂàÜÊûê-ÂÜÖÂÆπÊ†áÁ≠æ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.3.) &lt;a href="/docs/nlp/3.3.%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.md"&gt;ÁØáÁ´†ÂàÜÊûê-ÊÉÖÊÑüÂàÜÊûê&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;3.4.) &lt;a href="/docs/nlp/3.4.%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90-%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81.md"&gt;ÁØáÁ´†ÂàÜÊûê-Ëá™Âä®ÊëòË¶Å&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Á¨¨ÂõõÈÉ®ÂàÜ UNIT-ËØ≠Ë®ÄÁêÜËß£‰∏é‰∫§‰∫íÊäÄÊúØ&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;4.) &lt;a href="/docs/nlp/4.UNIT-%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E4%BA%A4%E4%BA%92%E6%8A%80%E6%9C%AF.md"&gt;UNIT-ËØ≠Ë®ÄÁêÜËß£‰∏é‰∫§‰∫íÊäÄÊúØ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-Â∫îÁî®È¢ÜÂüü" class="anchor" aria-hidden="true" href="#Â∫îÁî®È¢ÜÂüü"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Â∫îÁî®È¢ÜÂüü&lt;/h3&gt;
&lt;h4&gt;&lt;a id="user-content-‰∏≠ÊñáÂàÜËØç" class="anchor" aria-hidden="true" href="#‰∏≠ÊñáÂàÜËØç"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;‰∏≠ÊñáÂàÜËØçÔºö&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ÊûÑÂª∫DAGÂõæ&lt;/li&gt;
&lt;li&gt;Âä®ÊÄÅËßÑÂàíÊü•ÊâæÔºåÁªºÂêàÊ≠£ÂèçÂêëÔºàÊ≠£ÂêëÂä†ÊùÉÂèçÂêëËæìÂá∫ÔºâÊ±ÇÂæóDAGÊúÄÂ§ßÊ¶ÇÁéáË∑ØÂæÑ&lt;/li&gt;
&lt;li&gt;‰ΩøÁî®‰∫ÜSBMEËØ≠ÊñôËÆ≠ÁªÉ‰∫Ü‰∏ÄÂ•ó HMM + Viterbi Ê®°ÂûãÔºåËß£ÂÜ≥Êú™ÁôªÂΩïËØçÈóÆÈ¢ò&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-1ÊñáÊú¨ÂàÜÁ±ªtext-classification" class="anchor" aria-hidden="true" href="#1ÊñáÊú¨ÂàÜÁ±ªtext-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;1.ÊñáÊú¨ÂàÜÁ±ªÔºàText ClassificationÔºâ&lt;/h4&gt;
&lt;p&gt;ÊñáÊú¨ÂàÜÁ±ªÊòØÊåáÊ†áËÆ∞Âè•Â≠êÊàñÊñáÊ°£Ôºå‰æãÂ¶ÇÁîµÂ≠êÈÇÆ‰ª∂ÂûÉÂúæÈÇÆ‰ª∂ÂàÜÁ±ªÂíåÊÉÖÊÑüÂàÜÊûê„ÄÇ&lt;/p&gt;
&lt;p&gt;‰∏ãÈù¢ÊòØ‰∏Ä‰∫õÂæàÂ•ΩÁöÑÂàùÂ≠¶ËÄÖÊñáÊú¨ÂàÜÁ±ªÊï∞ÊçÆÈõÜ„ÄÇ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html" rel="nofollow"&gt;Ë∑ØÈÄèÁ§æNewswire‰∏ªÈ¢òÂàÜÁ±ª&lt;/a&gt;ÔºàË∑ØÈÄèÁ§æ-21578Ôºâ„ÄÇ1987Âπ¥Ë∑ØÈÄèÁ§æÂá∫Áé∞ÁöÑ‰∏ÄÁ≥ªÂàóÊñ∞ÈóªÊñá‰ª∂ÔºåÊåâÁ±ªÂà´ÁºñÂà∂Á¥¢Âºï„ÄÇ&lt;a href="http://trec.nist.gov/data/reuters/reuters.html" rel="nofollow"&gt;Âè¶ËßÅRCV1ÔºåRCV2ÂíåTRC2&lt;/a&gt;„ÄÇ&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ai.stanford.edu/~amaas/data/sentiment" rel="nofollow"&gt;IMDBÁîµÂΩ±ËØÑËÆ∫ÊÉÖÊÑüÂàÜÁ±ªÔºàÊñØÂù¶Á¶èÔºâ&lt;/a&gt;„ÄÇÊù•Ëá™ÁΩëÁ´ôimdb.comÁöÑ‰∏ÄÁ≥ªÂàóÁîµÂΩ±ËØÑËÆ∫ÂèäÂÖ∂ÁßØÊûÅÊàñÊ∂àÊûÅÁöÑÊÉÖÁª™„ÄÇ&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/" rel="nofollow"&gt;Êñ∞ÈóªÁªÑÁîµÂΩ±ËØÑËÆ∫ÊÉÖÊÑüÂàÜÁ±ªÔºàÂ∫∑Â•àÂ∞îÔºâ&lt;/a&gt;„ÄÇÊù•Ëá™ÁΩëÁ´ôimdb.comÁöÑ‰∏ÄÁ≥ªÂàóÁîµÂΩ±ËØÑËÆ∫ÂèäÂÖ∂ÁßØÊûÅÊàñÊ∂àÊûÅÁöÑÊÉÖÁª™„ÄÇ&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ÊúâÂÖ≥Êõ¥Â§ö‰ø°ÊÅØÔºåËØ∑ÂèÇÈòÖÂ∏ñÂ≠êÔºö
&lt;a href="http://ana.cachopo.org/datasets-for-single-label-text-categorization" rel="nofollow"&gt;ÂçïÊ†áÁ≠æÊñáÊú¨ÂàÜÁ±ªÁöÑÊï∞ÊçÆÈõÜ&lt;/a&gt;„ÄÇ&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ÊÉÖÊÑüÂàÜÊûê&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ÊØîËµõÂú∞ÂùÄ: &lt;a href="https://www.kaggle.com/c/word2vec-nlp-tutorial" rel="nofollow"&gt;https://www.kaggle.com/c/word2vec-nlp-tutorial&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ÊñπÊ°à‰∏Ä(0.86)ÔºöWordCount + Êú¥Á¥† Bayes&lt;/li&gt;
&lt;li&gt;ÊñπÊ°à‰∫å(0.94)ÔºöLDA + ÂàÜÁ±ªÊ®°ÂûãÔºàknn/ÂÜ≥Á≠ñÊ†ë/ÈÄªËæëÂõûÂΩí/svm/xgboost/ÈöèÊú∫Ê£ÆÊûóÔºâ
&lt;ul&gt;
&lt;li&gt;a) ÂÜ≥Á≠ñÊ†ëÊïàÊûú‰∏çÊòØÂæàÂ•ΩÔºåËøôÁßçËøûÁª≠ÁâπÂæÅ‰∏çÂ§™ÈÄÇÂêàÁöÑ&lt;/li&gt;
&lt;li&gt;b) ÈÄöËøáÂèÇÊï∞Ë∞ÉÊï¥ 200 ‰∏™topicÔºå‰ø°ÊÅØÈáè‰øùÂ≠òÊïàÊûúËæÉ‰ºòÔºàËÆ°ÁÆó‰∏ªÈ¢òÔºâ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ÊñπÊ°à‰∏â(0.72)Ôºöword2vec + CNN
&lt;ul&gt;
&lt;li&gt;ËØ¥ÂÆûËØùÔºöÊ≤°Êúâ‰∏Ä‰∏™Â•ΩÁöÑÊú∫Âô®ÔºåÊòØË∞É‰∏çÂá∫Êù•‰∏Ä‰∏™Â•ΩÁöÑÁªìÊûú (: ÈÄÉ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;ÈÄöËøáAUC Êù•ËØÑ‰º∞Ê®°ÂûãÁöÑÊïàÊûú&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-2ËØ≠Ë®ÄÊ®°Âûãlanguage-modeling" class="anchor" aria-hidden="true" href="#2ËØ≠Ë®ÄÊ®°Âûãlanguage-modeling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;2.ËØ≠Ë®ÄÊ®°ÂûãÔºàLanguage ModelingÔºâ&lt;/h4&gt;
&lt;p&gt;ËØ≠Ë®ÄÂª∫Ê®°Ê∂âÂèäÂºÄÂèë‰∏ÄÁßçÁªüËÆ°Ê®°ÂûãÔºåÁî®‰∫éÈ¢ÑÊµãÂè•Â≠ê‰∏≠ÁöÑ‰∏ã‰∏Ä‰∏™ÂçïËØçÊàñ‰∏Ä‰∏™ÂçïËØç‰∏≠ÁöÑ‰∏ã‰∏Ä‰∏™ÂçïËØç„ÄÇÂÆÉÊòØËØ≠Èü≥ËØÜÂà´ÂíåÊú∫Âô®ÁøªËØëÁ≠â‰ªªÂä°‰∏≠ÁöÑÂâçÁΩÆ‰ªªÂä°„ÄÇ&lt;/p&gt;
&lt;p&gt;ÂÆÉÊòØËØ≠Èü≥ËØÜÂà´ÂíåÊú∫Âô®ÁøªËØëÁ≠â‰ªªÂä°‰∏≠ÁöÑÂâçÁΩÆ‰ªªÂä°„ÄÇ&lt;/p&gt;
&lt;p&gt;‰∏ãÈù¢ÊòØ‰∏Ä‰∫õÂæàÂ•ΩÁöÑÂàùÂ≠¶ËÄÖËØ≠Ë®ÄÂª∫Ê®°Êï∞ÊçÆÈõÜ„ÄÇ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.gutenberg.org/" rel="nofollow"&gt;Âè§ËÖæÂ†°È°πÁõÆ&lt;/a&gt;Ôºå‰∏ÄÁ≥ªÂàóÂÖçË¥π‰π¶Á±çÔºåÂèØ‰ª•Áî®Á∫ØÊñáÊú¨Ê£ÄÁ¥¢ÂêÑÁßçËØ≠Ë®Ä„ÄÇ&lt;/li&gt;
&lt;li&gt;ËøòÊúâÊõ¥Â§öÊ≠£ÂºèÁöÑËØ≠ÊñôÂ∫ìÂæóÂà∞‰∫ÜÂæàÂ•ΩÁöÑÁ†îÁ©∂;¬†‰æãÂ¶ÇÔºö
&lt;a href="https://en.wikipedia.org/wiki/Brown_Corpus" rel="nofollow"&gt;Â∏ÉÊúóÂ§ßÂ≠¶Áé∞‰ª£ÁæéÂõΩËã±ËØ≠Ê†áÂáÜËØ≠ÊñôÂ∫ì&lt;/a&gt;„ÄÇÂ§ßÈáèËã±ËØ≠ÂçïËØçÊ†∑Êú¨„ÄÇ
&lt;a href="https://github.com/ciprian-chelba/1-billion-word-language-modeling-benchmark"&gt;Ë∞∑Ê≠å10‰∫øÂ≠óËØ≠ÊñôÂ∫ì&lt;/a&gt;„ÄÇ&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Êñ∞ËØçÂèëÁé∞&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;‰∏≠ÊñáÂàÜËØçÊñ∞ËØçÂèëÁé∞&lt;/li&gt;
&lt;li&gt;python3Âà©Áî®‰∫í‰ø°ÊÅØÂíåÂ∑¶Âè≥‰ø°ÊÅØÁÜµÁöÑ‰∏≠ÊñáÂàÜËØçÊñ∞ËØçÂèëÁé∞&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/zhanzecheng/Chinese_segment_augment"&gt;https://github.com/zhanzecheng/Chinese_segment_augment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Âè•Â≠êÁõ∏‰ººÂ∫¶ËØÜÂà´&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;È°πÁõÆÂú∞ÂùÄ: &lt;a href="https://www.kaggle.com/c/quora-question-pairs" rel="nofollow"&gt;https://www.kaggle.com/c/quora-question-pairs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ëß£ÂÜ≥ÊñπÊ°à: word2vec + Bi-GRU&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;ÊñáÊú¨Á∫†Èîô&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;bi-gram + levenshtein&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-3ÂõæÂÉèÂ≠óÂπïimage-captioning" class="anchor" aria-hidden="true" href="#3ÂõæÂÉèÂ≠óÂπïimage-captioning"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;3.ÂõæÂÉèÂ≠óÂπïÔºàImage CaptioningÔºâ&lt;/h4&gt;
&lt;p&gt;mageÂ≠óÂπïÊòØ‰∏∫ÁªôÂÆöÂõæÂÉèÁîüÊàêÊñáÊú¨ÊèèËø∞ÁöÑ‰ªªÂä°„ÄÇ&lt;/p&gt;
&lt;p&gt;‰∏ãÈù¢ÊòØ‰∏Ä‰∫õÂæàÂ•ΩÁöÑÂàùÂ≠¶ËÄÖÂõæÂÉèÂ≠óÂπïÊï∞ÊçÆÈõÜ„ÄÇ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://mscoco.org/dataset/#overview" rel="nofollow"&gt;‰∏ä‰∏ãÊñá‰∏≠ÁöÑÂÖ¨ÂÖ±ÂØπË±°ÔºàCOCOÔºâ&lt;/a&gt;„ÄÇÂåÖÂê´Ë∂ÖËøá12‰∏áÂº†Â∏¶ÊèèËø∞ÁöÑÂõæÂÉèÁöÑÈõÜÂêà&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html" rel="nofollow"&gt;Flickr 8K&lt;/a&gt;„ÄÇ‰ªéflickr.comËé∑ÂèñÁöÑ8ÂçÉ‰∏™ÊèèËø∞ÂõæÂÉèÁöÑÈõÜÂêà„ÄÇ&lt;/li&gt;
&lt;li&gt;&lt;a href="http://shannon.cs.illinois.edu/DenotationGraph/" rel="nofollow"&gt;Flickr 30K&lt;/a&gt;„ÄÇ‰ªéflickr.comËé∑ÂèñÁöÑ3‰∏á‰∏™ÊèèËø∞ÂõæÂÉèÁöÑÈõÜÂêà„ÄÇ
Ê¨≤‰∫ÜËß£Êõ¥Â§öÔºåËØ∑ÁúãÂ∏ñÂ≠êÔºö&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="http://sidgan.me/technical/2016/01/09/Exploring-Datasets" rel="nofollow"&gt;Êé¢Á¥¢ÂõæÂÉèÂ≠óÂπïÊï∞ÊçÆÈõÜÔºå2016Âπ¥&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-4Êú∫Âô®ÁøªËØëmachine-translation" class="anchor" aria-hidden="true" href="#4Êú∫Âô®ÁøªËØëmachine-translation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;4.Êú∫Âô®ÁøªËØëÔºàMachine TranslationÔºâ&lt;/h4&gt;
&lt;p&gt;Êú∫Âô®ÁøªËØëÊòØÂ∞ÜÊñáÊú¨‰ªé‰∏ÄÁßçËØ≠Ë®ÄÁøªËØëÊàêÂè¶‰∏ÄÁßçËØ≠Ë®ÄÁöÑ‰ªªÂä°„ÄÇ&lt;/p&gt;
&lt;p&gt;‰∏ãÈù¢ÊòØ‰∏Ä‰∫õÂæàÂ•ΩÁöÑÂàùÂ≠¶ËÄÖÊú∫Âô®ÁøªËØëÊï∞ÊçÆÈõÜ„ÄÇ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.isi.edu/natural-language/download/hansard/" rel="nofollow"&gt;Âä†ÊãøÂ§ßÁ¨¨36Â±äËÆÆ‰ºöÁöÑÂçèË∞ÉÂõΩ‰ºöËÆÆÂëò&lt;/a&gt;„ÄÇÊàêÂØπÁöÑËã±ËØ≠ÂíåÊ≥ïËØ≠Âè•Â≠ê„ÄÇ&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.statmt.org/europarl/" rel="nofollow"&gt;Ê¨ßÊ¥≤ËÆÆ‰ºöËØâËÆºÂπ≥Ë°åËØ≠ÊñôÂ∫ì1996-2011&lt;/a&gt;„ÄÇÂè•Â≠êÂØπ‰∏ÄÂ•óÊ¨ßÊ¥≤ËØ≠Ë®Ä„ÄÇ
ÊúâÂ§ßÈáèÊ†áÂáÜÊï∞ÊçÆÈõÜÁî®‰∫éÂπ¥Â∫¶Êú∫Âô®ÁøªËØëÊåëÊàò;¬†ÁúãÂà∞Ôºö&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="http://www.statmt.org/" rel="nofollow"&gt;ÁªüËÆ°Êú∫Âô®ÁøªËØë&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Êú∫Âô®ÁøªËØë&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Encoder + Decoder(Attention)&lt;/li&gt;
&lt;li&gt;ÂèÇËÄÉÊ°à‰æã: &lt;a href="http://pytorch.apachecn.org/cn/tutorials/intermediate/seq2seq_translation_tutorial.html" rel="nofollow"&gt;http://pytorch.apachecn.org/cn/tutorials/intermediate/seq2seq_translation_tutorial.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-5ÈóÆÁ≠îÁ≥ªÁªüquestion-answering" class="anchor" aria-hidden="true" href="#5ÈóÆÁ≠îÁ≥ªÁªüquestion-answering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;5.ÈóÆÁ≠îÁ≥ªÁªüÔºàQuestion AnsweringÔºâ&lt;/h4&gt;
&lt;p&gt;ÈóÆÁ≠îÊòØ‰∏ÄÈ°π‰ªªÂä°ÔºåÂÖ∂‰∏≠Êèê‰æõ‰∫Ü‰∏Ä‰∏™Âè•Â≠êÊàñÊñáÊú¨Ê†∑Êú¨Ôºå‰ªé‰∏≠ÊèêÂá∫ÈóÆÈ¢òÂπ∂‰∏îÂøÖÈ°ªÂõûÁ≠îÈóÆÈ¢ò„ÄÇ&lt;/p&gt;
&lt;p&gt;‰∏ãÈù¢ÊòØ‰∏Ä‰∫õÂæàÂ•ΩÁöÑÂàùÂ≠¶ËÄÖÈóÆÈ¢òÂõûÁ≠îÊï∞ÊçÆÈõÜ„ÄÇ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;ÊñØÂù¶Á¶èÈóÆÈ¢òÂõûÁ≠îÊï∞ÊçÆÈõÜÔºàSQuADÔºâ&lt;/a&gt;„ÄÇÂõûÁ≠îÊúâÂÖ≥Áª¥Âü∫ÁôæÁßëÊñáÁ´†ÁöÑÈóÆÈ¢ò„ÄÇ&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/deepmind/rc-data"&gt;DeepmindÈóÆÈ¢òÂõûÁ≠îËØ≠ÊñôÂ∫ì&lt;/a&gt;„ÄÇ‰ªéÊØèÊó•ÈÇÆÊä•ÂõûÁ≠îÊúâÂÖ≥Êñ∞ÈóªÊñáÁ´†ÁöÑÈóÆÈ¢ò„ÄÇ&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jmcauley.ucsd.edu/data/amazon/qa/" rel="nofollow"&gt;‰∫öÈ©¨ÈÄäÈóÆÁ≠îÊï∞ÊçÆ&lt;/a&gt;„ÄÇÂõûÁ≠îÊúâÂÖ≥‰∫öÈ©¨ÈÄä‰∫ßÂìÅÁöÑÈóÆÈ¢ò„ÄÇ
ÊúâÂÖ≥Êõ¥Â§ö‰ø°ÊÅØÔºåËØ∑ÂèÇÈòÖÂ∏ñÂ≠êÔºö&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="https://www.quora.com/Datasets-How-can-I-get-corpus-of-a-question-answering-website-like-Quora-or-Yahoo-Answers-or-Stack-Overflow-for-analyzing-answer-quality" rel="nofollow"&gt;Êï∞ÊçÆÈõÜÔºöÊàëÂ¶Ç‰ΩïËé∑ÂæóÈóÆÁ≠îÁΩëÁ´ôÁöÑËØ≠ÊñôÂ∫ìÔºåÂ¶ÇQuoraÊàñYahoo AnswersÊàñStack OverflowÊù•ÂàÜÊûêÁ≠îÊ°àË¥®ÈáèÔºü&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-6ËØ≠Èü≥ËØÜÂà´speech-recognition" class="anchor" aria-hidden="true" href="#6ËØ≠Èü≥ËØÜÂà´speech-recognition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;6.ËØ≠Èü≥ËØÜÂà´ÔºàSpeech RecognitionÔºâ&lt;/h4&gt;
&lt;p&gt;ËØ≠Èü≥ËØÜÂà´ÊòØÂ∞ÜÂè£ËØ≠ÁöÑÈü≥È¢ëËΩ¨Êç¢‰∏∫‰∫∫Á±ªÂèØËØªÊñáÊú¨ÁöÑ‰ªªÂä°„ÄÇ&lt;/p&gt;
&lt;p&gt;‰∏ãÈù¢ÊòØ‰∏Ä‰∫õÂæàÂ•ΩÁöÑÂàùÂ≠¶ËÄÖËØ≠Èü≥ËØÜÂà´Êï∞ÊçÆÈõÜ„ÄÇ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://catalog.ldc.upenn.edu/LDC93S1" rel="nofollow"&gt;TIMITÂ£∞Â≠¶ - ËØ≠Èü≥ËøûÁª≠ËØ≠Èü≥ËØ≠ÊñôÂ∫ì&lt;/a&gt;„ÄÇ‰∏çÊòØÂÖçË¥πÁöÑÔºå‰ΩÜÂõ†ÂÖ∂ÂπøÊ≥õ‰ΩøÁî®ËÄå‰∏äÂ∏Ç„ÄÇÂè£ËØ≠ÁæéÂõΩËã±ËØ≠ÂíåÁõ∏ÂÖ≥ÁöÑËΩ¨ÂΩï„ÄÇ&lt;/li&gt;
&lt;li&gt;&lt;a href="http://voxforge.org/" rel="nofollow"&gt;VoxForge&lt;/a&gt;„ÄÇÁî®‰∫éÊûÑÂª∫Áî®‰∫éËØ≠Èü≥ËØÜÂà´ÁöÑÂºÄÊ∫êÊï∞ÊçÆÂ∫ìÁöÑÈ°πÁõÆ„ÄÇ&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.openslr.org/12/" rel="nofollow"&gt;LibriSpeech ASRËØ≠ÊñôÂ∫ì&lt;/a&gt;„ÄÇ‰ªéLibriVoxÊî∂ÈõÜÁöÑÂ§ßÈáèËã±ËØ≠ÊúâÂ£∞ËØªÁâ©„ÄÇ&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;&lt;a id="user-content-7Ëá™Âä®ÊñáÊëòdocument-summarization" class="anchor" aria-hidden="true" href="#7Ëá™Âä®ÊñáÊëòdocument-summarization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;7.Ëá™Âä®ÊñáÊëòÔºàDocument SummarizationÔºâ&lt;/h4&gt;
&lt;p&gt;ÊñáÊ°£ÊëòË¶ÅÊòØÂàõÂª∫ËæÉÂ§ßÊñáÊ°£ÁöÑÁÆÄÁü≠ÊúâÊÑè‰πâÊèèËø∞ÁöÑ‰ªªÂä°„ÄÇ&lt;/p&gt;
&lt;p&gt;‰∏ãÈù¢ÊòØ‰∏Ä‰∫õÂæàÂ•ΩÁöÑÂàùÂ≠¶ËÄÖÊñáÊ°£ÊëòË¶ÅÊï∞ÊçÆÈõÜ„ÄÇ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports" rel="nofollow"&gt;Ê≥ïÂæãÊ°à‰æãÊä•ÂëäÊï∞ÊçÆÈõÜ&lt;/a&gt;„ÄÇÊî∂ÈõÜ‰∫Ü4000‰ªΩÊ≥ïÂæãÊ°à‰ª∂ÂèäÂÖ∂ÊëòË¶Å„ÄÇ&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www-nlpir.nist.gov/related_projects/tipster_summac/cmp_lg.html" rel="nofollow"&gt;TIPSTERÊñáÊú¨ÊëòË¶ÅËØÑ‰º∞‰ºöËÆÆËØ≠ÊñôÂ∫ì&lt;/a&gt;„ÄÇÊî∂ÈõÜ‰∫ÜËøë200‰ªΩÊñá‰ª∂ÂèäÂÖ∂ÊëòË¶Å„ÄÇ&lt;/li&gt;
&lt;li&gt;&lt;a href="https://catalog.ldc.upenn.edu/LDC2002T31" rel="nofollow"&gt;Ëã±ËØ≠Êñ∞ÈóªÊñáÊú¨ÁöÑAQUAINTËØ≠ÊñôÂ∫ì&lt;/a&gt;„ÄÇ‰∏çÊòØÂÖçË¥πÁöÑÔºåËÄåÊòØÂπøÊ≥õ‰ΩøÁî®ÁöÑ„ÄÇÊñ∞ÈóªÊñáÁ´†ÁöÑËØ≠ÊñôÂ∫ì„ÄÇ
Ê¨≤‰∫ÜËß£Êõ¥Â§ö‰ø°ÊÅØÔºö&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="http://www-nlpir.nist.gov/projects/duc/data.html" rel="nofollow"&gt;ÊñáÊ°£ÁêÜËß£‰ºöËÆÆÔºàDUCÔºâ‰ªªÂä°&lt;/a&gt;„ÄÇ
&lt;a href="https://www.quora.com/Where-can-I-find-good-data-sets-for-text-summarization" rel="nofollow"&gt;Âú®Âì™ÈáåÂèØ‰ª•ÊâæÂà∞Áî®‰∫éÊñáÊú¨ÊëòË¶ÅÁöÑËâØÂ•ΩÊï∞ÊçÆÈõÜÔºü&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ÂëΩÂêçÂÆû‰ΩìËØÜÂà´&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Bi-LSTM CRF&lt;/li&gt;
&lt;li&gt;ÂèÇËÄÉÊ°à‰æã: &lt;a href="http://pytorch.apachecn.org/cn/tutorials/beginner/nlp/advanced_tutorial.html" rel="nofollow"&gt;http://pytorch.apachecn.org/cn/tutorials/beginner/nlp/advanced_tutorial.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CRFÊé®ËçêÊñáÊ°£: &lt;a href="https://www.jianshu.com/p/55755fc649b1" rel="nofollow"&gt;https://www.jianshu.com/p/55755fc649b1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;ÊñáÊú¨ÊëòË¶Å&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ÊäΩÂèñÂºè&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;word2vec + textrank&lt;/li&gt;
&lt;li&gt;word2vecÊé®ËçêÊñáÊ°£: &lt;a href="https://www.zhihu.com/question/44832436/answer/266068967" rel="nofollow"&gt;https://www.zhihu.com/question/44832436/answer/266068967&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;textrankÊé®ËçêÊñáÊ°£: &lt;a href="https://blog.csdn.net/BaiHuaXiu123/article/details/77847232" rel="nofollow"&gt;https://blog.csdn.net/BaiHuaXiu123/article/details/77847232&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-graphÂõæËÆ°ÁÆóÊÖ¢ÊÖ¢Êõ¥Êñ∞" class="anchor" aria-hidden="true" href="#graphÂõæËÆ°ÁÆóÊÖ¢ÊÖ¢Êõ¥Êñ∞"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GraphÂõæËÆ°ÁÆó„ÄêÊÖ¢ÊÖ¢Êõ¥Êñ∞„Äë&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Êï∞ÊçÆÈõÜ: &lt;a href="data/nlp/graph"&gt;data/nlp/graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Â≠¶‰π†ËµÑÊñô: spark graphXÂÆûÊàò.pdf „ÄêÊñá‰ª∂Â§™Â§ß‰∏çÊñπ‰æøÊèê‰æõÔºåËá™Â∑±ÁôæÂ∫¶„Äë&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-Áü•ËØÜÂõæË∞±" class="anchor" aria-hidden="true" href="#Áü•ËØÜÂõæË∞±"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Áü•ËØÜÂõæË∞±&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Áü•ËØÜÂõæË∞±ÔºåÊàëÂè™ËÆ§ &lt;a href="https://www.zhihu.com/people/simmerchan" rel="nofollow"&gt;SimmerChan&lt;/a&gt;: &lt;a href="https://zhuanlan.zhihu.com/knowledgegraph" rel="nofollow"&gt;„ÄêÁü•ËØÜÂõæË∞±-ÁªôAIË£Ö‰∏™Â§ßËÑë„Äë&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ËØ¥ÂÆûËØùÔºåÊàëÊòØÁúãËøôÂçö‰∏ªËÄÅÂì•ÂÜôÁöÑÂçöÂÆ¢ÈïøÂ§ßÁöÑÔºåÂÜôÁöÑÁúüÁöÑÊòØÊ∑±ÂÖ•ÊµÖÂá∫„ÄÇÊàëÂæàÂñúÊ¨¢ÔºåÊâÄ‰ª•Â∞±ÂàÜ‰∫´ÁªôÂ§ßÂÆ∂ÔºåÂ∏åÊúõ‰Ω†‰ª¨‰πüÂñúÊ¨¢„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-Ëøõ‰∏ÄÊ≠•ÈòÖËØª" class="anchor" aria-hidden="true" href="#Ëøõ‰∏ÄÊ≠•ÈòÖËØª"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ëøõ‰∏ÄÊ≠•ÈòÖËØª&lt;/h3&gt;
&lt;p&gt;Â¶ÇÊûúÊÇ®Â∏åÊúõÊõ¥Ê∑±ÂÖ•ÔºåÊú¨ËäÇÊèê‰æõ‰∫ÜÂÖ∂‰ªñÊï∞ÊçÆÈõÜÂàóË°®„ÄÇ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#Text_data" rel="nofollow"&gt;Áª¥Âü∫ÁôæÁßëÁ†îÁ©∂‰∏≠‰ΩøÁî®ÁöÑÊñáÊú¨Êï∞ÊçÆÈõÜ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/Datasets-What-are-the-major-text-corpora-used-by-computational-linguists-and-natural-language-processing-researchers-and-what-are-the-characteristics-biases-of-each-corpus" rel="nofollow"&gt;Êï∞ÊçÆÈõÜÔºöËÆ°ÁÆóËØ≠Ë®ÄÂ≠¶ÂÆ∂ÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁ†îÁ©∂‰∫∫Âëò‰ΩøÁî®ÁöÑ‰∏ªË¶ÅÊñáÊú¨ËØ≠ÊñôÂ∫ìÊòØ‰ªÄ‰πàÔºü&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nlp.stanford.edu/links/statnlp.html#Corpora" rel="nofollow"&gt;ÊñØÂù¶Á¶èÁªüËÆ°Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËØ≠ÊñôÂ∫ì&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/niderhoff/nlp-datasets"&gt;ÊåâÂ≠óÊØçÈ°∫Â∫èÊéíÂàóÁöÑNLPÊï∞ÊçÆÈõÜÂàóË°®&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nltk.org/nltk_data/" rel="nofollow"&gt;ËØ•Êú∫ÊûÑNLTK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://deeplearning4j.org/opendata" rel="nofollow"&gt;Âú®DL4J‰∏äÊâìÂºÄÊ∑±Â∫¶Â≠¶‰π†Êï∞ÊçÆ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/caesar0301/awesome-public-datasets#natural-language"&gt;NLPÊï∞ÊçÆÈõÜ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ÂõΩÂÜÖÂºÄÊîæÊï∞ÊçÆÈõÜ: &lt;a href="https://bosonnlp.com/dev/resource" rel="nofollow"&gt;https://bosonnlp.com/dev/resource&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-È°πÁõÆË¥üË¥£‰∫∫" class="anchor" aria-hidden="true" href="#È°πÁõÆË¥üË¥£‰∫∫"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;È°πÁõÆË¥üË¥£‰∫∫&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml Á¨¨‰∏ÄÊúü (2017-02-27)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jiangzhonglian"&gt;@ÁâáÂàª&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangyangting"&gt;@ÈÇ£‰ºäÊäπÂæÆÁ¨ë&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chenyyx"&gt;@Áë∂Â¶π&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/report/2017-04-08_%E7%AC%AC%E4%B8%80%E6%9C%9F%E7%9A%84%E6%80%BB%E7%BB%93.md"&gt;2017-04-08_Á¨¨‰∏ÄÊúüÁöÑÊÄªÁªì&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml Á¨¨‰∫åÊúü (2017-08-14)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/jiangzhonglian"&gt;@ÁâáÂàª&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangyangting"&gt;@ÈÇ£‰ºäÊäπÂæÆÁ¨ë&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/chenyyx"&gt;@Áë∂Â¶π&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mikechengwei"&gt;@Mike&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml Á¨¨‰∏âÊúü (2018-04-16)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-È°πÁõÆË¥°ÁåÆËÄÖ" class="anchor" aria-hidden="true" href="#È°πÁõÆË¥°ÁåÆËÄÖ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;È°πÁõÆË¥°ÁåÆËÄÖ&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml Á¨¨‰∏ÄÊúü (2017-02-27)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/geekidentity"&gt;@‰æØÊ≥ïË∂Ö&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hello19883"&gt;@hello19883&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sheepmen"&gt;@ÂæêÈë´&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/highfei2011"&gt;@ibe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml Á¨¨‰∫åÊúü (2017-08-14)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/LeeMoonCh"&gt;@Arithmetic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/caopeirui"&gt;@Veyron C&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Cugtyt"&gt;@Cugtyt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/hey-bruce"&gt;@BBruceyuan&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml Á¨¨‰∏âÊúü (2018-04-16)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-Áæ§ÁÆ°ÁêÜÂëòÊç¢Â±ä" class="anchor" aria-hidden="true" href="#Áæ§ÁÆ°ÁêÜÂëòÊç¢Â±ä"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Áæ§ÁÆ°ÁêÜÂëòÊç¢Â±ä&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/chenyyx"&gt;@Áë∂Â¶π&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wizardforcel"&gt;@È£ûÈæô&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jiangzhonglian"&gt;@ÁâáÂàª&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Watermelon233"&gt;@‰º™ÊñáËâ∫.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/wangyangting"&gt;@ÈÇ£‰ºäÊäπÂæÆÁ¨ë&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@LAMDA-ÂÅ•ÂøòÁóá&lt;/a&gt; Ê∞∏‰πÖÁïô‰ªª-ÈùûÂ∏∏ÊÑüË∞¢ÂØπÁæ§ÁöÑË¥°ÁåÆ&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml Á¨¨‰∏ÄÂ±ä (2017-09-01)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;@ÊòìÊº†&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mikechengwei"&gt;@Mike&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@Books&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@ÊùéÂ≠üÁ¶π&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@Âº†ÂÅáÈ£û&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@Glassy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@Á∫¢Ëâ≤Áü≥Â§¥&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@ÂæÆÂÖâÂêåÂ∞ò&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml Á¨¨‰∫åÂ±ä (2018-07-04)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;@Âº†ÂÅáÈ£û&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@ÊùéÂ≠üÁ¶π&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@Â∞èÊòéÊïô‰∏ª&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@Âπ≥Ê∑°ÁöÑÂ§©&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@ÂáåÂ∞ëskier„Çû&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;@„Åò‚òÜŒΩ–ÅÂùêÁúã‰∫ëËµ∑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Âè§Êü≥-DesertsX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;woodchuck&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Ëá™Áî±Á≤æÁÅµ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Ê•öÁõü&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;99ÊùÜÊ∏ÖÂè∞&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Êó∂Á©∫ÂÆàÊúõËÄÖ@&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Âè™ÊÉ≥ÂèëËÆ∫ÊñáÁöÑÊ∏£Ê∏£&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;ÁõÆÊ†á: mlÂäùÈÄÄ‰∏ìÂÆ∂&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml Á¨¨‰∏âÂ±ä (2019-01-01)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;Âè™‰ºöÂñä666ÁöÑÂ≠òÂú®&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;codefun007.xyz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;ËçºÈù°&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Â§ßÈ±º&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;ÈùíÈ∏ü&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Âè§Êü≥-DesertsX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Edge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Alluka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;‰∏çÂèëÁØápaper‰∏çÊîπÂêçÁâá&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;FontTian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Bigjing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;‰ªÅ Á§º Êô∫ Áà±&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;ÂèØÂï™ÁöÑÂ∞è‰πñÂèó&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;ËÄÅÂè§Ëë£&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Êó∂Á©∫ÂÆàÊúõËÄÖ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;ÊàëÂ•ΩËèúÂïä&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Messi¬†19&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;ËêåJayÂ∞èÂÖ¨‰∏æ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Ml Á¨¨ÂõõÂ±ä (2019-06-01)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=""&gt;‰ΩõÂ≠¶Áà±Â•ΩËÄÖ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Ê•öÁõü&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;codefun007.xyz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Â§ßÈ±º-Áæ§Ëä±-Â£∞‰ºò&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Â§ßÊµ∑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Edge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;if only&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;ÊùéÂ≠üÁ¶π&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Âπ≥Èùô&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;‰ªªÂä°ÂÅö‰∏çÂÆå&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;‰ªÅÁ§ºÊô∫Áà±&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Âõ≠Êó∂Á©∫ÂÆàÊúõËÄÖ@&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;ÂùêÁúã‰∫ëËµ∑&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;ÈòøËä±ÂêõÈú∏Âç†Ë∑Ø‰∫∫&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;ÁÉ¶ÁÑñÈ∏°&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Âè§Êü≥-DesertsX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;ÈùíÈ∏ü(ÊúçÂä°Âëò)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Â∞èÊòéÊïô‰∏ª&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;zhiqing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;SrL.z&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Ê¨¢ËøéË¥°ÁåÆËÄÖ‰∏çÊñ≠ÁöÑËøΩÂä†&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ÂÖçË¥£Â£∞Êòé---Âè™‰æõÂ≠¶‰π†ÂèÇËÄÉ" class="anchor" aria-hidden="true" href="#ÂÖçË¥£Â£∞Êòé---Âè™‰æõÂ≠¶‰π†ÂèÇËÄÉ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÂÖçË¥£Â£∞Êòé - „ÄêÂè™‰æõÂ≠¶‰π†ÂèÇËÄÉ„Äë&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ApacheCN Á∫ØÁ≤πÂá∫‰∫éÂ≠¶‰π†ÁõÆÁöÑ‰∏é‰∏™‰∫∫ÂÖ¥Ë∂£ÁøªËØëÊú¨‰π¶&lt;/li&gt;
&lt;li&gt;ApacheCN ‰øùÁïôÂØπÊ≠§ÁâàÊú¨ËØëÊñáÁöÑÁΩ≤ÂêçÊùÉÂèäÂÖ∂ÂÆÉÁõ∏ÂÖ≥ÊùÉÂà©&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ÂçèËÆÆ" class="anchor" aria-hidden="true" href="#ÂçèËÆÆ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;ÂçèËÆÆ&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;‰ª•ÂêÑÈ°πÁõÆÂçèËÆÆ‰∏∫ÂáÜ„ÄÇ&lt;/li&gt;
&lt;li&gt;ApacheCN Ë¥¶Âè∑‰∏ãÊ≤°ÊúâÂçèËÆÆÁöÑÈ°πÁõÆÔºå‰∏ÄÂæãËßÜ‰∏∫ &lt;a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="nofollow"&gt;CC BY-NC-SA 4.0&lt;/a&gt;„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-ËµÑÊñôÊù•Ê∫ê" class="anchor" aria-hidden="true" href="#ËµÑÊñôÊù•Ê∫ê"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ËµÑÊñôÊù•Ê∫ê:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;„ÄêÊØîËµõÊî∂ÈõÜÂπ≥Âè∞„Äë: &lt;a href="https://github.com/iphysresearch/DataSciComp"&gt;https://github.com/iphysresearch/DataSciComp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pbharrin/machinelearninginaction"&gt;https://github.com/pbharrin/machinelearninginaction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/datasets-natural-language-processing" rel="nofollow"&gt;https://machinelearningmastery.com/datasets-natural-language-processing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ÊÑüË∞¢‰ø°" class="anchor" aria-hidden="true" href="#ÊÑüË∞¢‰ø°"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÊÑüË∞¢‰ø°&lt;/h2&gt;
&lt;p&gt;ÊúÄËøëÊó†ÊÑèÊî∂Âà∞Áæ§ÂèãÊé®ÈÄÅÁöÑÈìæÊé•ÔºåÂèëÁé∞ÂæóÂà∞Â§ß‰Ω¨È´òÂ∫¶ÁöÑËÆ§ÂèØÔºåÂπ∂Âú®ÁÉ≠ÂøÉÁöÑÊé®Âπø&lt;/p&gt;
&lt;p&gt;Âú®Ê≠§ÊÑüË∞¢:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.zhihu.com/org/liang-zi-wei-48" rel="nofollow"&gt;ÈáèÂ≠ê‰Ωç&lt;/a&gt;: &lt;a href="https://www.zhihu.com/question/20472776/answer/691646493" rel="nofollow"&gt;https://www.zhihu.com/question/20472776/answer/691646493&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;‰∫∫Â∑•Êô∫ËÉΩÂâçÊ≤øËÆ≤‰π†: &lt;a href="https://mp.weixin.qq.com/s/f2dqulxOPkt7k5hqPsydyQ" rel="nofollow"&gt;https://mp.weixin.qq.com/s/f2dqulxOPkt7k5hqPsydyQ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ËµûÂä©Êàë‰ª¨" class="anchor" aria-hidden="true" href="#ËµûÂä©Êàë‰ª¨"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ËµûÂä©Êàë‰ª¨&lt;/h2&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/2814efae28977e977f631af3a30acfe4e9089dd9/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f61626f75742f646f6e6174652e6a7067"&gt;&lt;img src="https://camo.githubusercontent.com/2814efae28977e977f631af3a30acfe4e9089dd9/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f61626f75742f646f6e6174652e6a7067" alt="ÂæÆ‰ø°&amp;amp;ÊîØ‰ªòÂÆù" data-canonical-src="http://data.apachecn.org/img/about/donate.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;ÁâπÂà´ËµûÂä©ÂïÜ(Ê¨¢Ëøé‚ÄúÁßÅËÅä‚ÄùËµûÂä©)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td align="center" valign="middle"&gt;
            &lt;a href="https://coding.net/?utm_source=ApacheCN&amp;amp;utm_medium=banner&amp;amp;utm_campaign=march2019" rel="nofollow"&gt;
              &lt;img width="1080" src="https://camo.githubusercontent.com/8d33a9d36a6822434ce78147cdb7cb41aba56a02/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f5370656369616c53706f6e736f72732f436f64696e674e65742e706e67" data-canonical-src="http://data.apachecn.org/img/SpecialSponsors/CodingNet.png" style="max-width:100%;"&gt;
            &lt;/a&gt;
          &lt;/td&gt;
      &lt;/tr&gt;&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>apachecn</author><guid isPermaLink="false">https://github.com/apachecn/AiLearning</guid><pubDate>Tue, 19 Nov 2019 00:05:00 GMT</pubDate></item><item><title>wangzheng0822/algo #6 in Python, This month</title><link>https://github.com/wangzheng0822/algo</link><description>&lt;p&gt;&lt;i&gt;Êï∞ÊçÆÁªìÊûÑÂíåÁÆóÊ≥ïÂøÖÁü•ÂøÖ‰ºöÁöÑ50‰∏™‰ª£Á†ÅÂÆûÁé∞&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-Êï∞ÊçÆÁªìÊûÑÂíåÁÆóÊ≥ïÂøÖÁü•ÂøÖ‰ºöÁöÑ50‰∏™‰ª£Á†ÅÂÆûÁé∞" class="anchor" aria-hidden="true" href="#Êï∞ÊçÆÁªìÊûÑÂíåÁÆóÊ≥ïÂøÖÁü•ÂøÖ‰ºöÁöÑ50‰∏™‰ª£Á†ÅÂÆûÁé∞"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Êï∞ÊçÆÁªìÊûÑÂíåÁÆóÊ≥ïÂøÖÁü•ÂøÖ‰ºöÁöÑ50‰∏™‰ª£Á†ÅÂÆûÁé∞&lt;/h1&gt;
&lt;h3&gt;&lt;a id="user-content-ÂæÆ‰ø°ÊêúÁ¥¢ÊàëÁöÑÂÖ¨‰ºóÂè∑Â∞è‰∫âÂì•ÊàñËÄÖÂæÆ‰ø°Êâ´Êèè‰∏ãÈù¢‰∫åÁª¥Á†Å-Ëé∑ÂèñÊõ¥Â§öÂéãÁÆ±Â∫ïÁöÑÂπ≤Ë¥ßÂàÜ‰∫´" class="anchor" aria-hidden="true" href="#ÂæÆ‰ø°ÊêúÁ¥¢ÊàëÁöÑÂÖ¨‰ºóÂè∑Â∞è‰∫âÂì•ÊàñËÄÖÂæÆ‰ø°Êâ´Êèè‰∏ãÈù¢‰∫åÁª¥Á†Å-Ëé∑ÂèñÊõ¥Â§öÂéãÁÆ±Â∫ïÁöÑÂπ≤Ë¥ßÂàÜ‰∫´"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÂæÆ‰ø°ÊêúÁ¥¢ÊàëÁöÑÂÖ¨‰ºóÂè∑‚ÄúÂ∞è‰∫âÂì•‚ÄùÔºåÊàñËÄÖÂæÆ‰ø°Êâ´Êèè‰∏ãÈù¢‰∫åÁª¥Á†Å, Ëé∑ÂèñÊõ¥Â§öÂéãÁÆ±Â∫ïÁöÑÂπ≤Ë¥ßÂàÜ‰∫´&lt;/h3&gt;
&lt;h3&gt;&lt;a id="user-content-ÂâçgoogleÂ∑•Á®ãÂ∏à5‰∏á‰∫∫Ë∑üÁùÄÂ≠¶ÁöÑÊï∞ÊçÆÁªìÊûÑÂíåÁÆóÊ≥ï‰πãÁæé‰∏ìÊ†è‰ΩúËÄÖ" class="anchor" aria-hidden="true" href="#ÂâçgoogleÂ∑•Á®ãÂ∏à5‰∏á‰∫∫Ë∑üÁùÄÂ≠¶ÁöÑÊï∞ÊçÆÁªìÊûÑÂíåÁÆóÊ≥ï‰πãÁæé‰∏ìÊ†è‰ΩúËÄÖ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÂâçGoogleÂ∑•Á®ãÂ∏àÔºå5‰∏á+‰∫∫Ë∑üÁùÄÂ≠¶ÁöÑ„ÄäÊï∞ÊçÆÁªìÊûÑÂíåÁÆóÊ≥ï‰πãÁæé„Äã‰∏ìÊ†è‰ΩúËÄÖ&lt;/h3&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/wangzheng0822/markdownphotos/blob/master/pics/qrcode_for_gh_9b0e7afdff20_258.jpg"&gt;&lt;img src="https://github.com/wangzheng0822/markdownphotos/raw/master/pics/qrcode_for_gh_9b0e7afdff20_258.jpg" alt="t2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-Êï∞ÁªÑ" class="anchor" aria-hidden="true" href="#Êï∞ÁªÑ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Êï∞ÁªÑ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ÂÆûÁé∞‰∏Ä‰∏™ÊîØÊåÅÂä®ÊÄÅÊâ©ÂÆπÁöÑÊï∞ÁªÑ&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞‰∏Ä‰∏™Â§ßÂ∞èÂõ∫ÂÆöÁöÑÊúâÂ∫èÊï∞ÁªÑÔºåÊîØÊåÅÂä®ÊÄÅÂ¢ûÂà†ÊîπÊìç‰Ωú&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞‰∏§‰∏™ÊúâÂ∫èÊï∞ÁªÑÂêàÂπ∂‰∏∫‰∏Ä‰∏™ÊúâÂ∫èÊï∞ÁªÑ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ÈìæË°®" class="anchor" aria-hidden="true" href="#ÈìæË°®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÈìæË°®&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ÂÆûÁé∞ÂçïÈìæË°®„ÄÅÂæ™ÁéØÈìæË°®„ÄÅÂèåÂêëÈìæË°®ÔºåÊîØÊåÅÂ¢ûÂà†Êìç‰Ωú&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞ÂçïÈìæË°®ÂèçËΩ¨&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞‰∏§‰∏™ÊúâÂ∫èÁöÑÈìæË°®ÂêàÂπ∂‰∏∫‰∏Ä‰∏™ÊúâÂ∫èÈìæË°®&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞Ê±ÇÈìæË°®ÁöÑ‰∏≠Èó¥ÁªìÁÇπ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-Ê†à" class="anchor" aria-hidden="true" href="#Ê†à"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ê†à&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Áî®Êï∞ÁªÑÂÆûÁé∞‰∏Ä‰∏™È°∫Â∫èÊ†à&lt;/li&gt;
&lt;li&gt;Áî®ÈìæË°®ÂÆûÁé∞‰∏Ä‰∏™ÈìæÂºèÊ†à&lt;/li&gt;
&lt;li&gt;ÁºñÁ®ãÊ®°ÊãüÂÆûÁé∞‰∏Ä‰∏™ÊµèËßàÂô®ÁöÑÂâçËøõ„ÄÅÂêéÈÄÄÂäüËÉΩ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ÈòüÂàó" class="anchor" aria-hidden="true" href="#ÈòüÂàó"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÈòüÂàó&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Áî®Êï∞ÁªÑÂÆûÁé∞‰∏Ä‰∏™È°∫Â∫èÈòüÂàó&lt;/li&gt;
&lt;li&gt;Áî®ÈìæË°®ÂÆûÁé∞‰∏Ä‰∏™ÈìæÂºèÈòüÂàó&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞‰∏Ä‰∏™Âæ™ÁéØÈòüÂàó&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ÈÄíÂΩí" class="anchor" aria-hidden="true" href="#ÈÄíÂΩí"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÈÄíÂΩí&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ÁºñÁ®ãÂÆûÁé∞ÊñêÊ≥¢ÈÇ£Â•ëÊï∞ÂàóÊ±ÇÂÄºf(n)=f(n-1)+f(n-2)&lt;/li&gt;
&lt;li&gt;ÁºñÁ®ãÂÆûÁé∞Ê±ÇÈò∂‰πòn!&lt;/li&gt;
&lt;li&gt;ÁºñÁ®ãÂÆûÁé∞‰∏ÄÁªÑÊï∞ÊçÆÈõÜÂêàÁöÑÂÖ®ÊéíÂàó&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ÊéíÂ∫è" class="anchor" aria-hidden="true" href="#ÊéíÂ∫è"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÊéíÂ∫è&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ÂÆûÁé∞ÂΩíÂπ∂ÊéíÂ∫è„ÄÅÂø´ÈÄüÊéíÂ∫è„ÄÅÊèíÂÖ•ÊéíÂ∫è„ÄÅÂÜíÊ≥°ÊéíÂ∫è„ÄÅÈÄâÊã©ÊéíÂ∫è&lt;/li&gt;
&lt;li&gt;ÁºñÁ®ãÂÆûÁé∞O(n)Êó∂Èó¥Â§çÊùÇÂ∫¶ÂÜÖÊâæÂà∞‰∏ÄÁªÑÊï∞ÊçÆÁöÑÁ¨¨KÂ§ßÂÖÉÁ¥†&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-‰∫åÂàÜÊü•Êâæ" class="anchor" aria-hidden="true" href="#‰∫åÂàÜÊü•Êâæ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;‰∫åÂàÜÊü•Êâæ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ÂÆûÁé∞‰∏Ä‰∏™ÊúâÂ∫èÊï∞ÁªÑÁöÑ‰∫åÂàÜÊü•ÊâæÁÆóÊ≥ï&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞Ê®°Á≥ä‰∫åÂàÜÊü•ÊâæÁÆóÊ≥ïÔºàÊØîÂ¶ÇÂ§ß‰∫éÁ≠â‰∫éÁªôÂÆöÂÄºÁöÑÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†Ôºâ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-Êï£ÂàóË°®" class="anchor" aria-hidden="true" href="#Êï£ÂàóË°®"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Êï£ÂàóË°®&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ÂÆûÁé∞‰∏Ä‰∏™Âü∫‰∫éÈìæË°®Ê≥ïËß£ÂÜ≥ÂÜ≤Á™ÅÈóÆÈ¢òÁöÑÊï£ÂàóË°®&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞‰∏Ä‰∏™LRUÁºìÂ≠òÊ∑òÊ±∞ÁÆóÊ≥ï&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-Â≠óÁ¨¶‰∏≤" class="anchor" aria-hidden="true" href="#Â≠óÁ¨¶‰∏≤"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Â≠óÁ¨¶‰∏≤&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ÂÆûÁé∞‰∏Ä‰∏™Â≠óÁ¨¶ÈõÜÔºåÂè™ÂåÖÂê´aÔΩûzËøô26‰∏™Ëã±ÊñáÂ≠óÊØçÁöÑTrieÊ†ë&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞Êú¥Á¥†ÁöÑÂ≠óÁ¨¶‰∏≤ÂåπÈÖçÁÆóÊ≥ï&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-‰∫åÂèâÊ†ë" class="anchor" aria-hidden="true" href="#‰∫åÂèâÊ†ë"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;‰∫åÂèâÊ†ë&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ÂÆûÁé∞‰∏Ä‰∏™‰∫åÂèâÊü•ÊâæÊ†ëÔºåÂπ∂‰∏îÊîØÊåÅÊèíÂÖ•„ÄÅÂà†Èô§„ÄÅÊü•ÊâæÊìç‰Ωú&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞Êü•Êâæ‰∫åÂèâÊü•ÊâæÊ†ë‰∏≠Êüê‰∏™ËäÇÁÇπÁöÑÂêéÁªß„ÄÅÂâçÈ©±ËäÇÁÇπ&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞‰∫åÂèâÊ†ëÂâç„ÄÅ‰∏≠„ÄÅÂêéÂ∫è‰ª•ÂèäÊåâÂ±ÇÈÅçÂéÜ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-Â†Ü" class="anchor" aria-hidden="true" href="#Â†Ü"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Â†Ü&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ÂÆûÁé∞‰∏Ä‰∏™Â∞èÈ°∂Â†Ü„ÄÅÂ§ßÈ°∂Â†Ü„ÄÅ‰ºòÂÖàÁ∫ßÈòüÂàó&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞Â†ÜÊéíÂ∫è&lt;/li&gt;
&lt;li&gt;Âà©Áî®‰ºòÂÖàÁ∫ßÈòüÂàóÂêàÂπ∂K‰∏™ÊúâÂ∫èÊï∞ÁªÑ&lt;/li&gt;
&lt;li&gt;Ê±Ç‰∏ÄÁªÑÂä®ÊÄÅÊï∞ÊçÆÈõÜÂêàÁöÑÊúÄÂ§ßTop K&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-Âõæ" class="anchor" aria-hidden="true" href="#Âõæ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Âõæ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ÂÆûÁé∞ÊúâÂêëÂõæ„ÄÅÊó†ÂêëÂõæ„ÄÅÊúâÊùÉÂõæ„ÄÅÊó†ÊùÉÂõæÁöÑÈÇªÊé•Áü©ÈòµÂíåÈÇªÊé•Ë°®Ë°®Á§∫ÊñπÊ≥ï&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞ÂõæÁöÑÊ∑±Â∫¶‰ºòÂÖàÊêúÁ¥¢„ÄÅÂπøÂ∫¶‰ºòÂÖàÊêúÁ¥¢&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞DijkstraÁÆóÊ≥ï„ÄÅA*ÁÆóÊ≥ï&lt;/li&gt;
&lt;li&gt;ÂÆûÁé∞ÊãìÊâëÊéíÂ∫èÁöÑKahnÁÆóÊ≥ï„ÄÅDFSÁÆóÊ≥ï&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ÂõûÊ∫Ø" class="anchor" aria-hidden="true" href="#ÂõûÊ∫Ø"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÂõûÊ∫Ø&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Âà©Áî®ÂõûÊ∫ØÁÆóÊ≥ïÊ±ÇËß£ÂÖ´ÁöáÂêéÈóÆÈ¢ò&lt;/li&gt;
&lt;li&gt;Âà©Áî®ÂõûÊ∫ØÁÆóÊ≥ïÊ±ÇËß£0-1ËÉåÂåÖÈóÆÈ¢ò&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ÂàÜÊ≤ª" class="anchor" aria-hidden="true" href="#ÂàÜÊ≤ª"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÂàÜÊ≤ª&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Âà©Áî®ÂàÜÊ≤ªÁÆóÊ≥ïÊ±Ç‰∏ÄÁªÑÊï∞ÊçÆÁöÑÈÄÜÂ∫èÂØπ‰∏™Êï∞&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-Âä®ÊÄÅËßÑÂàí" class="anchor" aria-hidden="true" href="#Âä®ÊÄÅËßÑÂàí"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Âä®ÊÄÅËßÑÂàí&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;0-1ËÉåÂåÖÈóÆÈ¢ò&lt;/li&gt;
&lt;li&gt;ÊúÄÂ∞èË∑ØÂæÑÂíå&lt;/li&gt;
&lt;li&gt;ÁºñÁ®ãÂÆûÁé∞Ëé±ÊñáÊñØÂù¶ÊúÄÁü≠ÁºñËæëË∑ùÁ¶ª&lt;/li&gt;
&lt;li&gt;ÁºñÁ®ãÂÆûÁé∞Êü•Êâæ‰∏§‰∏™Â≠óÁ¨¶‰∏≤ÁöÑÊúÄÈïøÂÖ¨ÂÖ±Â≠êÂ∫èÂàó&lt;/li&gt;
&lt;li&gt;ÁºñÁ®ãÂÆûÁé∞‰∏Ä‰∏™Êï∞ÊçÆÂ∫èÂàóÁöÑÊúÄÈïøÈÄíÂ¢ûÂ≠êÂ∫èÂàó&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>wangzheng0822</author><guid isPermaLink="false">https://github.com/wangzheng0822/algo</guid><pubDate>Tue, 19 Nov 2019 00:06:00 GMT</pubDate></item><item><title>home-assistant/home-assistant #7 in Python, This month</title><link>https://github.com/home-assistant/home-assistant</link><description>&lt;p&gt;&lt;i&gt;:house_with_garden: Open source home automation that puts local control and privacy first&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-home-assistant-" class="anchor" aria-hidden="true" href="#home-assistant-"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Home Assistant &lt;a href="https://discord.gg/c5DvZ4e" rel="nofollow"&gt;&lt;img alt="Chat Status" src="https://camo.githubusercontent.com/599662b08725231a9f847723b9bc4f6dc4757d0c/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3333303934343233383931303936333731342e737667" data-canonical-src="https://img.shields.io/discord/330944238910963714.svg" style="max-width:100%;"&gt;
&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Home Assistant is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control.&lt;/p&gt;
&lt;p&gt;To get started:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 -m pip install homeassistant
hass --open-ui&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Check out &lt;a href="https://home-assistant.io" rel="nofollow"&gt;home-assistant.io&lt;/a&gt; for &lt;a href="https://home-assistant.io/demo/" rel="nofollow"&gt;a
demo&lt;/a&gt;, &lt;a href="https://home-assistant.io/getting-started/" rel="nofollow"&gt;installation instructions&lt;/a&gt;,
&lt;a href="https://home-assistant.io/getting-started/automation-2/" rel="nofollow"&gt;tutorials&lt;/a&gt; and &lt;a href="https://home-assistant.io/docs/" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://home-assistant.io/demo/" rel="nofollow"&gt;&lt;img alt="screenshot-states" src="https://camo.githubusercontent.com/99578d7bca06d9c2973c2564e06f1ca444a4cce1/68747470733a2f2f7261772e6769746875622e636f6d2f686f6d652d617373697374616e742f686f6d652d617373697374616e742f6d61737465722f646f63732f73637265656e73686f74732e706e67" data-canonical-src="https://raw.github.com/home-assistant/home-assistant/master/docs/screenshots.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a name="user-content-featured-integrations"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-featured-integrations" class="anchor" aria-hidden="true" href="#featured-integrations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Featured integrations&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://home-assistant.io/integrations/" rel="nofollow"&gt;&lt;img alt="screenshot-components" src="https://camo.githubusercontent.com/ba21a6029ccb81d4a26b1ad9c198e61d01a07e7a/68747470733a2f2f7261772e6769746875622e636f6d2f686f6d652d617373697374616e742f686f6d652d617373697374616e742f6465762f646f63732f73637265656e73686f742d636f6d706f6e656e74732e706e67" data-canonical-src="https://raw.github.com/home-assistant/home-assistant/dev/docs/screenshot-components.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The system is built using a modular approach so support for other devices or actions can be implemented easily. See also the &lt;a href="https://developers.home-assistant.io/docs/en/architecture_index.html" rel="nofollow"&gt;section on architecture&lt;/a&gt; and the &lt;a href="https://developers.home-assistant.io/docs/en/creating_component_index.html" rel="nofollow"&gt;section on creating your own
components&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you run into issues while using Home Assistant or during development
of a component, check the &lt;a href="https://home-assistant.io/help/" rel="nofollow"&gt;Home Assistant help section&lt;/a&gt; of our website for further help and information.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>home-assistant</author><guid isPermaLink="false">https://github.com/home-assistant/home-assistant</guid><pubDate>Tue, 19 Nov 2019 00:07:00 GMT</pubDate></item><item><title>sundowndev/PhoneInfoga #8 in Python, This month</title><link>https://github.com/sundowndev/PhoneInfoga</link><description>&lt;p&gt;&lt;i&gt;Advanced information gathering &amp; OSINT tool for phone numbers&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/d3fc46eb3ce41e8df8b75fabf670cc20eef2545a/68747470733a2f2f692e696d6775722e636f6d2f4c7455476e46332e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/d3fc46eb3ce41e8df8b75fabf670cc20eef2545a/68747470733a2f2f692e696d6775722e636f6d2f4c7455476e46332e706e67" width="500" data-canonical-src="https://i.imgur.com/LtUGnF3.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a href="https://travis-ci.org/sundowndev/PhoneInfoga" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/46d99e4427dc377be912f425a6527c6785733524/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f73756e646f776e6465762f50686f6e65496e666f67612f6d61737465722e7376673f7374796c653d666c61742d737175617265" alt="Build Status" data-canonical-src="https://img.shields.io/travis/sundowndev/PhoneInfoga/master.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://hub.docker.com/r/sundowndev/phoneinfoga/builds" rel="nofollow"&gt;
    &lt;img src="https://camo.githubusercontent.com/117d6963fac80d7ad53079b83a8eb326aabb0c16/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f636c6f75642f6275696c642f73756e646f776e6465762f70686f6e65696e666f67612e7376673f7374796c653d666c61742d737175617265" alt="Build Status" data-canonical-src="https://img.shields.io/docker/cloud/build/sundowndev/phoneinfoga.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="#"&gt;
    &lt;img src="https://camo.githubusercontent.com/7d89761ee8648eceb9db7eba9b49cd3c985f9c3d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d626c75652e7376673f7374796c653d666c61742d737175617265" alt="Python version" data-canonical-src="https://img.shields.io/badge/python-3.6-blue.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/sundowndev/PhoneInfoga/releases"&gt;
    &lt;img src="https://camo.githubusercontent.com/a4003bf13d0817cc46214e39aa5f5957c1ac549a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f53756e646f776e4445562f50686f6e65496e666f67612e7376673f7374796c653d666c61742d737175617265" alt="Latest version" data-canonical-src="https://img.shields.io/github/release/SundownDEV/PhoneInfoga.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
  &lt;a href="https://github.com/sundowndev/PhoneInfoga/blob/master/LICENSE"&gt;
    &lt;img src="https://camo.githubusercontent.com/46c222ff7fbbf26999df33d732182ec6307e69f8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f73756e646f776e6465762f50686f6e65496e666f67612e7376673f7374796c653d666c61742d737175617265" alt="License" data-canonical-src="https://img.shields.io/github/license/sundowndev/PhoneInfoga.svg?style=flat-square" style="max-width:100%;"&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;h4 align="center"&gt;&lt;a id="user-content-information-gathering--osint-reconnaissance-tool-for-phone-numbers" class="anchor" aria-hidden="true" href="#information-gathering--osint-reconnaissance-tool-for-phone-numbers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Information gathering &amp;amp; OSINT reconnaissance tool for phone numbers&lt;/h4&gt;
&lt;p align="center"&gt;
  &lt;a href="https://sundowndev.github.io/PhoneInfoga/" rel="nofollow"&gt;Documentation&lt;/a&gt; ‚Ä¢
  &lt;a href="https://sundowndev.github.io/PhoneInfoga/usage/" rel="nofollow"&gt;Basic usage&lt;/a&gt; ‚Ä¢
  &lt;a href="https://sundowndev.github.io/PhoneInfoga/resources/" rel="nofollow"&gt;OSINT resources&lt;/a&gt; ‚Ä¢
  &lt;a href="https://medium.com/@SundownDEV/phone-number-scanning-osint-recon-tool-6ad8f0cac27b" rel="nofollow"&gt;Related blog post&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-about" class="anchor" aria-hidden="true" href="#about"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;About&lt;/h2&gt;
&lt;p&gt;PhoneInfoga is one of the most advanced tools to scan phone numbers using only free resources. The goal is to first gather standard information such as country, area, carrier and line type on any international phone numbers with a very good accuracy. Then search for footprints on search engines to try to find the VoIP provider or identify the owner.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Check if phone number exists and is possible&lt;/li&gt;
&lt;li&gt;Gather standard informations such as country, line type and carrier&lt;/li&gt;
&lt;li&gt;OSINT footprinting using external APIs, Google Hacking, phone books &amp;amp; search engines&lt;/li&gt;
&lt;li&gt;Check for reputation reports, social media, disposable numbers and more&lt;/li&gt;
&lt;li&gt;Scan several numbers at once&lt;/li&gt;
&lt;li&gt;Use custom formatting for more effective OSINT reconnaissance&lt;/li&gt;
&lt;li&gt;Automatic footprinting on several custom formats&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/48694e177cacb5bdbcda70d8dcdd57c1c8e1f512/68747470733a2f2f692e696d6775722e636f6d2f71436b677a7a382e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/48694e177cacb5bdbcda70d8dcdd57c1c8e1f512/68747470733a2f2f692e696d6775722e636f6d2f71436b677a7a382e706e67" alt="Footprinting process" data-canonical-src="https://i.imgur.com/qCkgzz8.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;This tool is licensed under the GNU General Public License v3.0.&lt;/p&gt;
&lt;p&gt;Some parts of this code comes from &lt;a href="https://github.com/m4ll0k/infoga"&gt;Infoga&lt;/a&gt;, another project licensed under GPLv3.0.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.flaticon.com/free-icon/fingerprint-search-symbol-of-secret-service-investigation_48838" rel="nofollow"&gt;Icon&lt;/a&gt; made by &lt;a href="https://www.freepik.com/" title="Freepik" rel="nofollow"&gt;Freepik&lt;/a&gt; from &lt;a href="https://www.flaticon.com/" title="Flaticon" rel="nofollow"&gt;flaticon.com&lt;/a&gt; is licensed by &lt;a href="http://creativecommons.org/licenses/by/3.0/" title="Creative Commons BY 3.0" rel="nofollow"&gt;CC 3.0 BY&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://app.fossa.io/projects/git%2Bgithub.com%2Fsundowndev%2FPhoneInfoga?ref=badge_large" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c1bf6c1d6350a0785cc8b07161bcc1cb587e7063/68747470733a2f2f6170702e666f7373612e696f2f6170692f70726f6a656374732f6769742532426769746875622e636f6d25324673756e646f776e64657625324650686f6e65496e666f67612e7376673f747970653d6c61726765" alt="FOSSA Status" data-canonical-src="https://app.fossa.io/api/projects/git%2Bgithub.com%2Fsundowndev%2FPhoneInfoga.svg?type=large" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>sundowndev</author><guid isPermaLink="false">https://github.com/sundowndev/PhoneInfoga</guid><pubDate>Tue, 19 Nov 2019 00:08:00 GMT</pubDate></item><item><title>google-research/bert #9 in Python, This month</title><link>https://github.com/google-research/bert</link><description>&lt;p&gt;&lt;i&gt;TensorFlow code and pre-trained models for BERT&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-bert" class="anchor" aria-hidden="true" href="#bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BERT&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;***** New May 31st, 2019: Whole Word Masking Models *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a release of several new models which were the result of an improvement
the pre-processing code.&lt;/p&gt;
&lt;p&gt;In the original pre-processing code, we randomly select WordPiece tokens to
mask. For example:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Input Text: the man jumped up , put his basket on phil ##am ##mon ' s head&lt;/code&gt;
&lt;code&gt;Original Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The new technique is called Whole Word Masking. In this case, we always mask
&lt;em&gt;all&lt;/em&gt; of the the tokens corresponding to a word at once. The overall masking
rate remains the same.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The training is identical -- we still predict each masked WordPiece token
independently. The improvement comes from the fact that the original prediction
task was too 'easy' for words that had been split into multiple WordPieces.&lt;/p&gt;
&lt;p&gt;This can be enabled during data generation by passing the flag
&lt;code&gt;--do_whole_word_mask=True&lt;/code&gt; to &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Pre-trained models with Whole Word Masking are linked below. The data and
training were otherwise identical, and the models have identical structure and
vocab to the original models. We only include BERT-Large models. When using
these models, please make it clear in the paper that you are using the Whole
Word Masking variant of BERT-Large.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;SQUAD 1.1 F1/EM&lt;/th&gt;
&lt;th align="center"&gt;Multi NLI Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.0/84.3&lt;/td&gt;
&lt;td align="center"&gt;86.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.8/86.7&lt;/td&gt;
&lt;td align="center"&gt;87.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Original)&lt;/td&gt;
&lt;td align="center"&gt;91.5/84.8&lt;/td&gt;
&lt;td align="center"&gt;86.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BERT-Large, Cased (Whole Word Masking)&lt;/td&gt;
&lt;td align="center"&gt;92.9/86.7&lt;/td&gt;
&lt;td align="center"&gt;86.46&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;***** New February 7th, 2019: TfHub Module *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;BERT has been uploaded to &lt;a href="https://tfhub.dev" rel="nofollow"&gt;TensorFlow Hub&lt;/a&gt;. See
&lt;code&gt;run_classifier_with_tfhub.py&lt;/code&gt; for an example of how to use the TF Hub module,
or run an example in the browser on
&lt;a href="https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb" rel="nofollow"&gt;Colab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 23rd, 2018: Un-normalized multilingual model + Thai +
Mongolian *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We uploaded a new multilingual model which does &lt;em&gt;not&lt;/em&gt; perform any normalization
on the input (no lower casing, accent stripping, or Unicode normalization), and
additionally inclues Thai and Mongolian.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It is recommended to use this version for developing multilingual models,
especially on languages with non-Latin alphabets.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This does not require any code changes, and can be downloaded here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;***** New November 15th, 2018: SOTA SQuAD 2.0 System *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which is
currently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of the
README for details.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 5th, 2018: Third-party PyTorch and Chainer versions of
BERT available *****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;NLP researchers from HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. Sosuke Kobayashi also made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
(Thanks!) We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** New November 3rd, 2018: Multilingual and Chinese models available
*****&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have made two new BERT models available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use character-based tokenization for Chinese, and WordPiece tokenization for
all other languages. Both models should work out-of-the-box without any code
changes. We did update the implementation of &lt;code&gt;BasicTokenizer&lt;/code&gt; in
&lt;code&gt;tokenization.py&lt;/code&gt; to support Chinese character tokenization, so please update if
you forked it. However, we did not change the tokenization API.&lt;/p&gt;
&lt;p&gt;For more, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;***** End new information *****&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;BERT&lt;/strong&gt;, or &lt;strong&gt;B&lt;/strong&gt;idirectional &lt;strong&gt;E&lt;/strong&gt;ncoder &lt;strong&gt;R&lt;/strong&gt;epresentations from
&lt;strong&gt;T&lt;/strong&gt;ransformers, is a new method of pre-training language representations which
obtains state-of-the-art results on a wide array of Natural Language Processing
(NLP) tasks.&lt;/p&gt;
&lt;p&gt;Our academic paper which describes BERT in detail and provides full results on a
number of tasks can be found here:
&lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To give a few numbers, here are the results on the
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD v1.1&lt;/a&gt; question answering
task:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SQuAD v1.1 Leaderboard (Oct 8th 2018)&lt;/th&gt;
&lt;th align="center"&gt;Test EM&lt;/th&gt;
&lt;th align="center"&gt;Test F1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Ensemble - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;87.4&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;93.2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Ensemble - nlnet&lt;/td&gt;
&lt;td align="center"&gt;86.0&lt;/td&gt;
&lt;td align="center"&gt;91.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1st Place Single Model - BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;85.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.8&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2nd Place Single Model - nlnet&lt;/td&gt;
&lt;td align="center"&gt;83.5&lt;/td&gt;
&lt;td align="center"&gt;90.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And several natural language inference tasks:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th align="center"&gt;MultiNLI&lt;/th&gt;
&lt;th align="center"&gt;Question NLI&lt;/th&gt;
&lt;th align="center"&gt;SWAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BERT&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.7&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;91.1&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;86.3&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenAI GPT (Prev. SOTA)&lt;/td&gt;
&lt;td align="center"&gt;82.2&lt;/td&gt;
&lt;td align="center"&gt;88.1&lt;/td&gt;
&lt;td align="center"&gt;75.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Plus many other tasks.&lt;/p&gt;
&lt;p&gt;Moreover, these results were all obtained with almost no task-specific neural
network architecture design.&lt;/p&gt;
&lt;p&gt;If you already know what BERT is and you just want to get started, you can
&lt;a href="#pre-trained-models"&gt;download the pre-trained models&lt;/a&gt; and
&lt;a href="#fine-tuning-with-bert"&gt;run a state-of-the-art fine-tuning&lt;/a&gt; in only a few
minutes.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-is-bert" class="anchor" aria-hidden="true" href="#what-is-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What is BERT?&lt;/h2&gt;
&lt;p&gt;BERT is a method of pre-training language representations, meaning that we train
a general-purpose "language understanding" model on a large text corpus (like
Wikipedia), and then use that model for downstream NLP tasks that we care about
(like question answering). BERT outperforms previous methods because it is the
first &lt;em&gt;unsupervised&lt;/em&gt;, &lt;em&gt;deeply bidirectional&lt;/em&gt; system for pre-training NLP.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unsupervised&lt;/em&gt; means that BERT was trained using only a plain text corpus, which
is important because an enormous amount of plain text data is publicly available
on the web in many languages.&lt;/p&gt;
&lt;p&gt;Pre-trained representations can also either be &lt;em&gt;context-free&lt;/em&gt; or &lt;em&gt;contextual&lt;/em&gt;,
and contextual representations can further be &lt;em&gt;unidirectional&lt;/em&gt; or
&lt;em&gt;bidirectional&lt;/em&gt;. Context-free models such as
&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec" rel="nofollow"&gt;word2vec&lt;/a&gt; or
&lt;a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow"&gt;GloVe&lt;/a&gt; generate a single "word
embedding" representation for each word in the vocabulary, so &lt;code&gt;bank&lt;/code&gt; would have
the same representation in &lt;code&gt;bank deposit&lt;/code&gt; and &lt;code&gt;river bank&lt;/code&gt;. Contextual models
instead generate a representation of each word that is based on the other words
in the sentence.&lt;/p&gt;
&lt;p&gt;BERT was built upon recent work in pre-training contextual representations ‚Äî
including &lt;a href="https://arxiv.org/abs/1511.01432" rel="nofollow"&gt;Semi-supervised Sequence Learning&lt;/a&gt;,
&lt;a href="https://blog.openai.com/language-unsupervised/" rel="nofollow"&gt;Generative Pre-Training&lt;/a&gt;,
&lt;a href="https://allennlp.org/elmo" rel="nofollow"&gt;ELMo&lt;/a&gt;, and
&lt;a href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html" rel="nofollow"&gt;ULMFit&lt;/a&gt;
‚Äî but crucially these models are all &lt;em&gt;unidirectional&lt;/em&gt; or &lt;em&gt;shallowly
bidirectional&lt;/em&gt;. This means that each word is only contextualized using the words
to its left (or right). For example, in the sentence &lt;code&gt;I made a bank deposit&lt;/code&gt; the
unidirectional representation of &lt;code&gt;bank&lt;/code&gt; is only based on &lt;code&gt;I made a&lt;/code&gt; but not
&lt;code&gt;deposit&lt;/code&gt;. Some previous work does combine the representations from separate
left-context and right-context models, but only in a "shallow" manner. BERT
represents "bank" using both its left and right context ‚Äî &lt;code&gt;I made a ... deposit&lt;/code&gt;
‚Äî starting from the very bottom of a deep neural network, so it is &lt;em&gt;deeply
bidirectional&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;BERT uses a simple approach for this: We mask out 15% of the words in the input,
run the entire sequence through a deep bidirectional
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; encoder, and then predict only
the masked words. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input: the man went to the [MASK1] . he bought a [MASK2] of milk.
Labels: [MASK1] = store; [MASK2] = gallon
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to learn relationships between sentences, we also train on a simple
task which can be generated from any monolingual corpus: Given two sentences &lt;code&gt;A&lt;/code&gt;
and &lt;code&gt;B&lt;/code&gt;, is &lt;code&gt;B&lt;/code&gt; the actual next sentence that comes after &lt;code&gt;A&lt;/code&gt;, or just a random
sentence from the corpus?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: he bought a gallon of milk .
Label: IsNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: penguins are flightless .
Label: NotNextSentence
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then train a large model (12-layer to 24-layer Transformer) on a large corpus
(Wikipedia + &lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt;) for a long time (1M
update steps), and that's BERT.&lt;/p&gt;
&lt;p&gt;Using BERT has two stages: &lt;em&gt;Pre-training&lt;/em&gt; and &lt;em&gt;fine-tuning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pre-training&lt;/strong&gt; is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a
one-time procedure for each language (current models are English-only, but
multilingual models will be released in the near future). We are releasing a
number of pre-trained models from the paper which were pre-trained at Google.
Most NLP researchers will never need to pre-train their own model from scratch.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt; is inexpensive. All of the results in the paper can be
replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU,
starting from the exact same pre-trained model. SQuAD, for example, can be
trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of
91.0%, which is the single system state-of-the-art.&lt;/p&gt;
&lt;p&gt;The other important aspect of BERT is that it can be adapted to many types of
NLP tasks very easily. In the paper, we demonstrate state-of-the-art results on
sentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level
(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific
modifications.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-what-has-been-released-in-this-repository" class="anchor" aria-hidden="true" href="#what-has-been-released-in-this-repository"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What has been released in this repository?&lt;/h2&gt;
&lt;p&gt;We are releasing the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow code for the BERT model architecture (which is mostly a standard
&lt;a href="https://arxiv.org/abs/1706.03762" rel="nofollow"&gt;Transformer&lt;/a&gt; architecture).&lt;/li&gt;
&lt;li&gt;Pre-trained checkpoints for both the lowercase and cased version of
&lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; from the paper.&lt;/li&gt;
&lt;li&gt;TensorFlow code for push-button replication of the most important
fine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of the code in this repository works out-of-the-box with CPU, GPU, and Cloud
TPU.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-trained-models" class="anchor" aria-hidden="true" href="#pre-trained-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-trained models&lt;/h2&gt;
&lt;p&gt;We are releasing the &lt;code&gt;BERT-Base&lt;/code&gt; and &lt;code&gt;BERT-Large&lt;/code&gt; models from the paper.
&lt;code&gt;Uncased&lt;/code&gt; means that the text has been lowercased before WordPiece tokenization,
e.g., &lt;code&gt;John Smith&lt;/code&gt; becomes &lt;code&gt;john smith&lt;/code&gt;. The &lt;code&gt;Uncased&lt;/code&gt; model also strips out any
accent markers. &lt;code&gt;Cased&lt;/code&gt; means that the true case and accent markers are
preserved. Typically, the &lt;code&gt;Uncased&lt;/code&gt; model is better unless you know that case
information is important for your task (e.g., Named Entity Recognition or
Part-of-Speech tagging).&lt;/p&gt;
&lt;p&gt;These models are all released under the same license as the source code (Apache
2.0).&lt;/p&gt;
&lt;p&gt;For information about the Multilingual and Chinese model, see the
&lt;a href="https://github.com/google-research/bert/blob/master/multilingual.md"&gt;Multilingual README&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When using a cased model, make sure to pass &lt;code&gt;--do_lower=False&lt;/code&gt; to the training
scripts. (Or pass &lt;code&gt;do_lower_case=False&lt;/code&gt; directly to &lt;code&gt;FullTokenizer&lt;/code&gt; if you're
using your own script.)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The links to the models are here (right-click, 'Save link as...' on the name):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased (Whole Word Masking)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Uncased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
12-layer, 768-hidden, 12-heads , 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Large, Cased&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Cased (New, recommended)&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Multilingual Uncased (Orig, not recommended)&lt;/code&gt;&lt;/a&gt;
(Not recommended, use &lt;code&gt;Multilingual Cased&lt;/code&gt; instead)&lt;/strong&gt;: 102 languages,
12-layer, 768-hidden, 12-heads, 110M parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="nofollow"&gt;&lt;code&gt;BERT-Base, Chinese&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;:
Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M
parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each .zip file contains three items:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A TensorFlow checkpoint (&lt;code&gt;bert_model.ckpt&lt;/code&gt;) containing the pre-trained
weights (which is actually 3 files).&lt;/li&gt;
&lt;li&gt;A vocab file (&lt;code&gt;vocab.txt&lt;/code&gt;) to map WordPiece to word id.&lt;/li&gt;
&lt;li&gt;A config file (&lt;code&gt;bert_config.json&lt;/code&gt;) which specifies the hyperparameters of
the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-fine-tuning-with-bert" class="anchor" aria-hidden="true" href="#fine-tuning-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with BERT&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: All results on the paper were fine-tuned on a single Cloud TPU,
which has 64GB of RAM. It is currently not possible to re-produce most of the
&lt;code&gt;BERT-Large&lt;/code&gt; results on the paper using a GPU with 12GB - 16GB of RAM, because
the maximum batch size that can fit in memory is too small. We are working on
adding code to this repository which allows for much larger effective batch size
on the GPU. See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for
more details.&lt;/p&gt;
&lt;p&gt;This code was tested with TensorFlow 1.11.0. It was tested with Python2 and
Python3 (but more thoroughly with Python2, since this is what's used internally
in Google).&lt;/p&gt;
&lt;p&gt;The fine-tuning examples which use &lt;code&gt;BERT-Base&lt;/code&gt; should be able to run on a GPU
that has at least 12GB of RAM using the hyperparameters given.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-fine-tuning-with-cloud-tpus" class="anchor" aria-hidden="true" href="#fine-tuning-with-cloud-tpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning with Cloud TPUs&lt;/h3&gt;
&lt;p&gt;Most of the examples below assumes that you will be running training/evaluation
on your local machine, using a GPU like a Titan X or GTX 1080.&lt;/p&gt;
&lt;p&gt;However, if you have access to a Cloud TPU that you want to train on, just add
the following flags to &lt;code&gt;run_classifier.py&lt;/code&gt; or &lt;code&gt;run_squad.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --use_tpu=True \
  --tpu_name=$TPU_NAME
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please see the
&lt;a href="https://cloud.google.com/tpu/docs/tutorials/mnist" rel="nofollow"&gt;Google Cloud TPU tutorial&lt;/a&gt;
for how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;On Cloud TPUs, the pretrained model and the output directory will need to be on
Google Cloud Storage. For example, if you have a bucket named &lt;code&gt;some_bucket&lt;/code&gt;, you
might use the following flags instead:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  --output_dir=gs://some_bucket/my_output_dir/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The unzipped pre-trained model files can also be found in the Google Cloud
Storage folder &lt;code&gt;gs://bert_models/2018_10_18&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export BERT_BASE_DIR=gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-sentence-and-sentence-pair-classification-tasks" class="anchor" aria-hidden="true" href="#sentence-and-sentence-pair-classification-tasks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sentence (and sentence-pair) classification tasks&lt;/h3&gt;
&lt;p&gt;Before running this example you must download the
&lt;a href="https://gluebenchmark.com/tasks" rel="nofollow"&gt;GLUE data&lt;/a&gt; by running
&lt;a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"&gt;this script&lt;/a&gt;
and unpack it to some directory &lt;code&gt;$GLUE_DIR&lt;/code&gt;. Next, download the &lt;code&gt;BERT-Base&lt;/code&gt;
checkpoint and unzip it to some directory &lt;code&gt;$BERT_BASE_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This example code fine-tunes &lt;code&gt;BERT-Base&lt;/code&gt; on the Microsoft Research Paraphrase
Corpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in a
few minutes on most GPUs.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue

python run_classifier.py \
  --task_name=MRPC \
  --do_train=true \
  --do_eval=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  eval_accuracy = 0.845588
  eval_loss = 0.505248
  global_step = 343
  loss = 0.505248
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means that the Dev set accuracy was 84.55%. Small sets like MRPC have a
high variance in the Dev set accuracy, even when starting from the same
pre-training checkpoint. If you re-run multiple times (making sure to point to
different &lt;code&gt;output_dir&lt;/code&gt;), you should see results between 84% and 88%.&lt;/p&gt;
&lt;p&gt;A few other pre-trained models are implemented off-the-shelf in
&lt;code&gt;run_classifier.py&lt;/code&gt;, so it should be straightforward to follow those examples to
use BERT for any single-sentence or sentence-pair classification task.&lt;/p&gt;
&lt;p&gt;Note: You might see a message &lt;code&gt;Running train on CPU&lt;/code&gt;. This really just means
that it's running on something other than a Cloud TPU, which includes a GPU.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-prediction-from-classifier" class="anchor" aria-hidden="true" href="#prediction-from-classifier"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Prediction from classifier&lt;/h4&gt;
&lt;p&gt;Once you have trained your classifier you can use it in inference mode by using
the --do_predict=true command. You need to have a file named test.tsv in the
input folder. Output will be created in file called test_results.tsv in the
output folder. Each line will contain output for each sample, columns are the
class probabilities.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue
&lt;span class="pl-k"&gt;export&lt;/span&gt; TRAINED_CLASSIFIER=/path/to/fine/tuned/classifier

python run_classifier.py \
  --task_name=MRPC \
  --do_predict=true \
  --data_dir=&lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$TRAINED_CLASSIFIER&lt;/span&gt; \
  --max_seq_length=128 \
  --output_dir=/tmp/mrpc_output/&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-squad-11" class="anchor" aria-hidden="true" href="#squad-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 1.1&lt;/h3&gt;
&lt;p&gt;The Stanford Question Answering Dataset (SQuAD) is a popular question answering
benchmark dataset. BERT (at the time of the release) obtains state-of-the-art
results on SQuAD with almost no task-specific network architecture modifications
or data augmentation. However, it does require semi-complex data pre-processing
and post-processing to deal with (a) the variable-length nature of SQuAD context
paragraphs, and (b) the character-level answer annotations which are used for
SQuAD training. This processing is implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD, you will first need to download the dataset. The
&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="nofollow"&gt;SQuAD website&lt;/a&gt; does not seem to
link to the v1.1 datasets any longer, but the necessary files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json" rel="nofollow"&gt;train-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json" rel="nofollow"&gt;dev-v1.1.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py"&gt;evaluate-v1.1.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The state-of-the-art SQuAD results from the paper currently cannot be reproduced
on a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 does
not seem to fit on a 12GB GPU using &lt;code&gt;BERT-Large&lt;/code&gt;). However, a reasonably strong
&lt;code&gt;BERT-Base&lt;/code&gt; model can be trained on the GPU with these hyperparameters:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=12 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=/tmp/squad_base/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The dev set predictions will be saved into a file called &lt;code&gt;predictions.json&lt;/code&gt; in
the &lt;code&gt;output_dir&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/evaluate-v1.1.py &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json ./squad/predictions.json&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Which should produce an output like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 88.41249612335034, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 81.2488174077578}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see a result similar to the 88.5% reported in the paper for
&lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you have access to a Cloud TPU, you can train with &lt;code&gt;BERT-Large&lt;/code&gt;. Here is a
set of hyperparameters (slightly different than the paper) which consistently
obtain around 90.5%-91.0% F1 single-system trained only on SQuAD:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For example, one random run with these parameters produces the following Dev
scores:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 90.87081895814865, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 84.38978240302744}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you fine-tune for one epoch on
&lt;a href="http://nlp.cs.washington.edu/triviaqa/" rel="nofollow"&gt;TriviaQA&lt;/a&gt; before this the results will
be even better, but you will need to convert TriviaQA into the SQuAD json
format.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-squad-20" class="anchor" aria-hidden="true" href="#squad-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SQuAD 2.0&lt;/h3&gt;
&lt;p&gt;This model is also implemented and documented in &lt;code&gt;run_squad.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To run on SQuAD 2.0, you will first need to download the dataset. The necessary
files can be found here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json" rel="nofollow"&gt;train-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json" rel="nofollow"&gt;dev-v2.0.json&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/" rel="nofollow"&gt;evaluate-v2.0.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download these to some directory &lt;code&gt;$SQUAD_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On Cloud TPU you can run with BERT-Large as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=True \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We assume you have copied everything from the output directory to a local
directory called ./squad/. The initial dev set predictions will be at
./squad/predictions.json and the differences between the score of no answer ("")
and the best non-null answer for each question will be in the file
./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Run this script to tune a threshold for predicting null versus non-null answers:&lt;/p&gt;
&lt;p&gt;python $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json
./squad/predictions.json --na-prob-file ./squad/null_odds.json&lt;/p&gt;
&lt;p&gt;Assume the script outputs "best_f1_thresh" THRESH. (Typical values are between
-1.0 and -5.0). You can now re-run the model to generate predictions with the
derived threshold or alternatively you can extract the appropriate answers from
./squad/nbest_predictions.json.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_squad.py \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_LARGE_DIR&lt;/span&gt;/bert_model.ckpt \
  --do_train=False \
  --train_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v2.0.json \
  --do_predict=True \
  --predict_file=&lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=&lt;span class="pl-smi"&gt;$TPU_NAME&lt;/span&gt; \
  --version_2_with_negative=True \
  --null_score_diff_threshold=&lt;span class="pl-smi"&gt;$THRESH&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-out-of-memory-issues" class="anchor" aria-hidden="true" href="#out-of-memory-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Out-of-memory issues&lt;/h3&gt;
&lt;p&gt;All experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB of
device RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely
to encounter out-of-memory issues if you use the same hyperparameters described
in the paper.&lt;/p&gt;
&lt;p&gt;The factors that affect memory usage are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;max_seq_length&lt;/code&gt;&lt;/strong&gt;: The released models were trained with sequence lengths
up to 512, but you can fine-tune with a shorter max sequence length to save
substantial memory. This is controlled by the &lt;code&gt;max_seq_length&lt;/code&gt; flag in our
example code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;train_batch_size&lt;/code&gt;&lt;/strong&gt;: The memory usage is also directly proportional to
the batch size.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model type, &lt;code&gt;BERT-Base&lt;/code&gt; vs. &lt;code&gt;BERT-Large&lt;/code&gt;&lt;/strong&gt;: The &lt;code&gt;BERT-Large&lt;/code&gt; model
requires significantly more memory than &lt;code&gt;BERT-Base&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimizer&lt;/strong&gt;: The default optimizer for BERT is Adam, which requires a lot
of extra memory to store the &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;v&lt;/code&gt; vectors. Switching to a more memory
efficient optimizer can reduce memory usage, but can also affect the
results. We have not experimented with other optimizers for fine-tuning.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the default training scripts (&lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;run_squad.py&lt;/code&gt;), we
benchmarked the maximum batch size on single Titan X GPU (12GB RAM) with
TensorFlow 1.11.0:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Seq Length&lt;/th&gt;
&lt;th&gt;Max Batch Size&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Base&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;BERT-Large&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;128&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;256&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;320&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;384&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Unfortunately, these max batch sizes for &lt;code&gt;BERT-Large&lt;/code&gt; are so small that they
will actually harm the model accuracy, regardless of the learning rate used. We
are working on adding code to this repository which will allow much larger
effective batch sizes to be used on the GPU. The code will be based on one (or
both) of the following techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gradient accumulation&lt;/strong&gt;: The samples in a minibatch are typically
independent with respect to gradient computation (excluding batch
normalization, which is not used here). This means that the gradients of
multiple smaller minibatches can be accumulated before performing the weight
update, and this will be exactly equivalent to a single larger update.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/openai/gradient-checkpointing"&gt;&lt;strong&gt;Gradient checkpointing&lt;/strong&gt;&lt;/a&gt;:
The major use of GPU/TPU memory during DNN training is caching the
intermediate activations in the forward pass that are necessary for
efficient computation in the backward pass. "Gradient checkpointing" trades
memory for compute time by re-computing the activations in an intelligent
way.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;However, this is not implemented in the current release.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-to-extract-fixed-feature-vectors-like-elmo" class="anchor" aria-hidden="true" href="#using-bert-to-extract-fixed-feature-vectors-like-elmo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT to extract fixed feature vectors (like ELMo)&lt;/h2&gt;
&lt;p&gt;In certain cases, rather than fine-tuning the entire pre-trained model
end-to-end, it can be beneficial to obtained &lt;em&gt;pre-trained contextual
embeddings&lt;/em&gt;, which are fixed contextual representations of each input token
generated from the hidden layers of the pre-trained model. This should also
mitigate most of the out-of-memory issues.&lt;/p&gt;
&lt;p&gt;As an example, we include the script &lt;code&gt;extract_features.py&lt;/code&gt; which can be used
like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Sentence A and Sentence B are separated by the ||| delimiter for sentence&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; pair tasks like question answering and entailment.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; For single sentence inputs, put one sentence per line and DON'T use the&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; delimiter.&lt;/span&gt;
&lt;span class="pl-c1"&gt;echo&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Who was Jim Henson ? ||| Jim Henson was a puppeteer&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; /tmp/input.txt

python extract_features.py \
  --input_file=/tmp/input.txt \
  --output_file=/tmp/output.jsonl \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --layers=-1,-2,-3,-4 \
  --max_seq_length=128 \
  --batch_size=8&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will create a JSON file (one line per line of input) containing the BERT
activations from each Transformer layer specified by &lt;code&gt;layers&lt;/code&gt; (-1 is the final
hidden layer of the Transformer, etc.)&lt;/p&gt;
&lt;p&gt;Note that this script will produce very large output files (by default, around
15kb for every input token).&lt;/p&gt;
&lt;p&gt;If you need to maintain alignment between the original and tokenized words (for
projecting training labels), see the &lt;a href="#tokenization"&gt;Tokenization&lt;/a&gt; section
below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You may see a message like &lt;code&gt;Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict.&lt;/code&gt; This message is expected, it
just means that we are using the &lt;code&gt;init_from_checkpoint()&lt;/code&gt; API rather than the
saved model API. If you don't specify a checkpoint or specify an invalid
checkpoint, this script will complain.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-tokenization" class="anchor" aria-hidden="true" href="#tokenization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;For sentence-level tasks (or sentence-pair) tasks, tokenization is very simple.
Just follow the example code in &lt;code&gt;run_classifier.py&lt;/code&gt; and &lt;code&gt;extract_features.py&lt;/code&gt;.
The basic procedure for sentence-level tasks is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Instantiate an instance of &lt;code&gt;tokenizer = tokenization.FullTokenizer&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tokenize the raw text with &lt;code&gt;tokens = tokenizer.tokenize(raw_text)&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Truncate to the maximum sequence length. (You can use up to 512, but you
probably want to use shorter if possible for memory and speed reasons.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[SEP]&lt;/code&gt; tokens in the right place.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Word-level and span-level tasks (e.g., SQuAD and NER) are more complex, since
you need to maintain alignment between your input text and output text so that
you can project your training labels. SQuAD is a particularly complex example
because the input labels are &lt;em&gt;character&lt;/em&gt;-based, and SQuAD paragraphs are often
longer than our maximum sequence length. See the code in &lt;code&gt;run_squad.py&lt;/code&gt; to show
how we handle this.&lt;/p&gt;
&lt;p&gt;Before we describe the general recipe for handling word-level tasks, it's
important to understand what exactly our tokenizer is doing. It has three main
steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Text normalization&lt;/strong&gt;: Convert all whitespace characters to spaces, and
(for the &lt;code&gt;Uncased&lt;/code&gt; model) lowercase the input and strip out accent markers.
E.g., &lt;code&gt;John Johanson's, ‚Üí john johanson's,&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Punctuation splitting&lt;/strong&gt;: Split &lt;em&gt;all&lt;/em&gt; punctuation characters on both sides
(i.e., add whitespace around all punctuation characters). Punctuation
characters are defined as (a) Anything with a &lt;code&gt;P*&lt;/code&gt; Unicode class, (b) any
non-letter/number/space ASCII character (e.g., characters like &lt;code&gt;$&lt;/code&gt; which are
technically not punctuation). E.g., &lt;code&gt;john johanson's, ‚Üí john johanson ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;WordPiece tokenization&lt;/strong&gt;: Apply whitespace tokenization to the output of
the above procedure, and apply
&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py"&gt;WordPiece&lt;/a&gt;
tokenization to each token separately. (Our implementation is directly based
on the one from &lt;code&gt;tensor2tensor&lt;/code&gt;, which is linked). E.g., &lt;code&gt;john johanson ' s , ‚Üí john johan ##son ' s ,&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The advantage of this scheme is that it is "compatible" with most existing
English tokenizers. For example, imagine that you have a part-of-speech tagging
task which looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input:  John Johanson 's   house
Labels: NNP  NNP      POS NN
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The tokenized output will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Tokens: john johan ##son ' s house
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Crucially, this would be the same output as if the raw text were &lt;code&gt;John Johanson's house&lt;/code&gt; (with no space before the &lt;code&gt;'s&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;If you have a pre-tokenized representation with word-level annotations, you can
simply tokenize each input word independently, and deterministically maintain an
original-to-tokenized alignment:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Input&lt;/span&gt;
orig_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;John&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Johanson&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;'s&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;house&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]
labels      &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NNP&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;,      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;POS&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;NN&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Output&lt;/span&gt;
bert_tokens &lt;span class="pl-k"&gt;=&lt;/span&gt; []

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Token map will be an int -&amp;gt; int mapping between the `orig_tokens` index and&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; the `bert_tokens` index.&lt;/span&gt;
orig_to_tok_map &lt;span class="pl-k"&gt;=&lt;/span&gt; []

tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenization.FullTokenizer(
    &lt;span class="pl-v"&gt;vocab_file&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;vocab_file, &lt;span class="pl-v"&gt;do_lower_case&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[CLS]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-k"&gt;for&lt;/span&gt; orig_token &lt;span class="pl-k"&gt;in&lt;/span&gt; orig_tokens:
  orig_to_tok_map.append(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(bert_tokens))
  bert_tokens.extend(tokenizer.tokenize(orig_token))
bert_tokens.append(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;[SEP]&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; bert_tokens == ["[CLS]", "john", "johan", "##son", "'", "s", "house", "[SEP]"]&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; orig_to_tok_map == [1, 2, 4, 6]&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now &lt;code&gt;orig_to_tok_map&lt;/code&gt; can be used to project &lt;code&gt;labels&lt;/code&gt; to the tokenized
representation.&lt;/p&gt;
&lt;p&gt;There are common English tokenization schemes which will cause a slight mismatch
between how BERT was pre-trained. For example, if your input tokenization splits
off contractions like &lt;code&gt;do n't&lt;/code&gt;, this will cause a mismatch. If it is possible to
do so, you should pre-process your data to convert these back to raw-looking
text, but if it's not possible, this mismatch is likely not a big deal.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pre-training-with-bert" class="anchor" aria-hidden="true" href="#pre-training-with-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training with BERT&lt;/h2&gt;
&lt;p&gt;We are releasing code to do "masked LM" and "next sentence prediction" on an
arbitrary text corpus. Note that this is &lt;em&gt;not&lt;/em&gt; the exact code that was used for
the paper (the original code was written in C++, and had some additional
complexity), but this code does generate pre-training data as described in the
paper.&lt;/p&gt;
&lt;p&gt;Here's how to run the data generation. The input is a plain text file, with one
sentence per line. (It is important that these be actual sentences for the "next
sentence prediction" task). Documents are delimited by empty lines. The output
is a set of &lt;code&gt;tf.train.Example&lt;/code&gt;s serialized into &lt;code&gt;TFRecord&lt;/code&gt; file format.&lt;/p&gt;
&lt;p&gt;You can perform sentence segmentation with an off-the-shelf NLP toolkit such as
&lt;a href="https://spacy.io/" rel="nofollow"&gt;spaCy&lt;/a&gt;. The &lt;code&gt;create_pretraining_data.py&lt;/code&gt; script will
concatenate segments until they reach the maximum sequence length to minimize
computational waste from padding (see the script for more details). However, you
may want to intentionally add a slight amount of noise to your input data (e.g.,
randomly truncate 2% of input segments) to make it more robust to non-sentential
input during fine-tuning.&lt;/p&gt;
&lt;p&gt;This script stores all of the examples for the entire input file in memory, so
for large data files you should shard the input file and call the script
multiple times. (You can pass in a file glob to &lt;code&gt;run_pretraining.py&lt;/code&gt;, e.g.,
&lt;code&gt;tf_examples.tf_record*&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;max_predictions_per_seq&lt;/code&gt; is the maximum number of masked LM predictions per
sequence. You should set this to around &lt;code&gt;max_seq_length&lt;/code&gt; * &lt;code&gt;masked_lm_prob&lt;/code&gt; (the
script doesn't do that automatically because the exact value needs to be passed
to both scripts).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python create_pretraining_data.py \
  --input_file=./sample_text.txt \
  --output_file=/tmp/tf_examples.tfrecord \
  --vocab_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's how to run the pre-training. Do not include &lt;code&gt;init_checkpoint&lt;/code&gt; if you are
pre-training from scratch. The model configuration (including vocab size) is
specified in &lt;code&gt;bert_config_file&lt;/code&gt;. This demo code only pre-trains for a small
number of steps (20), but in practice you will probably want to set
&lt;code&gt;num_train_steps&lt;/code&gt; to 10000 steps or more. The &lt;code&gt;max_seq_length&lt;/code&gt; and
&lt;code&gt;max_predictions_per_seq&lt;/code&gt; parameters passed to &lt;code&gt;run_pretraining.py&lt;/code&gt; must be the
same as &lt;code&gt;create_pretraining_data.py&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python run_pretraining.py \
  --input_file=/tmp/tf_examples.tfrecord \
  --output_dir=/tmp/pretraining_output \
  --do_train=True \
  --do_eval=True \
  --bert_config_file=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_config.json \
  --init_checkpoint=&lt;span class="pl-smi"&gt;$BERT_BASE_DIR&lt;/span&gt;/bert_model.ckpt \
  --train_batch_size=32 \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --num_train_steps=20 \
  --num_warmup_steps=10 \
  --learning_rate=2e-5&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will produce an output like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;***** Eval results *****
  global_step = 20
  loss = 0.0979674
  masked_lm_accuracy = 0.985479
  masked_lm_loss = 0.0979328
  next_sentence_accuracy = 1.0
  next_sentence_loss = 3.45724e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that since our &lt;code&gt;sample_text.txt&lt;/code&gt; file is very small, this example training
will overfit that data in only a few steps and produce unrealistically high
accuracy numbers.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-tips-and-caveats" class="anchor" aria-hidden="true" href="#pre-training-tips-and-caveats"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training tips and caveats&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;If using your own vocabulary, make sure to change &lt;code&gt;vocab_size&lt;/code&gt; in
&lt;code&gt;bert_config.json&lt;/code&gt;. If you use a larger vocabulary without changing this,
you will likely get NaNs when training on GPU or TPU due to unchecked
out-of-bounds access.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;If your task has a large domain-specific corpus available (e.g., "movie
reviews" or "scientific papers"), it will likely be beneficial to run
additional steps of pre-training on your corpus, starting from the BERT
checkpoint.&lt;/li&gt;
&lt;li&gt;The learning rate we used in the paper was 1e-4. However, if you are doing
additional steps of pre-training starting from an existing BERT checkpoint,
you should use a smaller learning rate (e.g., 2e-5).&lt;/li&gt;
&lt;li&gt;Current BERT models are English-only, but we do plan to release a
multilingual model which has been pre-trained on a lot of languages in the
near future (hopefully by the end of November 2018).&lt;/li&gt;
&lt;li&gt;Longer sequences are disproportionately expensive because attention is
quadratic to the sequence length. In other words, a batch of 64 sequences of
length 512 is much more expensive than a batch of 256 sequences of
length 128. The fully-connected/convolutional cost is the same, but the
attention cost is far greater for the 512-length sequences. Therefore, one
good recipe is to pre-train for, say, 90,000 steps with a sequence length of
128 and then for 10,000 additional steps with a sequence length of 512. The
very long sequences are mostly needed to learn positional embeddings, which
can be learned fairly quickly. Note that this does require generating the
data twice with different values of &lt;code&gt;max_seq_length&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you are pre-training from scratch, be prepared that pre-training is
computationally expensive, especially on GPUs. If you are pre-training from
scratch, our recommended recipe is to pre-train a &lt;code&gt;BERT-Base&lt;/code&gt; on a single
&lt;a href="https://cloud.google.com/tpu/docs/pricing" rel="nofollow"&gt;preemptible Cloud TPU v2&lt;/a&gt;, which
takes about 2 weeks at a cost of about $500 USD (based on the pricing in
October 2018). You will have to scale down the batch size when only training
on a single Cloud TPU, compared to what was used in the paper. It is
recommended to use the largest batch size that fits into TPU memory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-pre-training-data" class="anchor" aria-hidden="true" href="#pre-training-data"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pre-training data&lt;/h3&gt;
&lt;p&gt;We will &lt;strong&gt;not&lt;/strong&gt; be able to release the pre-processed datasets used in the paper.
For Wikipedia, the recommended pre-processing is to download
&lt;a href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2" rel="nofollow"&gt;the latest dump&lt;/a&gt;,
extract the text with
&lt;a href="https://github.com/attardi/wikiextractor"&gt;&lt;code&gt;WikiExtractor.py&lt;/code&gt;&lt;/a&gt;, and then apply
any necessary cleanup to convert it into plain text.&lt;/p&gt;
&lt;p&gt;Unfortunately the researchers who collected the
&lt;a href="http://yknzhu.wixsite.com/mbweb" rel="nofollow"&gt;BookCorpus&lt;/a&gt; no longer have it available for
public download. The
&lt;a href="https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html" rel="nofollow"&gt;Project Guttenberg Dataset&lt;/a&gt;
is a somewhat smaller (200M word) collection of older books that are public
domain.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://commoncrawl.org/" rel="nofollow"&gt;Common Crawl&lt;/a&gt; is another very large collection of
text, but you will likely have to do substantial pre-processing and cleanup to
extract a usable corpus for pre-training BERT.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-learning-a-new-wordpiece-vocabulary" class="anchor" aria-hidden="true" href="#learning-a-new-wordpiece-vocabulary"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Learning a new WordPiece vocabulary&lt;/h3&gt;
&lt;p&gt;This repository does not include code for &lt;em&gt;learning&lt;/em&gt; a new WordPiece vocabulary.
The reason is that the code used in the paper was implemented in C++ with
dependencies on Google's internal libraries. For English, it is almost always
better to just start with our vocabulary and pre-trained models. For learning
vocabularies of other languages, there are a number of open source options
available. However, keep in mind that these are not compatible with our
&lt;code&gt;tokenization.py&lt;/code&gt; library:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;Google's SentencePiece library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py"&gt;tensor2tensor's WordPiece generation script&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/rsennrich/subword-nmt"&gt;Rico Sennrich's Byte Pair Encoding library&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-using-bert-in-colab" class="anchor" aria-hidden="true" href="#using-bert-in-colab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using BERT in Colab&lt;/h2&gt;
&lt;p&gt;If you want to use BERT with &lt;a href="https://colab.research.google.com" rel="nofollow"&gt;Colab&lt;/a&gt;, you can
get started with the notebook
"&lt;a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb" rel="nofollow"&gt;BERT FineTuning with Cloud TPUs&lt;/a&gt;".
&lt;strong&gt;At the time of this writing (October 31st, 2018), Colab users can access a
Cloud TPU completely for free.&lt;/strong&gt; Note: One per user, availability limited,
requires a Google Cloud Platform account with storage (although storage may be
purchased with free credit for signing up with GCP), and this capability may not
longer be available in the future. Click on the BERT Colab that was just linked
for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;h4&gt;&lt;a id="user-content-is-this-code-compatible-with-cloud-tpus-what-about-gpus" class="anchor" aria-hidden="true" href="#is-this-code-compatible-with-cloud-tpus-what-about-gpus"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is this code compatible with Cloud TPUs? What about GPUs?&lt;/h4&gt;
&lt;p&gt;Yes, all of the code in this repository works out-of-the-box with CPU, GPU, and
Cloud TPU. However, GPU training is single-GPU only.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-i-am-getting-out-of-memory-errors-what-is-wrong" class="anchor" aria-hidden="true" href="#i-am-getting-out-of-memory-errors-what-is-wrong"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I am getting out-of-memory errors, what is wrong?&lt;/h4&gt;
&lt;p&gt;See the section on &lt;a href="#out-of-memory-issues"&gt;out-of-memory issues&lt;/a&gt; for more
information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-pytorch-version-available" class="anchor" aria-hidden="true" href="#is-there-a-pytorch-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a PyTorch version available?&lt;/h4&gt;
&lt;p&gt;There is no official PyTorch implementation. However, NLP researchers from
HuggingFace made a
&lt;a href="https://github.com/huggingface/pytorch-pretrained-BERT"&gt;PyTorch version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the PyTorch
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-is-there-a-chainer-version-available" class="anchor" aria-hidden="true" href="#is-there-a-chainer-version-available"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Is there a Chainer version available?&lt;/h4&gt;
&lt;p&gt;There is no official Chainer implementation. However, Sosuke Kobayashi made a
&lt;a href="https://github.com/soskek/bert-chainer"&gt;Chainer version of BERT available&lt;/a&gt;
which is compatible with our pre-trained checkpoints and is able to reproduce
our results. We were not involved in the creation or maintenance of the Chainer
implementation so please direct any questions towards the authors of that
repository.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-in-other-languages-be-released" class="anchor" aria-hidden="true" href="#will-models-in-other-languages-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models in other languages be released?&lt;/h4&gt;
&lt;p&gt;Yes, we plan to release a multi-lingual BERT model in the near future. We cannot
make promises about exactly which languages will be included, but it will likely
be a single model which includes &lt;em&gt;most&lt;/em&gt; of the languages which have a
significantly-sized Wikipedia.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-will-models-larger-than-bert-large-be-released" class="anchor" aria-hidden="true" href="#will-models-larger-than-bert-large-be-released"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Will models larger than &lt;code&gt;BERT-Large&lt;/code&gt; be released?&lt;/h4&gt;
&lt;p&gt;So far we have not attempted to train anything larger than &lt;code&gt;BERT-Large&lt;/code&gt;. It is
possible that we will release larger models if we are able to obtain significant
improvements.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-what-license-is-this-library-released-under" class="anchor" aria-hidden="true" href="#what-license-is-this-library-released-under"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What license is this library released under?&lt;/h4&gt;
&lt;p&gt;All code &lt;em&gt;and&lt;/em&gt; models are released under the Apache 2.0 license. See the
&lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-how-do-i-cite-bert" class="anchor" aria-hidden="true" href="#how-do-i-cite-bert"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How do I cite BERT?&lt;/h4&gt;
&lt;p&gt;For now, cite &lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;the Arxiv paper&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we submit the paper to a conference or journal, we will update the BibTeX.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-disclaimer" class="anchor" aria-hidden="true" href="#disclaimer"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;This is not an official Google product.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact-information" class="anchor" aria-hidden="true" href="#contact-information"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact information&lt;/h2&gt;
&lt;p&gt;For help or issues using BERT, please submit a GitHub issue.&lt;/p&gt;
&lt;p&gt;For personal communication related to BERT, please contact Jacob Devlin
(&lt;code&gt;jacobdevlin@google.com&lt;/code&gt;), Ming-Wei Chang (&lt;code&gt;mingweichang@google.com&lt;/code&gt;), or
Kenton Lee (&lt;code&gt;kentonl@google.com&lt;/code&gt;).&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>google-research</author><guid isPermaLink="false">https://github.com/google-research/bert</guid><pubDate>Tue, 19 Nov 2019 00:09:00 GMT</pubDate></item><item><title>dbolya/yolact #10 in Python, This month</title><link>https://github.com/dbolya/yolact</link><description>&lt;p&gt;&lt;i&gt;A simple, fully convolutional model for real-time instance segmentation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-you-only-look-at-coefficients" class="anchor" aria-hidden="true" href="#you-only-look-at-coefficients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;strong&gt;Y&lt;/strong&gt;ou &lt;strong&gt;O&lt;/strong&gt;nly &lt;strong&gt;L&lt;/strong&gt;ook &lt;strong&gt;A&lt;/strong&gt;t &lt;strong&gt;C&lt;/strong&gt;oefficien&lt;strong&gt;T&lt;/strong&gt;s&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;    ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ïö‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë        ‚ñà‚ñà‚ïë   
      ‚ïö‚ñà‚ñà‚ïî‚ïù  ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë        ‚ñà‚ñà‚ïë   
       ‚ñà‚ñà‚ïë   ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïë   
       ‚ïö‚ïê‚ïù    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù   ‚ïö‚ïê‚ïù 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A simple, fully convolutional model for real-time instance segmentation. This is the code for &lt;a href="https://arxiv.org/abs/1904.02689" rel="nofollow"&gt;our paper&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-iccv-update-v11-released-check-out-the-iccv-trailer-here" class="anchor" aria-hidden="true" href="#iccv-update-v11-released-check-out-the-iccv-trailer-here"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ICCV update (v1.1) released! Check out the ICCV trailer here:&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=0pMfmo8qfpQ" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c9f0f1403e25276c0beea78732b5cec6c9b610ab/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f30704d666d6f38716670512f302e6a7067" alt="IMAGE ALT TEXT HERE" data-canonical-src="https://img.youtube.com/vi/0pMfmo8qfpQ/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Read &lt;a href="CHANGELOG.md"&gt;the changelog&lt;/a&gt; for details on, well, what changed. Oh, and the paper got updated too with pascal results and an appendix with box mAP.&lt;/p&gt;
&lt;p&gt;Some examples from our base model (33.5 fps on a Titan Xp and 29.8 mAP on COCO's &lt;code&gt;test-dev&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_0.png"&gt;&lt;img src="data/yolact_example_0.png" alt="Example 0" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_1.png"&gt;&lt;img src="data/yolact_example_1.png" alt="Example 1" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="data/yolact_example_2.png"&gt;&lt;img src="data/yolact_example_2.png" alt="Example 2" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Set up a Python3 environment.&lt;/li&gt;
&lt;li&gt;Install &lt;a href="http://pytorch.org/" rel="nofollow"&gt;Pytorch&lt;/a&gt; 1.0.1 (or higher) and TorchVision.&lt;/li&gt;
&lt;li&gt;Install some other packages:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Cython needs to be installed before pycocotools&lt;/span&gt;
pip install cython
pip install opencv-python pillow pycocotools matplotlib &lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Clone this repository and enter it:
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;git clone https://github.com/dbolya/yolact.git
&lt;span class="pl-c1"&gt;cd&lt;/span&gt; yolact&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you'd like to train YOLACT, download the COCO dataset and the 2014/2017 annotations. Note that this script will take a while and dump 21gb of files into &lt;code&gt;./data/coco&lt;/code&gt;.
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh data/scripts/COCO.sh&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If you'd like to evaluate YOLACT on &lt;code&gt;test-dev&lt;/code&gt;, download &lt;code&gt;test-dev&lt;/code&gt; with this script.
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sh data/scripts/COCO_test.sh&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-evaluation" class="anchor" aria-hidden="true" href="#evaluation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Evaluation&lt;/h1&gt;
&lt;p&gt;As of April 5th, 2019 here are our latest models along with their FPS on a Titan Xp and mAP on &lt;code&gt;test-dev&lt;/code&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Image Size&lt;/th&gt;
&lt;th align="center"&gt;Backbone&lt;/th&gt;
&lt;th align="center"&gt;FPS&lt;/th&gt;
&lt;th align="center"&gt;mAP&lt;/th&gt;
&lt;th&gt;Weights&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Resnet50-FPN&lt;/td&gt;
&lt;td align="center"&gt;42.5&lt;/td&gt;
&lt;td align="center"&gt;28.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1yp7ZbbDwvMiFJEq4ptVKTYTI2VeRDXl0/view?usp=sharing" rel="nofollow"&gt;yolact_resnet50_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EUVpxoSXaqNIlssoLKOEoCcB1m0RpzGq_Khp5n1VX3zcUw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Darknet53-FPN&lt;/td&gt;
&lt;td align="center"&gt;40.0&lt;/td&gt;
&lt;td align="center"&gt;28.7&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1dukLrTzZQEuhzitGkHaGjphlmRJOjVnP/view?usp=sharing" rel="nofollow"&gt;yolact_darknet53_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/ERrao26c8llJn25dIyZPhwMBxUp2GdZTKIMUQA3t0djHLw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;550&lt;/td&gt;
&lt;td align="center"&gt;Resnet101-FPN&lt;/td&gt;
&lt;td align="center"&gt;33.0&lt;/td&gt;
&lt;td align="center"&gt;29.8&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1UYy3dMapbH1BnmtZU4WH1zbYgOzzHHf_/view?usp=sharing" rel="nofollow"&gt;yolact_base_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EYRWxBEoKU9DiblrWx2M89MBGFkVVB_drlRd_v5sdT3Hgg" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;700&lt;/td&gt;
&lt;td align="center"&gt;Resnet101-FPN&lt;/td&gt;
&lt;td align="center"&gt;23.6&lt;/td&gt;
&lt;td align="center"&gt;31.2&lt;/td&gt;
&lt;td&gt;&lt;a href="https://drive.google.com/file/d/1lE4Lz5p25teiXV-6HdTiOJSnS7u7GBzg/view?usp=sharing" rel="nofollow"&gt;yolact_im700_54_800000.pth&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/Eagg5RSc5hFEhp7sPtvLNyoBjhlf2feog7t8OQzHKKphjw" rel="nofollow"&gt;Mirror&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To evalute the model, put the corresponding weights file in the &lt;code&gt;./weights&lt;/code&gt; directory and run one of the following commands.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quantitative-results-on-coco" class="anchor" aria-hidden="true" href="#quantitative-results-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quantitative Results on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Quantitatively evaluate a trained model on the entire validation set. Make sure you have COCO downloaded as above.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This should get 29.92 validation mask mAP last time I checked.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Output a COCOEval json to submit to the website or to use the run_coco_eval.py script.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; This command will create './results/bbox_detections.json' and './results/mask_detections.json' for detection and instance segmentation respectively.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; You can run COCOEval on the files created in the previous command. The performance should match my implementation in eval.py.&lt;/span&gt;
python run_coco_eval.py

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To output a coco json file for test-dev, make sure you have test-dev downloaded from above and go&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json --dataset=coco2017_testdev_dataset&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-qualitative-results-on-coco" class="anchor" aria-hidden="true" href="#qualitative-results-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Qualitative Results on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display qualitative results on COCO. From here on I'll use a confidence threshold of 0.15.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --display&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-benchmarking-on-coco" class="anchor" aria-hidden="true" href="#benchmarking-on-coco"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Benchmarking on COCO&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Run just the raw model on the first 1k images of the validation set&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --benchmark --max_images=1000&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-images" class="anchor" aria-hidden="true" href="#images"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Images&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display qualitative results on the specified image.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=my_image.png

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process an image and save it to another file.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=input_image.png:output_image.png

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process a whole folder of images.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --images=path/to/input/folder:path/to/output/folder&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-video" class="anchor" aria-hidden="true" href="#video"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Video&lt;/h2&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display a video in real-time. "--video_multiframe" will process that many frames at once for improved performance.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; If you want, use "--display_fps" to draw the FPS directly on the frame.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=my_video.mp4

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Display a webcam feed in real-time. If you have multiple webcams pass the index of the webcam you want instead of 0.&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=0

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Process a video and save it to another file. This uses the same pipeline as the ones above now, so it's fast!&lt;/span&gt;
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=input_video.mp4:output_video.mp4&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can tell, &lt;code&gt;eval.py&lt;/code&gt; can do a ton of stuff. Run the &lt;code&gt;--help&lt;/code&gt; command to see everything it can do.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python eval.py --help&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h1&gt;
&lt;p&gt;By default, we train on COCO. Make sure to download the entire dataset using the commands above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To train, grab an imagenet-pretrained model and put it in &lt;code&gt;./weights&lt;/code&gt;.
&lt;ul&gt;
&lt;li&gt;For Resnet101, download &lt;code&gt;resnet101_reducedfc.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/1tvqFPd4bJtakOlmn-uIA492g2qurRChj/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Resnet50, download &lt;code&gt;resnet50-19c8e357.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/1Jy3yCdbatgXa5YYIdTCRrSV0S9V5g1rn/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For Darknet53, download &lt;code&gt;darknet53.pth&lt;/code&gt; from &lt;a href="https://drive.google.com/file/d/17Y431j4sagFpSReuPNoFcj9h7azDTZFf/view?usp=sharing" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run one of the training commands below.
&lt;ul&gt;
&lt;li&gt;Note that you can press ctrl+c while training and it will save an &lt;code&gt;*_interrupt.pth&lt;/code&gt; file at the current iteration.&lt;/li&gt;
&lt;li&gt;All weights are saved in the &lt;code&gt;./weights&lt;/code&gt; directory by default with the file name &lt;code&gt;&amp;lt;config&amp;gt;_&amp;lt;epoch&amp;gt;_&amp;lt;iter&amp;gt;.pth&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trains using the base config with a batch size of 8 (the default).&lt;/span&gt;
python train.py --config=yolact_base_config

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trains yolact_base_config with a batch_size of 5. For the 550px models, 1 batch takes up around 1.5 gigs of VRAM, so specify accordingly.&lt;/span&gt;
python train.py --config=yolact_base_config --batch_size=5

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Resume training yolact_base with a specific weight file and start from the iteration specified in the weight file's name.&lt;/span&gt;
python train.py --config=yolact_base_config --resume=weights/yolact_base_10_32100.pth --start_iter=-1

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Use the help option to see a description of all available command line arguments&lt;/span&gt;
python train.py --help&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-multi-gpu-support" class="anchor" aria-hidden="true" href="#multi-gpu-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Multi-GPU Support&lt;/h2&gt;
&lt;p&gt;YOLACT now supports multiple GPUs seamlessly during training:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before running any of the scripts, run: &lt;code&gt;export CUDA_VISIBLE_DEVICES=[gpus]&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Where you should replace [gpus] with a comma separated list of the index of each GPU you want to use (e.g., 0,1,2,3).&lt;/li&gt;
&lt;li&gt;You should still do this if only using 1 GPU.&lt;/li&gt;
&lt;li&gt;You can check the indices of your GPUs with &lt;code&gt;nvidia-smi&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Then, simply set the batch size to &lt;code&gt;8*num_gpus&lt;/code&gt; with the training commands above. The training script will automatically scale the hyperparameters to the right values.
&lt;ul&gt;
&lt;li&gt;If you have memory to spare you can increase the batch size further, but keep it a multiple of the number of GPUs you're using.&lt;/li&gt;
&lt;li&gt;If you want to allocate the images per GPU specific for different GPUs, you can use &lt;code&gt;--batch_alloc=[alloc]&lt;/code&gt; where [alloc] is a comma seprated list containing the number of images on each GPU. This must sum to &lt;code&gt;batch_size&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-logging" class="anchor" aria-hidden="true" href="#logging"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Logging&lt;/h2&gt;
&lt;p&gt;YOLACT now logs training and validation information by default. You can disable this with &lt;code&gt;--no_log&lt;/code&gt;. A guide on how to visualize these logs is coming soon, but now you can look at &lt;code&gt;LogVizualizer&lt;/code&gt; in &lt;code&gt;utils/logger.py&lt;/code&gt; for help.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pascal-sbd" class="anchor" aria-hidden="true" href="#pascal-sbd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pascal SBD&lt;/h2&gt;
&lt;p&gt;We also include a config for training on Pascal SBD annotations (for rapid experimentation or comparing with other methods). To train on Pascal SBD, proceed with the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download the dataset from &lt;a href="http://home.bharathh.info/pubs/codes/SBD/download.html" rel="nofollow"&gt;here&lt;/a&gt;. It's the first link in the top "Overview" section (and the file is called &lt;code&gt;benchmark.tgz&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Extract the dataset somewhere. In the dataset there should be a folder called &lt;code&gt;dataset/img&lt;/code&gt;. Create the directory &lt;code&gt;./data/sbd&lt;/code&gt; (where &lt;code&gt;.&lt;/code&gt; is YOLACT's root) and copy &lt;code&gt;dataset/img&lt;/code&gt; to &lt;code&gt;./data/sbd/img&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Download the COCO-style annotations from &lt;a href="https://drive.google.com/open?id=1ExrRSPVctHW8Nxrn0SofU1lVhK5Wn0_S" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Extract the annotations into &lt;code&gt;./data/sbd/&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Now you can train using &lt;code&gt;--config=yolact_resnet50_pascal_config&lt;/code&gt;. Check that config to see how to extend it to other models.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will automate this all with a script soon, don't worry. Also, if you want the script I used to convert the annotations, I put it in &lt;code&gt;./scripts/convert_sbd.py&lt;/code&gt;, but you'll have to check how it works to be able to use it because I don't actually remember at this point.&lt;/p&gt;
&lt;p&gt;If you want to verify our results, you can download our &lt;code&gt;yolact_resnet50_pascal_config&lt;/code&gt; weights from &lt;a href="https://drive.google.com/open?id=1yLVwtkRtNxyl0kxeMCtPXJsXFFyc_FHe" rel="nofollow"&gt;here&lt;/a&gt;. This model should get 72.3 mask AP_50 and 56.2 mask AP_70. Note that the "all" AP isn't the same as the "vol" AP reported in others papers for pascal (they use an averages of the thresholds from &lt;code&gt;0.1 - 0.9&lt;/code&gt; in increments of &lt;code&gt;0.1&lt;/code&gt; instead of what COCO uses).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-custom-datasets" class="anchor" aria-hidden="true" href="#custom-datasets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Custom Datasets&lt;/h2&gt;
&lt;p&gt;You can also train on your own dataset by following these steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a COCO-style Object Detection JSON annotation file for your dataset. The specification for this can be found &lt;a href="http://cocodataset.org/#format-data" rel="nofollow"&gt;here&lt;/a&gt;. Note that we don't use some fields, so the following may be omitted:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;info&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;liscense&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Under &lt;code&gt;image&lt;/code&gt;: &lt;code&gt;license, flickr_url, coco_url, date_captured&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;categories&lt;/code&gt; (we use our own format for categories, see below)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Create a definition for your dataset under &lt;code&gt;dataset_base&lt;/code&gt; in &lt;code&gt;data/config.py&lt;/code&gt; (see the comments in &lt;code&gt;dataset_base&lt;/code&gt; for an explanation of each field):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;my_custom_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; dataset_base.copy({
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;name&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;My Dataset&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_training_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train_info&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:   &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_training_annotation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_validation_images&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;valid_info&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;:   &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;path_to_validation_annotation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,

    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;has_gt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: &lt;span class="pl-c1"&gt;True&lt;/span&gt;,
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;class_names&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;: (&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_1&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_2&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;my_class_id_3&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;...&lt;/span&gt;)
})&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;A couple things to note:
&lt;ul&gt;
&lt;li&gt;Class IDs in the annotation file should start at 1 and increase sequentially on the order of &lt;code&gt;class_names&lt;/code&gt;. If this isn't the case for your annotation file (like in COCO), see the field &lt;code&gt;label_map&lt;/code&gt; in &lt;code&gt;dataset_base&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you do not want to create a validation split, use the same image path and annotations file for validation. By default (see &lt;code&gt;python train.py --help&lt;/code&gt;), &lt;code&gt;train.py&lt;/code&gt; will output validation mAP for the first 5000 images in the dataset every 2 epochs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Finally, in &lt;code&gt;yolact_base_config&lt;/code&gt; in the same file, change the value for &lt;code&gt;'dataset'&lt;/code&gt; to &lt;code&gt;'my_custom_dataset'&lt;/code&gt; or whatever you named the config object above. Then you can use any of the training commands in the previous section.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;a id="user-content-creating-a-custom-dataset-from-scratch" class="anchor" aria-hidden="true" href="#creating-a-custom-dataset-from-scratch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Creating a Custom Dataset from Scratch&lt;/h4&gt;
&lt;p&gt;See &lt;a href="https://github.com/dbolya/yolact/issues/70#issuecomment-504283008"&gt;this nice post by @Amit12690&lt;/a&gt; for tips on how to annotate a custom dataset and prepare it for use with YOLACT.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;p&gt;If you use YOLACT or this code base in your work, please cite&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{bolya-iccv2019,
  author    = {Daniel Bolya and Chong Zhou and Fanyi Xiao and Yong Jae Lee},
  title     = {YOLACT: {Real-time} Instance Segmentation},
  booktitle = {ICCV},
  year      = {2019},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h1&gt;
&lt;p&gt;For questions about our paper or code, please contact &lt;a href="mailto:dbolya@ucdavis.edu"&gt;Daniel Bolya&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>dbolya</author><guid isPermaLink="false">https://github.com/dbolya/yolact</guid><pubDate>Tue, 19 Nov 2019 00:10:00 GMT</pubDate></item><item><title>iGhibli/iOS-DeviceSupport #11 in Python, This month</title><link>https://github.com/iGhibli/iOS-DeviceSupport</link><description>&lt;p&gt;&lt;i&gt;This repository holds the device support files for the iOS, and I will update it regularly.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-ios-devicesupport" class="anchor" aria-hidden="true" href="#ios-devicesupport"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;iOS-DeviceSupport&lt;/h1&gt;
&lt;p&gt;This repository holds the device support files for the iOS, and I will update it regularly.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h2&gt;
&lt;p&gt;See docs: &lt;a href="https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/" rel="nofollow"&gt;https://ighibli.github.io/2017/03/28/Could-not-locate-device-support-files/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Below command will try to unzip all new device support files to &lt;code&gt;/Applications/Xcode.app&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo ./deploy.py&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can use &lt;code&gt;-t&lt;/code&gt; if your Xcode is not in &lt;code&gt;/Applications/&lt;/code&gt; or has different name.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo ./deploy.py -t /Applications/Xcode&lt;span class="pl-cce"&gt;\ &lt;/span&gt;9.app&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;./deploy.py -h
usage: deploy.py [-h] [-t TARGET]

optional arguments:
  -h, --help  show this &lt;span class="pl-c1"&gt;help&lt;/span&gt; message and &lt;span class="pl-c1"&gt;exit&lt;/span&gt;
  -t TARGET   The path &lt;span class="pl-k"&gt;for&lt;/span&gt; Xcode&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-supported-versions" class="anchor" aria-hidden="true" href="#supported-versions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Supported versions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;iOS8
&lt;ul&gt;
&lt;li&gt;8.0 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.1 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.2 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.3 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;8.4 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS9
&lt;ul&gt;
&lt;li&gt;9.0 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.1 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.2 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;9.3 &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS10
&lt;ul&gt;
&lt;li&gt;10.0 (14A345) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.0 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.1 (14B72) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.1 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.2 (14C92) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.2 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.3 (14E269) &lt;code&gt;2017/04/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;10.3 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS11
&lt;ul&gt;
&lt;li&gt;11.0 &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.1 (15B87) &lt;code&gt;2017/12/05&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.1 &lt;code&gt;2017/12/11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.2 (15C107) &lt;code&gt;2017/12/11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.2 &lt;code&gt;2018/03/06&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 (15E5167d) &lt;code&gt;2018/01/30&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 (15E5201e) &lt;code&gt;2018/03/06&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.3 &lt;code&gt;2018/04/09&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F5037c) &lt;code&gt;2018/04/09&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F5061c) &lt;code&gt;2018/07/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 (15F79) &lt;code&gt;2018/07/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;11.4 &lt;code&gt;2018/06/07&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS12
&lt;ul&gt;
&lt;li&gt;12.0 (16A5288q) &lt;code&gt;2018/06/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5308d) &lt;code&gt;2018/06/19&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5318d) &lt;code&gt;2018/06/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5327d) &lt;code&gt;2018/07/20&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5339e) &lt;code&gt;2018/07/31&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A5354b) &lt;code&gt;2018/08/15&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 (16A366) &lt;code&gt;2018/09/18&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.0 &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5059d) &lt;code&gt;2018/09/21&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5068g) &lt;code&gt;2018/10/08&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5084a) &lt;code&gt;2018/10/16&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B91) &lt;code&gt;2018/10/31&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 (16B5084a) &lt;code&gt;2018/10/16&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.1 &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E5181e) &lt;code&gt;2019/01/29&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E5212e) &lt;code&gt;2019/03/07&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.2 (16E226) &lt;code&gt;2019/03/27&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.3 &lt;code&gt;2019/06/04&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.4 (16G73) &lt;code&gt;2019/07/22&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;12.4 (FromXcode_11_Beta_7_xip) &lt;code&gt;2019/09/03&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iOS13
&lt;ul&gt;
&lt;li&gt;13.0 &lt;code&gt;2019/06/04&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.0 (FromXcode_11_Beta_7_xip) &lt;code&gt;2019/09/03&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.1 &lt;code&gt;2019/08/28&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.2 &lt;code&gt;2019/10/02&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.2 (FromXcode_11.2.1_GM_Seed) &lt;code&gt;2019/11/11&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;13.2 (FromXcode11.2.1(11B500)) &lt;code&gt;2019/11/15&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>iGhibli</author><guid isPermaLink="false">https://github.com/iGhibli/iOS-DeviceSupport</guid><pubDate>Tue, 19 Nov 2019 00:11:00 GMT</pubDate></item><item><title>facebookresearch/detectron2 #12 in Python, This month</title><link>https://github.com/facebookresearch/detectron2</link><description>&lt;p&gt;&lt;i&gt;Detectron2 is FAIR's next-generation research platform for object detection and segmentation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href=".github/Detectron2-Logo-Horz.svg"&gt;&lt;img src=".github/Detectron2-Logo-Horz.svg" width="300" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Detectron2 is Facebook AI Research's next generation software system
that implements state-of-the-art object detection algorithms.
It is a ground-up rewrite of the previous version,
&lt;a href="https://github.com/facebookresearch/Detectron/"&gt;Detectron&lt;/a&gt;,
and it originates from &lt;a href="https://github.com/facebookresearch/maskrcnn-benchmark/"&gt;maskrcnn-benchmark&lt;/a&gt;.&lt;/p&gt;
&lt;div align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png"&gt;&lt;img src="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-whats-new" class="anchor" aria-hidden="true" href="#whats-new"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's New&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;It is powered by the &lt;a href="https://pytorch.org" rel="nofollow"&gt;PyTorch&lt;/a&gt; deep learning framework.&lt;/li&gt;
&lt;li&gt;Includes more features such as panoptic segmentation, densepose, Cascade R-CNN, rotated bounding boxes, etc.&lt;/li&gt;
&lt;li&gt;Can be used as a library to support &lt;a href="projects/"&gt;different projects&lt;/a&gt; on top of it.
We'll open source more research projects in this way.&lt;/li&gt;
&lt;li&gt;It &lt;a href="https://detectron2.readthedocs.io/notes/benchmarks.html" rel="nofollow"&gt;trains much faster&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See our &lt;a href="https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/" rel="nofollow"&gt;blog post&lt;/a&gt;
to see more demos and learn about detectron2.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;See &lt;a href="INSTALL.md"&gt;INSTALL.md&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-start" class="anchor" aria-hidden="true" href="#quick-start"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;See &lt;a href="GETTING_STARTED.md"&gt;GETTING_STARTED.md&lt;/a&gt;,
or the &lt;a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5" rel="nofollow"&gt;Colab Notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Learn more at our &lt;a href="https://detectron2.readthedocs.org" rel="nofollow"&gt;documentation&lt;/a&gt;.
And see &lt;a href="projects/"&gt;projects/&lt;/a&gt; for some projects that are built on top of detectron2.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-zoo-and-baselines" class="anchor" aria-hidden="true" href="#model-zoo-and-baselines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model Zoo and Baselines&lt;/h2&gt;
&lt;p&gt;We provide a large set of baseline results and trained models available for download in the &lt;a href="MODEL_ZOO.md"&gt;Detectron2 Model Zoo&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Detectron2 is released under the &lt;a href="LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citing-detectron" class="anchor" aria-hidden="true" href="#citing-detectron"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing Detectron&lt;/h2&gt;
&lt;p&gt;If you use Detectron2 in your research or wish to refer to the baseline results published in the &lt;a href="MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt;, please use the following BibTeX entry.&lt;/p&gt;
&lt;div class="highlight highlight-text-bibtex"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;@misc&lt;/span&gt;{&lt;span class="pl-en"&gt;wu2019detectron2&lt;/span&gt;,
  &lt;span class="pl-s"&gt;author&lt;/span&gt; =       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;Yuxin Wu and Alexander Kirillov and Francisco Massa and&lt;/span&gt;
&lt;span class="pl-s"&gt;                  Wan-Yen Lo and Ross Girshick&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;title&lt;/span&gt; =        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;Detectron2&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;howpublished&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;\url{https://github.com/facebookresearch/detectron2}&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;,
  &lt;span class="pl-s"&gt;year&lt;/span&gt; =         &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;{&lt;/span&gt;2019&lt;span class="pl-pds"&gt;}&lt;/span&gt;&lt;/span&gt;
}&lt;/pre&gt;&lt;/div&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>facebookresearch</author><guid isPermaLink="false">https://github.com/facebookresearch/detectron2</guid><pubDate>Tue, 19 Nov 2019 00:12:00 GMT</pubDate></item><item><title>quantopian/zipline #13 in Python, This month</title><link>https://github.com/quantopian/zipline</link><description>&lt;p&gt;&lt;i&gt;Zipline, a Pythonic Algorithmic Trading Library&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;a href="https://www.zipline.io" rel="nofollow"&gt;&lt;img alt="Zipline" src="https://camo.githubusercontent.com/887b8228aa4b569b2a519ef711c7da7e5d6b40cd/68747470733a2f2f6d656469612e7175616e746f7069616e2e636f6d2f6c6f676f732f6f70656e5f736f757263652f7a69706c696e652d6c6f676f2d30335f2e706e67" data-canonical-src="https://media.quantopian.com/logos/open_source/zipline-logo-03_.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href="https://gitter.im/quantopian/zipline?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img alt="Gitter" src="https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667" data-canonical-src="https://badges.gitter.im/Join%20Chat.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://pypi.python.org/pypi/zipline" rel="nofollow"&gt;&lt;img alt="version status" src="https://camo.githubusercontent.com/3eb069ec0b3e276829eb1f88f7c594f33fe1e3d8/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f7a69706c696e652e737667" data-canonical-src="https://img.shields.io/pypi/pyversions/zipline.svg" style="max-width:100%;"&gt;
&lt;/a&gt;
&lt;a href="https://travis-ci.org/quantopian/zipline" rel="nofollow"&gt;&lt;img alt="travis status" src="https://camo.githubusercontent.com/29e70eddded2efd56e32b7fc5c150b90820099a1/68747470733a2f2f7472617669732d63692e6f72672f7175616e746f7069616e2f7a69706c696e652e706e673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.org/quantopian/zipline.png?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/quantopian/zipline/branch/master" rel="nofollow"&gt;&lt;img alt="appveyor status" src="https://camo.githubusercontent.com/b0eef78a2d89bc9ea40ea6cfeafb438444705605/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f336467313865363232376476737477362f6272616e63682f6d61737465723f7376673d74727565" data-canonical-src="https://ci.appveyor.com/api/projects/status/3dg18e6227dvstw6/branch/master?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/r/quantopian/zipline" rel="nofollow"&gt;&lt;img alt="Coverage Status" src="https://camo.githubusercontent.com/1a7165b07669cce4c6c69f92ca68d7e3e374bffc/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f7175616e746f7069616e2f7a69706c696e652f62616467652e706e67" data-canonical-src="https://coveralls.io/repos/quantopian/zipline/badge.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zipline is a Pythonic algorithmic trading library. It is an event-driven
system for backtesting. Zipline is currently used in production as the backtesting and live-trading
engine powering &lt;a href="https://www.quantopian.com" rel="nofollow"&gt;Quantopian&lt;/a&gt; -- a free,
community-centered, hosted platform for building and executing trading
strategies.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://groups.google.com/forum/#!forum/zipline" rel="nofollow"&gt;Join our Community!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.zipline.io" rel="nofollow"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Want to Contribute? See our &lt;a href="https://www.zipline.io/development-guidelines" rel="nofollow"&gt;Development Guidelines&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-features"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ease of Use:&lt;/strong&gt; Zipline tries to get out of your way so that you can
focus on algorithm development. See below for a code example.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;"Batteries Included":&lt;/strong&gt; many common statistics like
moving average and linear regression can be readily accessed from
within a user-written algorithm.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyData Integration:&lt;/strong&gt; Input of historical data and output of performance statistics are
based on Pandas DataFrames to integrate nicely into the existing
PyData ecosystem.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Statistics and Machine Learning Libraries:&lt;/strong&gt; You can use libraries like matplotlib, scipy,
statsmodels, and sklearn to support development, analysis, and
visualization of state-of-the-art trading systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-installation"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;a name="user-content-installing-with-pip"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-installing-with-pip" class="anchor" aria-hidden="true" href="#installing-with-pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing With &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Assuming you have all required (see note below) non-Python dependencies, you
can install Zipline with &lt;code&gt;pip&lt;/code&gt; via:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ pip install zipline&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Installing Zipline via &lt;code&gt;pip&lt;/code&gt; is slightly more involved than the
average Python package.  Simply running &lt;code&gt;pip install zipline&lt;/code&gt; will likely
fail if you've never installed any scientific Python packages before.&lt;/p&gt;
&lt;p&gt;There are two reasons for the additional complexity:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Zipline ships several C extensions that require access to the CPython C API.
In order to build the C extensions, &lt;code&gt;pip&lt;/code&gt; needs access to the CPython
header files for your Python installation.&lt;/li&gt;
&lt;li&gt;Zipline depends on &lt;a href="https://www.numpy.org/" rel="nofollow"&gt;numpy&lt;/a&gt;, the core library for
numerical array computing in Python.  Numpy depends on having the &lt;a href="https://www.netlib.org/lapack/" rel="nofollow"&gt;LAPACK&lt;/a&gt; linear algebra routines available.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Because LAPACK and the CPython headers are binary dependencies, the correct way
to install them varies from platform to platform.  On Linux, users generally
acquire these dependencies via a package manager like &lt;code&gt;apt&lt;/code&gt;, &lt;code&gt;yum&lt;/code&gt;, or
&lt;code&gt;pacman&lt;/code&gt;.  On OSX, &lt;a href="https://brew.sh/" rel="nofollow"&gt;Homebrew&lt;/a&gt; is a popular choice
providing similar functionality.&lt;/p&gt;
&lt;p&gt;See the full &lt;a href="https://www.zipline.io/install" rel="nofollow"&gt;Zipline Install Documentation&lt;/a&gt; for more information on acquiring
binary dependencies for your specific platform.&lt;/p&gt;
&lt;a name="user-content-conda"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-conda" class="anchor" aria-hidden="true" href="#conda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;conda&lt;/h3&gt;
&lt;p&gt;Another way to install Zipline is via the &lt;code&gt;conda&lt;/code&gt; package manager, which
comes as part of &lt;a href="https://www.anaconda.com/distribution/" rel="nofollow"&gt;Anaconda&lt;/a&gt; or can be
installed via &lt;code&gt;pip install conda&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once set up, you can install Zipline from our &lt;code&gt;Quantopian&lt;/code&gt; channel:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ conda install -c Quantopian zipline&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Currently supported platforms include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GNU/Linux 64-bit&lt;/li&gt;
&lt;li&gt;OSX 64-bit&lt;/li&gt;
&lt;li&gt;Windows 64-bit&lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;
&lt;p&gt;Note&lt;/p&gt;
&lt;p&gt;Windows 32-bit may work; however, it is not currently included in
continuous integration tests.&lt;/p&gt;
&lt;/div&gt;
&lt;a name="user-content-quickstart"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-quickstart" class="anchor" aria-hidden="true" href="#quickstart"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quickstart&lt;/h2&gt;
&lt;p&gt;See our &lt;a href="https://www.zipline.io/beginner-tutorial" rel="nofollow"&gt;getting started tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The following code implements a simple dual moving average algorithm.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;from&lt;/span&gt; zipline.api &lt;span class="pl-k"&gt;import&lt;/span&gt; order_target, record, symbol

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;initialize&lt;/span&gt;(&lt;span class="pl-smi"&gt;context&lt;/span&gt;):
    context.i &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;
    context.asset &lt;span class="pl-k"&gt;=&lt;/span&gt; symbol(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;AAPL&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)


&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;handle_data&lt;/span&gt;(&lt;span class="pl-smi"&gt;context&lt;/span&gt;, &lt;span class="pl-smi"&gt;data&lt;/span&gt;):
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Skip first 300 days to get full windows&lt;/span&gt;
    context.i &lt;span class="pl-k"&gt;+=&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;
    &lt;span class="pl-k"&gt;if&lt;/span&gt; context.i &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;300&lt;/span&gt;:
        &lt;span class="pl-k"&gt;return&lt;/span&gt;

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Compute averages&lt;/span&gt;
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; data.history() has to be called with the same params&lt;/span&gt;
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; from above and returns a pandas dataframe.&lt;/span&gt;
    short_mavg &lt;span class="pl-k"&gt;=&lt;/span&gt; data.history(context.asset, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;price&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;bar_count&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;100&lt;/span&gt;, &lt;span class="pl-v"&gt;frequency&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;1d&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).mean()
    long_mavg &lt;span class="pl-k"&gt;=&lt;/span&gt; data.history(context.asset, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;price&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;bar_count&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;300&lt;/span&gt;, &lt;span class="pl-v"&gt;frequency&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;1d&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;).mean()

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Trading logic&lt;/span&gt;
    &lt;span class="pl-k"&gt;if&lt;/span&gt; short_mavg &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; long_mavg:
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; order_target orders as many shares as needed to&lt;/span&gt;
        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; achieve the desired number of shares.&lt;/span&gt;
        order_target(context.asset, &lt;span class="pl-c1"&gt;100&lt;/span&gt;)
    &lt;span class="pl-k"&gt;elif&lt;/span&gt; short_mavg &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; long_mavg:
        order_target(context.asset, &lt;span class="pl-c1"&gt;0&lt;/span&gt;)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Save values for later inspection&lt;/span&gt;
    record(&lt;span class="pl-v"&gt;AAPL&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;data.current(context.asset, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;price&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
           &lt;span class="pl-v"&gt;short_mavg&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;short_mavg,
           &lt;span class="pl-v"&gt;long_mavg&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;long_mavg)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can then run this algorithm using the Zipline CLI; you'll need a &lt;a href="https://docs.quandl.com/docs#section-authentication" rel="nofollow"&gt;Quandl&lt;/a&gt; API key to ingest the default data bundle.
Once you have your key, run the following from the command line:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ QUANDL_API_KEY=&lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt;yourkey&lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; zipline ingest -b quandl
$ zipline run -f dual_moving_average.py --start 2014-1-1 --end 2018-1-1 -o dma.pickle&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will download asset pricing data data from quandl, and stream it through the algorithm
over the specified time range. Then, the resulting performance DataFrame is saved in dma.pickle, which you
can load and analyze from within Python.&lt;/p&gt;
&lt;p&gt;You can find other examples in the &lt;code&gt;zipline/examples&lt;/code&gt; directory.&lt;/p&gt;
&lt;a name="user-content-questions"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-questions" class="anchor" aria-hidden="true" href="#questions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Questions?&lt;/h2&gt;
&lt;p&gt;If you find a bug, feel free to &lt;a href="https://github.com/quantopian/zipline/issues/new"&gt;open an issue&lt;/a&gt; and fill out the issue template.&lt;/p&gt;
&lt;a name="user-content-contributing"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;All contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome. Details on how to set up a development environment can be found in our &lt;a href="https://www.zipline.io/development-guidelines" rel="nofollow"&gt;development guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are looking to start working with the Zipline codebase, navigate to the GitHub issues tab and start looking through interesting issues. Sometimes there are issues labeled as &lt;a href="https://github.com/quantopian/zipline/issues?q=is%3Aissue+is%3Aopen+label%3A%22Beginner+Friendly%22"&gt;Beginner Friendly&lt;/a&gt; or &lt;a href="https://github.com/quantopian/zipline/issues?q=is%3Aissue+is%3Aopen+label%3A%22Help+Wanted%22"&gt;Help Wanted&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feel free to ask questions on the &lt;a href="https://groups.google.com/forum/#!forum/zipline" rel="nofollow"&gt;mailing list&lt;/a&gt; or on &lt;a href="https://gitter.im/quantopian/zipline" rel="nofollow"&gt;Gitter&lt;/a&gt;.&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>quantopian</author><guid isPermaLink="false">https://github.com/quantopian/zipline</guid><pubDate>Tue, 19 Nov 2019 00:13:00 GMT</pubDate></item><item><title>trailofbits/algo #14 in Python, This month</title><link>https://github.com/trailofbits/algo</link><description>&lt;p&gt;&lt;i&gt;Set up a personal VPN in the cloud&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-algo-vpn" class="anchor" aria-hidden="true" href="#algo-vpn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Algo VPN&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://gitter.im/trailofbits/algo?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0c0e33218aa937f681d5088b670c988adf804264/68747470733a2f2f6261646765732e6769747465722e696d2f747261696c6f66626974732f616c676f2e737667" alt="Join the chat at https://gitter.im/trailofbits/algo" data-canonical-src="https://badges.gitter.im/trailofbits/algo.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/AlgoVPN" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a67add962c4c0beeead2da6dd98552fbce611fdb/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f68747470732f747769747465722e636f6d2f666f6c645f6c6566742e7376673f7374796c653d736f6369616c266c6162656c3d466f6c6c6f77253230253430416c676f56504e" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/url/https/twitter.com/fold_left.svg?style=social&amp;amp;label=Follow%20%40AlgoVPN" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://travis-ci.org/trailofbits/algo" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/960c464446296d169c0887c1641336b26bc8672f/68747470733a2f2f6170692e7472617669732d63692e6f72672f747261696c6f66626974732f616c676f2e7376673f6272616e63683d6d6173746572" alt="TravisCI Status" data-canonical-src="https://api.travis-ci.org/trailofbits/algo.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Algo VPN is a set of Ansible scripts that simplify the setup of a personal Wireguard and IPSEC VPN. It uses the most secure defaults available, works with common cloud providers, and does not require client software on most devices. See our &lt;a href="https://blog.trailofbits.com/2016/12/12/meet-algo-the-vpn-that-works/" rel="nofollow"&gt;release announcement&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Supports only IKEv2 with strong crypto (AES-GCM, SHA2, and P-256) and &lt;a href="https://www.wireguard.com/" rel="nofollow"&gt;WireGuard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Generates Apple profiles to auto-configure iOS and macOS devices&lt;/li&gt;
&lt;li&gt;Includes a helper script to add and remove users&lt;/li&gt;
&lt;li&gt;Blocks ads with a local DNS resolver (optional)&lt;/li&gt;
&lt;li&gt;Sets up limited SSH users for tunneling traffic (optional)&lt;/li&gt;
&lt;li&gt;Based on current versions of Ubuntu and strongSwan&lt;/li&gt;
&lt;li&gt;Installs to DigitalOcean, Amazon Lightsail, Amazon EC2, Vultr, Microsoft Azure, Google Compute Engine, Scaleway, OpenStack, CloudStack, Hetzner Cloud, or &lt;a href="docs/deploy-to-ubuntu.md"&gt;your own Ubuntu server&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-anti-features" class="anchor" aria-hidden="true" href="#anti-features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Anti-features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Does not support legacy cipher suites or protocols like L2TP, IKEv1, or RSA&lt;/li&gt;
&lt;li&gt;Does not install Tor, OpenVPN, or other risky servers&lt;/li&gt;
&lt;li&gt;Does not depend on the security of &lt;a href="https://tools.ietf.org/html/rfc7457" rel="nofollow"&gt;TLS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Does not require client software on most platforms&lt;/li&gt;
&lt;li&gt;Does not claim to provide anonymity or censorship avoidance&lt;/li&gt;
&lt;li&gt;Does not claim to protect you from the &lt;a href="https://en.wikipedia.org/wiki/Federal_Security_Service" rel="nofollow"&gt;FSB&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Ministry_of_State_Security_(China)" rel="nofollow"&gt;MSS&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Directorate-General_for_External_Security" rel="nofollow"&gt;DGSE&lt;/a&gt;, or &lt;a href="https://en.wikipedia.org/wiki/Flying_Spaghetti_Monster" rel="nofollow"&gt;FSM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-deploy-the-algo-server" class="anchor" aria-hidden="true" href="#deploy-the-algo-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deploy the Algo Server&lt;/h2&gt;
&lt;p&gt;The easiest way to get an Algo server running is to run it on your local system and let it set up a &lt;em&gt;new&lt;/em&gt; virtual machine in the cloud for you.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Setup an account on a cloud hosting provider.&lt;/strong&gt; Algo supports &lt;a href="https://m.do.co/c/4d7f4ff9cfe4" rel="nofollow"&gt;DigitalOcean&lt;/a&gt; (most user friendly), &lt;a href="https://aws.amazon.com/lightsail/" rel="nofollow"&gt;Amazon Lightsail&lt;/a&gt;, &lt;a href="https://aws.amazon.com/" rel="nofollow"&gt;Amazon EC2&lt;/a&gt;, &lt;a href="https://www.vultr.com/" rel="nofollow"&gt;Vultr&lt;/a&gt;, &lt;a href="https://azure.microsoft.com/" rel="nofollow"&gt;Microsoft Azure&lt;/a&gt;, &lt;a href="https://cloud.google.com/compute/" rel="nofollow"&gt;Google Compute Engine&lt;/a&gt;, &lt;a href="https://www.scaleway.com/" rel="nofollow"&gt;Scaleway&lt;/a&gt;, &lt;a href="https://www.dreamhost.com/cloud/computing/" rel="nofollow"&gt;DreamCompute&lt;/a&gt; or other OpenStack-based cloud hosting, &lt;a href="https://www.exoscale.com" rel="nofollow"&gt;Exoscale&lt;/a&gt; or other CloudStack-based cloud hosting,  or &lt;a href="https://www.hetzner.com/" rel="nofollow"&gt;Hetzner Cloud&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Get a copy of Algo.&lt;/strong&gt; The Algo scripts will be installed on your local system. There are two ways to get a copy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Download the &lt;a href="https://github.com/trailofbits/algo/archive/master.zip"&gt;ZIP file&lt;/a&gt;. Unzip the file to create a directory named &lt;code&gt;algo-master&lt;/code&gt; containing the Algo scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the command &lt;code&gt;git clone https://github.com/trailofbits/algo.git&lt;/code&gt; to create a directory named &lt;code&gt;algo&lt;/code&gt; containing the Algo scripts.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install Algo's core dependencies.&lt;/strong&gt; Algo requires that &lt;strong&gt;Python 3&lt;/strong&gt; and at least one supporting package are installed on your system.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt; Apple does not provide Python 3 with macOS. There are two ways to obtain it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use the &lt;a href="https://brew.sh" rel="nofollow"&gt;Homebrew&lt;/a&gt; package manager. After installing Homebrew install Python 3 by running &lt;code&gt;brew install python3&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download and install the latest stable &lt;a href="https://www.python.org/downloads/mac-osx/" rel="nofollow"&gt;Python 3 package&lt;/a&gt;. Be sure to run the included &lt;em&gt;Install Certificates&lt;/em&gt; command from Finder.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once Python 3 is installed on your Mac, from Terminal run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 -m pip install --upgrade virtualenv&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Linux:&lt;/strong&gt; Recent releases of Ubuntu, Debian, and Fedora come with Python 3 already installed. Make sure your system is up-to-date and install the supporting package(s):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ubuntu and Debian:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo apt install -y python3-virtualenv&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Fedora:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo dnf install -y python3-virtualenv&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Red Hat and CentOS 7 and later (for earlier versions see this &lt;a href="docs/deploy-from-redhat-centos6.md"&gt;documentation&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;sudo yum -y install epel-release
sudo yum install -y python36-virtualenv&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Windows:&lt;/strong&gt; Use the Windows Subsystem for Linux (WSL) to create your own copy of Ubuntu running under Windows from which to install and run Algo. See the &lt;a href="docs/deploy-from-windows.md"&gt;Windows documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install Algo's remaining dependencies.&lt;/strong&gt; You'll need to run these commands from the Algo directory each time you download a new copy of Algo. In a Terminal window &lt;code&gt;cd&lt;/code&gt; into the &lt;code&gt;algo-master&lt;/code&gt; (ZIP file) or &lt;code&gt;algo&lt;/code&gt; (&lt;code&gt;git clone&lt;/code&gt;) directory and run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python3 -m virtualenv --python=&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;$(&lt;/span&gt;command -v python3&lt;span class="pl-pds"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; .env &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt;
  &lt;span class="pl-c1"&gt;source&lt;/span&gt; .env/bin/activate &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt;
  python3 -m pip install -U pip virtualenv &lt;span class="pl-k"&gt;&amp;amp;&amp;amp;&lt;/span&gt;
  python3 -m pip install -r requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On Fedora add the option &lt;code&gt;--system-site-packages&lt;/code&gt; to the first command above. On macOS install the C compiler if prompted.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;List the users to create.&lt;/strong&gt; Open the file &lt;code&gt;config.cfg&lt;/code&gt; in your favorite text editor. Specify the users you wish to create in the &lt;code&gt;users&lt;/code&gt; list. Create a unique user for each device you plan to connect to your VPN. If you want to be able to add or delete users later, you &lt;strong&gt;must&lt;/strong&gt; select &lt;code&gt;yes&lt;/code&gt; at the &lt;code&gt;Do you want to retain the keys (PKI)?&lt;/code&gt; prompt during the deployment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Start the deployment.&lt;/strong&gt; Return to your terminal. In the Algo directory, run &lt;code&gt;./algo&lt;/code&gt; and follow the instructions. There are several optional features available. None are required for a fully functional VPN server. These optional features are described in greater detail in &lt;a href="docs/deploy-from-ansible.md"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That's it! You will get the message below when the server deployment process completes. Take note of the p12 (user certificate) password and the CA key in case you need them later, &lt;strong&gt;they will only be displayed this time&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You can now set up clients to connect to your VPN. Proceed to &lt;a href="#configure-the-vpn-clients"&gt;Configure the VPN Clients&lt;/a&gt; below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    "#                          Congratulations!                            #"
    "#                     Your Algo server is running.                     #"
    "#    Config files and certificates are in the ./configs/ directory.    #"
    "#              Go to https://whoer.net/ after connecting               #"
    "#        and ensure that all your traffic passes through the VPN.      #"
    "#                     Local DNS resolver 172.16.0.1                    #"
    "#        The p12 and SSH keys password for new users is XXXXXXXX       #"
    "#        The CA key password is XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX       #"
    "#      Shell access: ssh -i configs/algo.pem root@xxx.xxx.xx.xx        #"
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-configure-the-vpn-clients" class="anchor" aria-hidden="true" href="#configure-the-vpn-clients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configure the VPN Clients&lt;/h2&gt;
&lt;p&gt;Certificates and configuration files that users will need are placed in the &lt;code&gt;configs&lt;/code&gt; directory. Make sure to secure these files since many contain private keys. All files are saved under a subdirectory named with the IP address of your new Algo VPN server.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-apple-devices" class="anchor" aria-hidden="true" href="#apple-devices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Apple Devices&lt;/h3&gt;
&lt;p&gt;WireGuard is used to provide VPN services on Apple devices. Algo generates a WireGuard configuration file, &lt;code&gt;wireguard/&amp;lt;username&amp;gt;.conf&lt;/code&gt;, and a QR code, &lt;code&gt;wireguard/&amp;lt;username&amp;gt;.png&lt;/code&gt;, for each user defined in &lt;code&gt;config.cfg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On iOS, install the &lt;a href="https://itunes.apple.com/us/app/wireguard/id1441195209?mt=8" rel="nofollow"&gt;WireGuard&lt;/a&gt; app from the iOS App Store. Then, use the WireGuard app to scan the QR code or AirDrop the configuration file to the device.&lt;/p&gt;
&lt;p&gt;On macOS Mojave or later, install the &lt;a href="https://itunes.apple.com/us/app/wireguard/id1451685025?mt=12" rel="nofollow"&gt;WireGuard&lt;/a&gt; app from the Mac App Store. WireGuard will appear in the menu bar once you run the app. Click on the WireGuard icon, choose &lt;strong&gt;Import tunnel(s) from file...&lt;/strong&gt;, then select the appropriate WireGuard configuration file.&lt;/p&gt;
&lt;p&gt;On either iOS or macOS, you can enable "Connect on Demand" and/or exclude certain trusted Wi-Fi networks (such as your home or work) by editing the tunnel configuration in the WireGuard app. (Algo can't do this automatically for you.)&lt;/p&gt;
&lt;p&gt;Installing WireGuard is a little more complicated on older version of macOS. See &lt;a href="docs/client-macos-wireguard.md"&gt;Using macOS as a Client with WireGuard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you prefer to use the built-in IPSEC VPN on Apple devices, or need "Connect on Demand" or excluded Wi-Fi networks automatically configured, then see &lt;a href="docs/client-apple-ipsec.md"&gt;Using Apple Devices as a Client with IPSEC&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-android-devices" class="anchor" aria-hidden="true" href="#android-devices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Android Devices&lt;/h3&gt;
&lt;p&gt;WireGuard is used to provide VPN services on Android. Install the &lt;a href="https://play.google.com/store/apps/details?id=com.wireguard.android" rel="nofollow"&gt;WireGuard VPN Client&lt;/a&gt;. Import the corresponding &lt;code&gt;wireguard/&amp;lt;name&amp;gt;.conf&lt;/code&gt; file to your device, then setup a new connection with it. See the &lt;a href="/docs/client-android.md"&gt;Android setup instructions&lt;/a&gt; for more detailed walkthrough.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-windows" class="anchor" aria-hidden="true" href="#windows"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Windows&lt;/h3&gt;
&lt;p&gt;WireGuard is used to provide VPN services on Windows. Algo generates a WireGuard configuration file, &lt;code&gt;wireguard/&amp;lt;username&amp;gt;.conf&lt;/code&gt;, for each user defined in &lt;code&gt;config.cfg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Install the &lt;a href="https://www.wireguard.com/install/#windows-7-8-81-10-2012-2016-2019" rel="nofollow"&gt;WireGuard VPN Client&lt;/a&gt;. Import the generated &lt;code&gt;wireguard/&amp;lt;username&amp;gt;.conf&lt;/code&gt; file to your device, then setup a new connection with it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-linux-wireguard-clients" class="anchor" aria-hidden="true" href="#linux-wireguard-clients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linux WireGuard Clients&lt;/h3&gt;
&lt;p&gt;WireGuard works great with Linux clients. See &lt;a href="docs/client-linux-wireguard.md"&gt;this page&lt;/a&gt; for an example of how to configure WireGuard on Ubuntu.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-linux-strongswan-ipsec-clients-eg-openwrt-ubuntu-server-etc" class="anchor" aria-hidden="true" href="#linux-strongswan-ipsec-clients-eg-openwrt-ubuntu-server-etc"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Linux strongSwan IPsec Clients (e.g., OpenWRT, Ubuntu Server, etc.)&lt;/h3&gt;
&lt;p&gt;Please see &lt;a href="docs/client-linux-ipsec.md"&gt;this page&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-other-devices" class="anchor" aria-hidden="true" href="#other-devices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other Devices&lt;/h3&gt;
&lt;p&gt;Depending on the platform, you may need one or multiple of the following files.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ipsec/manual/cacert.pem: CA Certificate&lt;/li&gt;
&lt;li&gt;ipsec/manual/.p12: User Certificate and Private Key (in PKCS#12 format)&lt;/li&gt;
&lt;li&gt;ipsec/manual/.conf: strongSwan client configuration&lt;/li&gt;
&lt;li&gt;ipsec/manual/.secrets: strongSwan client configuration&lt;/li&gt;
&lt;li&gt;ipsec/apple/.mobileconfig: Apple Profile&lt;/li&gt;
&lt;li&gt;wireguard/.conf: WireGuard configuration profile&lt;/li&gt;
&lt;li&gt;wireguard/.png: WireGuard configuration QR code&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-setup-an-ssh-tunnel" class="anchor" aria-hidden="true" href="#setup-an-ssh-tunnel"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup an SSH Tunnel&lt;/h2&gt;
&lt;p&gt;If you turned on the optional SSH tunneling role, then local user accounts will be created for each user in &lt;code&gt;config.cfg&lt;/code&gt; and SSH authorized_key files for them will be in the &lt;code&gt;configs&lt;/code&gt; directory (user.ssh.pem). SSH user accounts do not have shell access, cannot authenticate with a password, and only have limited tunneling options (e.g., &lt;code&gt;ssh -N&lt;/code&gt; is required). This ensures that SSH users have the least access required to setup a tunnel and can perform no other actions on the Algo server.&lt;/p&gt;
&lt;p&gt;Use the example command below to start an SSH tunnel by replacing &lt;code&gt;user&lt;/code&gt; and &lt;code&gt;ip&lt;/code&gt; with your own. Once the tunnel is setup, you can configure a browser or other application to use 127.0.0.1:1080 as a SOCKS proxy to route traffic through the Algo server.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ssh -D 127.0.0.1:1080 -f -q -C -N user@ip -i configs/&amp;lt;server_ip&amp;gt;/ssh-tunnel/&amp;lt;user&amp;gt;.pem&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ssh-into-algo-server" class="anchor" aria-hidden="true" href="#ssh-into-algo-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SSH into Algo Server&lt;/h2&gt;
&lt;p&gt;Your Algo server is configured for key-only SSH access for administrative purposes. Open the Terminal app, &lt;code&gt;cd&lt;/code&gt; into the &lt;code&gt;algo-master&lt;/code&gt; directory where you originally downloaded Algo, and then use the command listed on the success message:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ssh -i configs/algo.pem user@ip&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;where &lt;code&gt;user&lt;/code&gt; is either &lt;code&gt;root&lt;/code&gt; or &lt;code&gt;ubuntu&lt;/code&gt; as listed on the success message, and &lt;code&gt;ip&lt;/code&gt; is the IP address of your Algo server. If you find yourself regularly logging into the server then it will be useful to load your Algo ssh key automatically. Add the following snippet to the bottom of &lt;code&gt;~/.bash_profile&lt;/code&gt; to add it to your shell environment permanently.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ssh-add ~/.ssh/algo &amp;gt; /dev/null 2&amp;gt;&amp;amp;1&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-adding-or-removing-users" class="anchor" aria-hidden="true" href="#adding-or-removing-users"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adding or Removing Users&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;If you chose to save the CA key during the deploy process,&lt;/em&gt; then Algo's own scripts can easily add and remove users from the VPN server.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Update the &lt;code&gt;users&lt;/code&gt; list in your &lt;code&gt;config.cfg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Open a terminal, &lt;code&gt;cd&lt;/code&gt; to the algo directory, and activate the virtual environment with &lt;code&gt;source .env/bin/activate&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Run the command: &lt;code&gt;./algo update-users&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After this process completes, the Algo VPN server will contain only the users listed in the &lt;code&gt;config.cfg&lt;/code&gt; file.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-additional-documentation" class="anchor" aria-hidden="true" href="#additional-documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Additional Documentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="docs/index.md"&gt;Deployment instructions, cloud provider setup instructions, and further client setup instructions available here.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/faq.md"&gt;FAQ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="docs/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you read all the documentation and have further questions, &lt;a href="https://gitter.im/trailofbits/algo" rel="nofollow"&gt;join the chat on Gitter&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-endorsements" class="anchor" aria-hidden="true" href="#endorsements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Endorsements&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;I've been ranting about the sorry state of VPN svcs for so long, probably about
time to give a proper talk on the subject. TL;DR: use Algo.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;-- &lt;a href="https://twitter.com/kennwhite/status/814166603587788800" rel="nofollow"&gt;Kenn White&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Before picking a VPN provider/app, make sure you do some research
&lt;a href="https://research.csiro.au/ng/wp-content/uploads/sites/106/2016/08/paper-1.pdf" rel="nofollow"&gt;https://research.csiro.au/ng/wp-content/uploads/sites/106/2016/08/paper-1.pdf&lt;/a&gt; ... ‚Äì or consider Algo&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;-- &lt;a href="https://twitter.com/TheRegister/status/825076303657177088" rel="nofollow"&gt;The Register&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Algo is really easy and secure.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;-- &lt;a href="https://twitter.com/thegrugq/status/786249040228786176" rel="nofollow"&gt;the grugq&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I played around with Algo VPN, a set of scripts that let you set up a VPN in the cloud in very little time, even if you don‚Äôt know much about development. I‚Äôve got to say that I was quite impressed with Trail of Bits‚Äô approach.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;-- &lt;a href="https://twitter.com/romaindillet/status/851037243728965632" rel="nofollow"&gt;Romain Dillet&lt;/a&gt; for &lt;a href="https://techcrunch.com/2017/04/09/how-i-made-my-own-vpn-server-in-15-minutes/" rel="nofollow"&gt;TechCrunch&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you‚Äôre uncomfortable shelling out the cash to an anonymous, random VPN provider, this is the best solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;-- &lt;a href="https://twitter.com/kingthor" rel="nofollow"&gt;Thorin Klosowski&lt;/a&gt; for &lt;a href="http://lifehacker.com/how-to-set-up-your-own-completely-free-vpn-in-the-cloud-1794302432" rel="nofollow"&gt;Lifehacker&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-support-algo-vpn" class="anchor" aria-hidden="true" href="#support-algo-vpn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Support Algo VPN&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://flattr.com/submit/auto?fid=kxw60j&amp;amp;url=https%3A%2F%2Fgithub.com%2Ftrailofbits%2Falgo" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/6b27c2d051d09e13f4009938f0b67aedd4ffd280/68747470733a2f2f627574746f6e2e666c617474722e636f6d2f666c617474722d62616467652d6c617267652e706e67" alt="Flattr" data-canonical-src="https://button.flattr.com/flattr-badge-large.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;amp;hosted_button_id=CYZZD39GXUJ3E" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/e14c85b542e06215f7e56c0763333ef1e9b9f9b7/68747470733a2f2f7777772e70617970616c6f626a656374732e636f6d2f656e5f55532f692f62746e2f62746e5f646f6e6174655f534d2e676966" alt="PayPal" data-canonical-src="https://www.paypalobjects.com/en_US/i/btn/btn_donate_SM.gif" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.patreon.com/algovpn" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/bf653361a158f7645497bf9490a97b697ec18f41/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6261636b5f6f6e2d70617472656f6e2d7265642e737667" alt="Patreon" data-canonical-src="https://img.shields.io/badge/back_on-patreon-red.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.bountysource.com/teams/trailofbits" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/79be76c663eb96142ff6f8d38ec10443107ffe97/68747470733a2f2f696d672e736869656c64732e696f2f626f756e7479736f757263652f7465616d2f747261696c6f66626974732f61637469766974792e737667" alt="Bountysource" data-canonical-src="https://img.shields.io/bountysource/team/trailofbits/activity.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All donations support continued development. Thanks!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We accept donations via &lt;a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;amp;hosted_button_id=CYZZD39GXUJ3E" rel="nofollow"&gt;PayPal&lt;/a&gt;, &lt;a href="https://www.patreon.com/algovpn" rel="nofollow"&gt;Patreon&lt;/a&gt;, and &lt;a href="https://flattr.com/submit/auto?fid=kxw60j&amp;amp;url=https%3A%2F%2Fgithub.com%2Ftrailofbits%2Falgo" rel="nofollow"&gt;Flattr&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Use our &lt;a href="https://m.do.co/c/4d7f4ff9cfe4" rel="nofollow"&gt;referral code&lt;/a&gt; when you sign up to Digital Ocean for a $10 credit.&lt;/li&gt;
&lt;li&gt;We also accept and appreciate contributions of new code and bugfixes via Github Pull Requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Algo is licensed and distributed under the AGPLv3. If you want to distribute a closed-source modification or service based on Algo, then please consider &lt;a href="mailto:opensource@trailofbits.com"&gt;purchasing an exception&lt;/a&gt; . As with the methods above, this will help support continued development.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>trailofbits</author><guid isPermaLink="false">https://github.com/trailofbits/algo</guid><pubDate>Tue, 19 Nov 2019 00:14:00 GMT</pubDate></item><item><title>521xueweihan/HelloGitHub #15 in Python, This month</title><link>https://github.com/521xueweihan/HelloGitHub</link><description>&lt;p&gt;&lt;i&gt;:octocat: Find pearls on open-source seashore ÂàÜ‰∫´ GitHub ‰∏äÊúâË∂£„ÄÅÂÖ•Èó®Á∫ßÁöÑÂºÄÊ∫êÈ°πÁõÆ&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/readme.gif"&gt;&lt;img src="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/readme.gif" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;br&gt;‰∏≠Êñá | &lt;a href="README_en.md"&gt;English&lt;/a&gt;
  &lt;br&gt;&lt;strong&gt;HelloGitHub&lt;/strong&gt; ‰∏Ä‰∏™ÂàÜ‰∫´ GitHub ‰∏äÊúâË∂£„ÄÅÂÖ•Èó®Á∫ßÁöÑÂºÄÊ∫êÈ°πÁõÆ„ÄÇ&lt;br&gt;ÂÖ¥Ë∂£ÊòØÊúÄÂ•ΩÁöÑËÄÅÂ∏àÔºåËøôÈáåËÉΩÂ§üÂ∏Æ‰Ω†ÊâæÂà∞ÁºñÁ®ãÁöÑÂÖ¥Ë∂£ÔºÅ
&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a href="https://raw.githubusercontent.com/521xueweihan/img/master/hellogithub/logo/weixin.png" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/61343b85520a4714ddb37eb300f8268cc881ae7e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f54616c6b2d2545352542452541452545342542462541312545372542452541342d627269676874677265656e2e7376673f7374796c653d706f706f75742d737175617265" alt="WeiXin" data-canonical-src="https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://github.com/521xueweihan/HelloGitHub/stargazers"&gt;&lt;img src="https://camo.githubusercontent.com/0aec7fa1a5647255bbe8af37a82a007be69d8739/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f35323178756577656968616e2f48656c6c6f4769744875622e7376673f7374796c653d706f706f75742d737175617265" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a href="https://github.com/521xueweihan/HelloGitHub/issues"&gt;&lt;img src="https://camo.githubusercontent.com/a8367e38e94eccf7e469023edfec05db15132454/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f35323178756577656968616e2f48656c6c6f4769744875622e7376673f7374796c653d706f706f75742d737175617265" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;a href="https://weibo.com/hellogithub" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4627590b5d81a690c6c83abaf47f678d70d26e6b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2545362539362542302545362542352541412d576569626f2d7265642e7376673f7374796c653d706f706f75742d737175617265" alt="Sina Weibo" data-canonical-src="https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ÁÆÄ‰ªã" class="anchor" aria-hidden="true" href="#ÁÆÄ‰ªã"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÁÆÄ‰ªã&lt;/h2&gt;
&lt;p&gt;ËøôÊòØ‰∏Ä‰∏™Èù¢ÂêëÁºñÁ®ãÊñ∞Êâã„ÄÅÁÉ≠Áà±ÁºñÁ®ã„ÄÅÂØπÂºÄÊ∫êÁ§æÂå∫ÊÑüÂÖ¥Ë∂£‰∫∫Áæ§ÁöÑÈ°πÁõÆÔºåÂÜÖÂÆπ&lt;strong&gt;ÊØèÊúà 28 Âè∑&lt;/strong&gt;‰ª•ÊúàÂàäÁöÑÂΩ¢ÂºèÊõ¥Êñ∞ÂèëÂ∏É„ÄÇÂÜÖÂÆπÂåÖÊã¨Ôºö&lt;strong&gt;ÊµÅË°åÈ°πÁõÆ&lt;/strong&gt;„ÄÅ&lt;strong&gt;ÂÖ•Èó®Á∫ßÈ°πÁõÆ&lt;/strong&gt;„ÄÅ&lt;strong&gt;ËÆ©ÁîüÊ¥ªÂèòÂæóÊõ¥ÁæéÂ•ΩÁöÑÂ∑•ÂÖ∑&lt;/strong&gt;„ÄÅ&lt;strong&gt;‰π¶Á±ç&lt;/strong&gt;„ÄÅ&lt;strong&gt;Â≠¶‰π†ÂøÉÂæóÁ¨îËÆ∞&lt;/strong&gt;„ÄÅ&lt;strong&gt;‰ºÅ‰∏öÁ∫ßÈ°πÁõÆ&lt;/strong&gt;Á≠âÔºåËøô‰∫õÂºÄÊ∫êÈ°πÁõÆÂ§ßÂ§öÈÉΩÊòØÈùûÂ∏∏ÂÆπÊòì‰∏äÊâã„ÄÅÂæà CoolÔºåËÉΩÂ§üËÆ©‰Ω†Áî®ÂæàÁü≠Êó∂Èó¥ÊÑüÂèóÂà∞ÁºñÁ®ãÁöÑÈ≠ÖÂäõÂíå‰æøÊç∑„ÄÇ‰ªéËÄåËÆ©Â§ßÂÆ∂ÊÑüÂèóÂà∞ÁºñÁ®ãÁöÑ‰πêË∂£ÔºåÂä®ÊâãÂºÄÂßãÁºñÁ®ã„ÄÇ&lt;/p&gt;
&lt;p&gt;Â∏åÊúõÈÄöËøáÊú¨È°πÁõÆËÉΩÂ§üÊúâÊõ¥Â§ö‰∫∫Âä†ÂÖ•Âà∞ÂºÄÊ∫êÁ§æÂå∫„ÄÅÂõûÈ¶àÁ§æÂå∫„ÄÇ&lt;strong&gt;ËÆ©ÊúâË∂£„ÄÅÊúâ‰ª∑ÂÄºÁöÑÈ°πÁõÆË¢´Êõ¥Â§ö‰∫∫ÂèëÁé∞ÂíåÂä†ÂÖ•&lt;/strong&gt;„ÄÇÂú®ÂèÇ‰∏éËøô‰∫õÈ°πÁõÆÁöÑËøáÁ®ã‰∏≠Ôºå‰Ω†Â∞ÜÂæóÂà∞Ôºö&lt;strong&gt;ÁÉ≠Áà±ÁºñÁ®ãÁöÑÂ∞è‰ºô‰º¥&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="man_dancing" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f57a.png"&gt;üï∫&lt;/g-emoji&gt; „ÄÅ&lt;strong&gt;Êõ¥Â§öÁºñÁ®ãÁü•ËØÜ&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="books" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png"&gt;üìö&lt;/g-emoji&gt; „ÄÅ&lt;strong&gt;‰ºòÁßÄÁöÑÁºñÁ®ãÊäÄÂ∑ß&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png"&gt;üíª&lt;/g-emoji&gt; „ÄÅ&lt;strong&gt;ÊâæÂà∞ÁºñÁ®ãÁöÑ‰πêË∂£&lt;/strong&gt;&lt;g-emoji class="g-emoji" alias="video_game" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ae.png"&gt;üéÆ&lt;/g-emoji&gt; „ÄÇ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;„ÄéÊØèÊó•Á≤æÈÄâ„Äè&lt;/strong&gt; ÂÖ≥Ê≥®Êàë‰ª¨ÁöÑ&lt;a href="https://weibo.com/hellogithub" rel="nofollow"&gt;ÊúÄÊÉ®ÂÆòÂæÆ&lt;/a&gt;Ëé∑ÂèñÊúÄÊñ∞È°πÁõÆÊé®Ëçê„ÄÇ&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;„ÄéËÆ≤Ëß£ÂºÄÊ∫êÈ°πÁõÆ„Äè&lt;/strong&gt; Ê¨¢ËøéÂºÄÊ∫êÁà±Â•ΩËÄÖÁªôÊàë‰ª¨ÊäïÁ®ø&lt;a href="https://github.com/HelloGitHub-Team/Article/blob/master/%E5%88%9B%E4%BD%9C%E9%A1%BB%E7%9F%A5.md"&gt;Êü•ÁúãÂàõ‰ΩúÈ°ªÁü•&lt;/a&gt;„ÄÇ&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-ÂÜÖÂÆπ" class="anchor" aria-hidden="true" href="#ÂÜÖÂÆπ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;ÂÜÖÂÆπ&lt;/h2&gt;
&lt;p&gt;ÊØèÊúà 28 Âè∑ÂèëÂ∏É&lt;a href="/content/last.md"&gt;ÊúÄÊñ∞‰∏ÄÊúü&lt;/a&gt; | &lt;a href="https://hellogithub.com" rel="nofollow"&gt;ÂÆòÁΩë&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img class="emoji" title=":shipit:" alt=":shipit:" src="https://github.githubassets.com/images/icons/emoji/shipit.png" height="20" width="20" align="absmiddle"&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="jack_o_lantern" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f383.png"&gt;üéÉ&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="beer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f37a.png"&gt;üç∫&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;g-emoji class="g-emoji" alias="fish_cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f365.png"&gt;üç•&lt;/g-emoji&gt;&lt;/th&gt;
&lt;th&gt;&lt;img class="emoji" title=":octocat:" alt=":octocat:" src="https://github.githubassets.com/images/icons/emoji/octocat.png" height="20" width="20" align="absmiddle"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/43/HelloGitHub43.md"&gt;Á¨¨ 43 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/42/HelloGitHub42.md"&gt;Á¨¨ 42 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/41/HelloGitHub41.md"&gt;Á¨¨ 41 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/40/HelloGitHub40.md"&gt;Á¨¨ 40 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/39/HelloGitHub39.md"&gt;Á¨¨ 39 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/38/HelloGitHub38.md"&gt;Á¨¨ 38 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/37/HelloGitHub37.md"&gt;Á¨¨ 37 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/36/HelloGitHub36.md"&gt;Á¨¨ 36 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/35/HelloGitHub35.md"&gt;Á¨¨ 35 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/34/HelloGitHub34.md"&gt;Á¨¨ 34 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/33/HelloGitHub33.md"&gt;Á¨¨ 33 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/32/HelloGitHub32.md"&gt;Á¨¨ 32 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/31/HelloGitHub31.md"&gt;Á¨¨ 31 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/30/HelloGitHub30.md"&gt;Á¨¨ 30 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/29/HelloGitHub29.md"&gt;Á¨¨ 29 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/28/HelloGitHub28.md"&gt;Á¨¨ 28 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/27/HelloGitHub27.md"&gt;Á¨¨ 27 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/26/HelloGitHub26.md"&gt;Á¨¨ 26 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/25/HelloGitHub25.md"&gt;Á¨¨ 25 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/24/HelloGitHub24.md"&gt;Á¨¨ 24 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/23/HelloGitHub23.md"&gt;Á¨¨ 23 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/22/HelloGitHub22.md"&gt;Á¨¨ 22 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/21/HelloGitHub21.md"&gt;Á¨¨ 21 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/20/HelloGitHub20.md"&gt;Á¨¨ 20 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/19/HelloGitHub19.md"&gt;Á¨¨ 19 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/18/HelloGitHub18.md"&gt;Á¨¨ 18 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/17/HelloGitHub17.md"&gt;Á¨¨ 17 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/16/HelloGitHub16.md"&gt;Á¨¨ 16 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/15/HelloGitHub15.md"&gt;Á¨¨ 15 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/14/HelloGitHub14.md"&gt;Á¨¨ 14 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/13/HelloGitHub13.md"&gt;Á¨¨ 13 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/12/HelloGitHub12.md"&gt;Á¨¨ 12 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/11/HelloGitHub11.md"&gt;Á¨¨ 11 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/10/HelloGitHub10.md"&gt;Á¨¨ 10 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/09/HelloGitHub09.md"&gt;Á¨¨ 09 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/08/HelloGitHub08.md"&gt;Á¨¨ 08 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/07/HelloGitHub07.md"&gt;Á¨¨ 07 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/06/HelloGitHub06.md"&gt;Á¨¨ 06 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="/content/05/HelloGitHub05.md"&gt;Á¨¨ 05 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/04/HelloGitHub04.md"&gt;Á¨¨ 04 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/03/HelloGitHub03.md"&gt;Á¨¨ 03 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/02/HelloGitHub02.md"&gt;Á¨¨ 02 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href="/content/01/HelloGitHub01.md"&gt;Á¨¨ 01 Êúü&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Ê¨¢Ëøé&lt;a href="https://github.com/521xueweihan/HelloGitHub/issues/new"&gt;Êé®ËçêÊàñËá™ËçêÈ°πÁõÆ&lt;/a&gt;Êàê‰∏∫ &lt;strong&gt;HelloGitHub&lt;/strong&gt; ÁöÑ&lt;a href="https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md"&gt;Ë¥°ÁåÆËÄÖ&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-Ë¥°ÁåÆËÄÖ" class="anchor" aria-hidden="true" href="#Ë¥°ÁåÆËÄÖ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ë¥°ÁåÆËÄÖ&lt;/h2&gt;
&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/521xueweihan"&gt;
          &lt;img src="https://avatars2.githubusercontent.com/u/8255800?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;ÂâäÂæÆÂØí&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/ming995"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/46031112?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Á≥ñÈÜãÈáåËÑä&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/FrontMage"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/17007026?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;FrontMage&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/xibinyue"&gt;
          &lt;img src="https://avatars0.githubusercontent.com/u/14122146?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;xibinyue&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/Eurus-Holmes"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/34226570?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Feiyang Chen&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/ChungZH"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/42088872?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;ChungZH&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/daixiang0"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/26538619?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;daixiang0&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/nivance"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/3291404?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;nivance&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/hellowHuaairen"&gt;
          &lt;img src="https://avatars2.githubusercontent.com/u/19610305?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;hellowHuaairen&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/17665302?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Êõ¥Â§öË¥°ÁåÆËÄÖ&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-Âêà‰ΩúÁªÑÁªá" class="anchor" aria-hidden="true" href="#Âêà‰ΩúÁªÑÁªá"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Âêà‰ΩúÁªÑÁªá&lt;/h2&gt;
&lt;p&gt;Ê¨¢ËøéÂêÑÁßç&lt;img class="emoji" title=":octocat:" alt=":octocat:" src="https://github.githubassets.com/images/icons/emoji/octocat.png" height="20" width="20" align="absmiddle"&gt;ÂºÄÊ∫êÁªÑÁªáÂêà‰Ωú&lt;a href="Mailto:595666367@qq.com"&gt;ÁÇπÂáªËÅîÁ≥ªÊàë&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/FGDBTKD"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/40509403?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;FGDBTKD&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;AI/ML/DL/NLP&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/d2-projects"&gt;
          &lt;img src="https://avatars3.githubusercontent.com/u/40857578?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;D2 Projects&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Vue/JavaScript&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
      &lt;th align="center"&gt;
        &lt;a href="https://github.com/doocs"&gt;
          &lt;img src="https://avatars1.githubusercontent.com/u/43716716?s=50&amp;amp;v=4" style="max-width:100%;"&gt;&lt;br&gt;
          &lt;sub&gt;Doocs&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;Technical Knowledge&lt;/sub&gt;
        &lt;/a&gt;&lt;br&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-Â£∞Êòé" class="anchor" aria-hidden="true" href="#Â£∞Êòé"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Â£∞Êòé&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="nofollow"&gt;&lt;img alt="Áü•ËØÜÂÖ±‰∫´ËÆ∏ÂèØÂçèËÆÆ" src="https://camo.githubusercontent.com/1ae74a56e22c4897b6fbfb9f301bd829c77429a7/68747470733a2f2f6c6963656e7365627574746f6e732e6e65742f6c2f62792d6e632d6e642f342e302f38387833312e706e67" data-canonical-src="https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;br&gt;Êú¨‰ΩúÂìÅÈááÁî® &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="nofollow"&gt;ÁΩ≤Âêç-ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî®-Á¶ÅÊ≠¢ÊºîÁªé 4.0 ÂõΩÈôÖ&lt;/a&gt; ËøõË°åËÆ∏ÂèØ„ÄÇ&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>521xueweihan</author><guid isPermaLink="false">https://github.com/521xueweihan/HelloGitHub</guid><pubDate>Tue, 19 Nov 2019 00:15:00 GMT</pubDate></item><item><title>localstack/localstack #16 in Python, This month</title><link>https://github.com/localstack/localstack</link><description>&lt;p&gt;&lt;i&gt;üíª  A fully functional local AWS cloud stack. Develop and test your cloud &amp; Serverless apps offline!&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a href="https://travis-ci.org/localstack/localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9776494b9a6b388dc743ef4f1fe0f48418996403/68747470733a2f2f7472617669732d63692e6f72672f6c6f63616c737461636b2f6c6f63616c737461636b2e737667" alt="Build Status" data-canonical-src="https://travis-ci.org/localstack/localstack.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="#backers"&gt;&lt;img src="https://camo.githubusercontent.com/7503d4e605e56494b94f7e899b59c12d6869e6d4/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f6261636b6572732f62616467652e737667" alt="Backers on Open Collective" data-canonical-src="https://opencollective.com/localstack/backers/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="#sponsors"&gt;&lt;img src="https://camo.githubusercontent.com/4ce5d939a7baa05f5513a28bced276b40163e726/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f72732f62616467652e737667" alt="Sponsors on Open Collective" data-canonical-src="https://opencollective.com/localstack/sponsors/badge.svg" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/localstack/localstack?branch=master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a25d81482ec1f47ecef8ab8f5c6ea87316d0df71/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6c6f63616c737461636b2f6c6f63616c737461636b2f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/localstack/localstack/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gitter.im/localstack/Platform" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/ef7f49d6c9d82d4762efd93e6c5190ed7ff070e8/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6c6f63616c737461636b2f506c6174666f726d2e737667" alt="Gitter" data-canonical-src="https://img.shields.io/gitter/room/localstack/Platform.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://badge.fury.io/py/localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/3f7ff1cf090f0a7a2ca744acb42a565188b1e51f/68747470733a2f2f62616467652e667572792e696f2f70792f6c6f63616c737461636b2e737667" alt="PyPI Version" data-canonical-src="https://badge.fury.io/py/localstack.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://img.shields.io/pypi/l/localstack.svg" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8c44673b3399efcaa024ef3f64e031acd53422f5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f6c6f63616c737461636b2e737667" alt="PyPI License" data-canonical-src="https://img.shields.io/pypi/l/localstack.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://codeclimate.com/github/localstack/localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/bb304a7e024bd89e75e3ee5922f276d69b0399f0/68747470733a2f2f636f6465636c696d6174652e636f6d2f6769746875622f6c6f63616c737461636b2f6c6f63616c737461636b2f6261646765732f6770612e737667" alt="Code Climate" data-canonical-src="https://codeclimate.com/github/localstack/localstack/badges/gpa.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://twitter.com/_localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/83d4084f7b71558e33b08844da5c773a8657e271/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-localstack---a-fully-functional-local-aws-cloud-stack" class="anchor" aria-hidden="true" href="#localstack---a-fully-functional-local-aws-cloud-stack"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LocalStack - A fully functional local AWS cloud stack&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/localstack/localstack/raw/master/localstack/dashboard/web/img/localstack.png"&gt;&lt;img src="https://github.com/localstack/localstack/raw/master/localstack/dashboard/web/img/localstack.png" alt="LocalStack" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;LocalStack&lt;/em&gt; provides an easy-to-use test/mocking framework for developing Cloud applications.&lt;/p&gt;
&lt;p&gt;Currently, the focus is primarily on supporting the AWS cloud stack.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-announcements" class="anchor" aria-hidden="true" href="#announcements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Announcements&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;2019-10-09&lt;/strong&gt;: &lt;strong&gt;LocalStack Pro is out!&lt;/strong&gt; We're incredibly excited to announce the launch of LocalStack Pro - the enterprise version of LocalStack with additional APIs and advanced features. Check out the free trial at &lt;a href="https://localstack.cloud" rel="nofollow"&gt;https://localstack.cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2018-01-10&lt;/strong&gt;: &lt;strong&gt;Help wanted!&lt;/strong&gt; Please &lt;a href="https://lambdastudy.typeform.com/to/kDUvvy?source=localstack-github" rel="nofollow"&gt;fill out this survey&lt;/a&gt; to support a research study on the usage of Serverless and Function-as-a-Service (FaaS) services, conducted at Chalmers University of Technology. The survey only takes 5-10 minutes of your time. Many thanks for your participation!!
&lt;ul&gt;
&lt;li&gt;The result from this study can be found &lt;a href="https://research.chalmers.se/en/publication/508147" rel="nofollow"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2017-08-27&lt;/strong&gt;: &lt;strong&gt;We need your support!&lt;/strong&gt; LocalStack is growing fast, we now have thousands of developers using the platform on a regular basis. Last month we have recorded a staggering 100k test runs, with 25k+ DynamoDB tables, 20k+ SQS queues, 15k+ Kinesis streams, 13k+ S3 buckets, and 10k+ Lambda functions created locally - for 0$ costs (more details to be published soon). Bug and feature requests are pouring in, and we now need some support from &lt;em&gt;you&lt;/em&gt; to keep the open source version actively maintained. Please check out &lt;a href="https://opencollective.com/localstack" rel="nofollow"&gt;Open Collective&lt;/a&gt; and become a &lt;a href="https://github.com/localstack/localstack#backers"&gt;backer&lt;/a&gt; or &lt;a href="https://github.com/localstack/localstack#backers"&gt;supporter&lt;/a&gt; of the project today! Thanks everybody for contributing. ‚ô•&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2017-07-20&lt;/strong&gt;: Please note: Starting with version &lt;code&gt;0.7.0&lt;/code&gt;, the Docker image will be pushed
and kept up to date under the &lt;strong&gt;new name&lt;/strong&gt; &lt;code&gt;localstack/localstack&lt;/code&gt;. (This means that you may
have to update your CI configurations.) Please refer to the updated
&lt;strong&gt;&lt;a href="doc/end_user_license_agreement"&gt;End-User License Agreement (EULA)&lt;/a&gt;&lt;/strong&gt; for the new versions.
The old Docker image (&lt;code&gt;atlassianlabs/localstack&lt;/code&gt;) is still available but will not be maintained
any longer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-overview" class="anchor" aria-hidden="true" href="#overview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overview&lt;/h1&gt;
&lt;p&gt;LocalStack spins up the following core Cloud APIs on your local machine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;API Gateway&lt;/strong&gt; at &lt;a href="http://localhost:4567" rel="nofollow"&gt;http://localhost:4567&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kinesis&lt;/strong&gt; at &lt;a href="http://localhost:4568" rel="nofollow"&gt;http://localhost:4568&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DynamoDB&lt;/strong&gt; at &lt;a href="http://localhost:4569" rel="nofollow"&gt;http://localhost:4569&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DynamoDB Streams&lt;/strong&gt; at &lt;a href="http://localhost:4570" rel="nofollow"&gt;http://localhost:4570&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Elasticsearch&lt;/strong&gt; at &lt;a href="http://localhost:4571" rel="nofollow"&gt;http://localhost:4571&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;S3&lt;/strong&gt; at &lt;a href="http://localhost:4572" rel="nofollow"&gt;http://localhost:4572&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Firehose&lt;/strong&gt; at &lt;a href="http://localhost:4573" rel="nofollow"&gt;http://localhost:4573&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lambda&lt;/strong&gt; at &lt;a href="http://localhost:4574" rel="nofollow"&gt;http://localhost:4574&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SNS&lt;/strong&gt; at &lt;a href="http://localhost:4575" rel="nofollow"&gt;http://localhost:4575&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SQS&lt;/strong&gt; at &lt;a href="http://localhost:4576" rel="nofollow"&gt;http://localhost:4576&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Redshift&lt;/strong&gt; at &lt;a href="http://localhost:4577" rel="nofollow"&gt;http://localhost:4577&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ES (Elasticsearch Service)&lt;/strong&gt; at &lt;a href="http://localhost:4578" rel="nofollow"&gt;http://localhost:4578&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SES&lt;/strong&gt; at &lt;a href="http://localhost:4579" rel="nofollow"&gt;http://localhost:4579&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Route53&lt;/strong&gt; at &lt;a href="http://localhost:4580" rel="nofollow"&gt;http://localhost:4580&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CloudFormation&lt;/strong&gt; at &lt;a href="http://localhost:4581" rel="nofollow"&gt;http://localhost:4581&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CloudWatch&lt;/strong&gt; at &lt;a href="http://localhost:4582" rel="nofollow"&gt;http://localhost:4582&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSM&lt;/strong&gt; at &lt;a href="http://localhost:4583" rel="nofollow"&gt;http://localhost:4583&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SecretsManager&lt;/strong&gt; at &lt;a href="http://localhost:4584" rel="nofollow"&gt;http://localhost:4584&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;StepFunctions&lt;/strong&gt; at &lt;a href="http://localhost:4585" rel="nofollow"&gt;http://localhost:4585&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CloudWatch Logs&lt;/strong&gt; at &lt;a href="http://localhost:4586" rel="nofollow"&gt;http://localhost:4586&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EventBridge (CloudWatch Events)&lt;/strong&gt; at &lt;a href="http://localhost:4587" rel="nofollow"&gt;http://localhost:4587&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;STS&lt;/strong&gt; at &lt;a href="http://localhost:4592" rel="nofollow"&gt;http://localhost:4592&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IAM&lt;/strong&gt; at &lt;a href="http://localhost:4593" rel="nofollow"&gt;http://localhost:4593&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EC2&lt;/strong&gt; at &lt;a href="http://localhost:4597" rel="nofollow"&gt;http://localhost:4597&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to the above, the &lt;a href="https://localstack.cloud/#pricing" rel="nofollow"&gt;&lt;strong&gt;Pro version&lt;/strong&gt; of LocalStack&lt;/a&gt; supports additional APIs and advanced features, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Athena&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cognito&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ElastiCache&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ECS/EKS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IoT&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lambda Layers&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RDS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;XRay&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interactive UIs to manage resources&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test report dashboards&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;...and much, much more to come!&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-why-localstack" class="anchor" aria-hidden="true" href="#why-localstack"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why LocalStack?&lt;/h2&gt;
&lt;p&gt;LocalStack builds on existing best-of-breed mocking/testing tools, most notably
&lt;a href="https://github.com/mhart/kinesalite"&gt;kinesalite&lt;/a&gt;/&lt;a href="https://github.com/mhart/dynalite"&gt;dynalite&lt;/a&gt;
and &lt;a href="https://github.com/spulec/moto"&gt;moto&lt;/a&gt;. While these tools are &lt;em&gt;awesome&lt;/em&gt; (!), they lack functionality
for certain use cases. LocalStack combines the tools, makes them interoperable, and adds important
missing functionality on top of them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Error injection:&lt;/strong&gt; LocalStack allows to inject errors frequently occurring in real Cloud environments,
for instance &lt;code&gt;ProvisionedThroughputExceededException&lt;/code&gt; which is thrown by Kinesis or DynamoDB if the amount of
read/write throughput is exceeded.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Isolated processes&lt;/strong&gt;: All services in LocalStack run in separate processes. The overhead of additional
processes is negligible, and the entire stack can easily be executed on any developer machine and CI server.
In moto, components are often hard-wired in RAM (e.g., when forwarding a message on an SNS topic to an SQS queue,
the queue endpoint is looked up in a local hash map). In contrast, LocalStack services live in isolation
(separate processes available via HTTP), which fosters true decoupling and more closely resembles the real
cloud environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pluggable services&lt;/strong&gt;: All services in LocalStack are easily pluggable (and replaceable), due to the fact that
we are using isolated processes for each service. This allows us to keep the framework up-to-date and select
best-of-breed mocks for each individual service.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-requirements" class="anchor" aria-hidden="true" href="#requirements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;python&lt;/code&gt; (both Python 2.x and 3.x supported)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pip&lt;/code&gt; (python package manager)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Docker&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installing" class="anchor" aria-hidden="true" href="#installing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installing&lt;/h2&gt;
&lt;p&gt;The easiest way to install LocalStack is via &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install localstack
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please do &lt;strong&gt;not&lt;/strong&gt; use &lt;code&gt;sudo&lt;/code&gt; or the &lt;code&gt;root&lt;/code&gt; user - LocalStack
should be installed and started entirely under a local non-root user. If you have problems
with permissions in MacOS X Sierra, install with &lt;code&gt;pip install --user localstack&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-running-in-docker" class="anchor" aria-hidden="true" href="#running-in-docker"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Running in Docker&lt;/h2&gt;
&lt;p&gt;By default, LocalStack gets started inside a Docker container using this command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localstack start
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that on MacOS you may have to run &lt;code&gt;TMPDIR=/private$TMPDIR localstack start --docker&lt;/code&gt; if
&lt;code&gt;$TMPDIR&lt;/code&gt; contains a symbolic link that cannot be mounted by Docker.)&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-using-docker-compose" class="anchor" aria-hidden="true" href="#using-docker-compose"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using &lt;code&gt;docker-compose&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;You can also use the &lt;code&gt;docker-compose.yml&lt;/code&gt; file from the repository and use this command (currently requires &lt;code&gt;docker-compose&lt;/code&gt; version 2.1+):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker-compose up
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that on MacOS you may have to run &lt;code&gt;TMPDIR=/private$TMPDIR docker-compose up&lt;/code&gt; if
&lt;code&gt;$TMPDIR&lt;/code&gt; contains a symbolic link that cannot be mounted by Docker.)&lt;/p&gt;
&lt;p&gt;Use on existing docker-compose project. Add in existing services. The project can be found in docker hub, no need to download or clone source:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;version: '2.1'
services:
...
  localstack:
    image: localstack/localstack
    ports:
      - "4567-4584:4567-4584"
      - "${PORT_WEB_UI-8080}:${PORT_WEB_UI-8080}"
    environment:
      - SERVICES=${SERVICES- }
      - DEBUG=${DEBUG- }
      - DATA_DIR=${DATA_DIR- }
      - PORT_WEB_UI=${PORT_WEB_UI- }
      - LAMBDA_EXECUTOR=${LAMBDA_EXECUTOR- }
      - KINESIS_ERROR_PROBABILITY=${KINESIS_ERROR_PROBABILITY- }
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - "${TMPDIR:-/tmp/localstack}:/tmp/localstack"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To facilitate interoperability, configuration variables can be prefixed with &lt;code&gt;LOCALSTACK_&lt;/code&gt; in docker. For instance, setting &lt;code&gt;LOCALSTACK_SERVICES=s3&lt;/code&gt; is equivalent to &lt;code&gt;SERVICES=s3&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-starting-locally-non-docker-mode" class="anchor" aria-hidden="true" href="#starting-locally-non-docker-mode"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Starting locally (non-Docker mode)&lt;/h2&gt;
&lt;p&gt;Alternatively, the infrastructure can be spun up on the local host machine (without using Docker) using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localstack start --host
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that this will require &lt;a href="#Developing"&gt;additional dependencies&lt;/a&gt;, and currently is not supported on some operating systems, including Windows.)&lt;/p&gt;
&lt;p&gt;LocalStack will attempt to automatically fetch the missing dependencies when you first start it up in "host" mode; alternatively, you can use the &lt;code&gt;full&lt;/code&gt; profile to install all dependencies at &lt;code&gt;pip&lt;/code&gt; installation time:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install "localstack[full]"
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-configurations" class="anchor" aria-hidden="true" href="#configurations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configurations&lt;/h2&gt;
&lt;p&gt;You can pass the following environment variables to LocalStack:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;SERVICES&lt;/code&gt;: Comma-separated list of service names and (optional) ports they should run on.
If no port is specified, a default port is used. Service names basically correspond to the
&lt;a href="http://docs.aws.amazon.com/cli/latest/reference/#available-services" rel="nofollow"&gt;service names of the AWS CLI&lt;/a&gt;
(&lt;code&gt;kinesis&lt;/code&gt;, &lt;code&gt;lambda&lt;/code&gt;, &lt;code&gt;sqs&lt;/code&gt;, etc), although LocalStack only supports a subset of them.
Example value: &lt;code&gt;kinesis,lambda:4569,sqs:4570&lt;/code&gt; to start Kinesis on the default port,
Lambda on port 4569, and SQS on port 4570. In addition, the following shorthand values can be
specified to run a predefined ensemble of services:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;serverless&lt;/code&gt;: run services often used for Serverless apps (&lt;code&gt;iam&lt;/code&gt;, &lt;code&gt;lambda&lt;/code&gt;, &lt;code&gt;dynamodb&lt;/code&gt;, &lt;code&gt;apigateway&lt;/code&gt;, &lt;code&gt;s3&lt;/code&gt;, &lt;code&gt;sns&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DEFAULT_REGION&lt;/code&gt;: AWS region to use when talking to the API (defaults to &lt;code&gt;us-east-1&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HOSTNAME&lt;/code&gt;: Name of the host to expose the services internally (defaults to &lt;code&gt;localhost&lt;/code&gt;).
Use this to customize the framework-internal communication, e.g., if services are
started in different containers using docker-compose.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HOSTNAME_EXTERNAL&lt;/code&gt;: Name of the host to expose the services externally (defaults to &lt;code&gt;localhost&lt;/code&gt;).
This host is used, e.g., when returning queue URLs from the SQS service to the client.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;SERVICE&amp;gt;_PORT&lt;/code&gt;: Port number to bind a specific service to (defaults to service ports above).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;SERVICE&amp;gt;_PORT_EXTERNAL&lt;/code&gt;: Port number to expose a specific service externally (defaults to service ports above). &lt;code&gt;SQS_PORT_EXTERNAL&lt;/code&gt;, for example, is used when returning queue URLs from the SQS service to the client.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;USE_SSL&lt;/code&gt;: Whether to use &lt;code&gt;https://...&lt;/code&gt; URLs with SSL encryption (defaults to &lt;code&gt;false&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;KINESIS_ERROR_PROBABILITY&lt;/code&gt;: Decimal value between 0.0 (default) and 1.0 to randomly
inject &lt;code&gt;ProvisionedThroughputExceededException&lt;/code&gt; errors into Kinesis API responses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;KINESIS_SHARD_LIMIT&lt;/code&gt;: Integer value (defaults to &lt;code&gt;100&lt;/code&gt;) or &lt;code&gt;Infinity&lt;/code&gt; (to disable), in which to kinesalite will start throwing exceptions to mimick the &lt;a href="https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html" rel="nofollow"&gt;default shard limit&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;KINESIS_LATENCY&lt;/code&gt;: Integer value (defaults to &lt;code&gt;500&lt;/code&gt;) or &lt;code&gt;0&lt;/code&gt; (to disable), in which to kinesalite will delay returning a response in order to mimick latency from a live AWS call.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DYNAMODB_ERROR_PROBABILITY&lt;/code&gt;: Decimal value between 0.0 (default) and 1.0 to randomly
inject &lt;code&gt;ProvisionedThroughputExceededException&lt;/code&gt; errors into DynamoDB API responses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_EXECUTOR&lt;/code&gt;: Method to use for executing Lambda functions. Possible values are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;local&lt;/code&gt;: run Lambda functions in a temporary directory on the local machine&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker&lt;/code&gt;: run each function invocation in a separate Docker container&lt;/li&gt;
&lt;li&gt;&lt;code&gt;docker-reuse&lt;/code&gt;: create one Docker container per function and reuse it across invocations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For &lt;code&gt;docker&lt;/code&gt; and &lt;code&gt;docker-reuse&lt;/code&gt;, if LocalStack itself is started inside Docker, then
the &lt;code&gt;docker&lt;/code&gt; command needs to be available inside the container (usually requires to run the
container in privileged mode). Default is &lt;code&gt;docker&lt;/code&gt;, fallback to &lt;code&gt;local&lt;/code&gt; if Docker is not available.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_REMOTE_DOCKER&lt;/code&gt; determines whether Lambda code is copied or mounted into containers.
Possible values are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;true&lt;/code&gt; (default): your Lambda function definitions will be passed to the container by
copying the zip file (potentially slower). It allows for remote execution, where the host
and the client are not on the same machine.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;false&lt;/code&gt;: your Lambda function definitions will be passed to the container by mounting a
volume (potentially faster). This requires to have the Docker client and the Docker
host on the same machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_DOCKER_NETWORK&lt;/code&gt; Specifies the docker network for the container running your lambda function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_CONTAINER_REGISTRY&lt;/code&gt; Use an alternative docker registry to pull lambda execution containers. Default is &lt;code&gt;lambci/lambda&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DATA_DIR&lt;/code&gt;: Local directory for saving persistent data (currently only supported for these services:
Kinesis, DynamoDB, Elasticsearch, S3). Set it to &lt;code&gt;/tmp/localstack/data&lt;/code&gt; to enable persistence
(&lt;code&gt;/tmp/localstack&lt;/code&gt; is mounted into the Docker container), leave blank to disable
persistence (default).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;PORT_WEB_UI&lt;/code&gt;: Port for the Web user interface (dashboard). Default is &lt;code&gt;8080&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;SERVICE&amp;gt;_BACKEND&lt;/code&gt;: Custom endpoint URL to use for a specific service, where &lt;code&gt;&amp;lt;SERVICE&amp;gt;&lt;/code&gt; is the uppercase
service name (currently works for: &lt;code&gt;APIGATEWAY&lt;/code&gt;, &lt;code&gt;CLOUDFORMATION&lt;/code&gt;, &lt;code&gt;DYNAMODB&lt;/code&gt;, &lt;code&gt;ELASTICSEARCH&lt;/code&gt;,
&lt;code&gt;KINESIS&lt;/code&gt;, &lt;code&gt;S3&lt;/code&gt;, &lt;code&gt;SNS&lt;/code&gt;, &lt;code&gt;SQS&lt;/code&gt;). This allows to easily integrate third-party services into LocalStack.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;FORCE_NONINTERACTIVE&lt;/code&gt;: when running with Docker, disables the &lt;code&gt;--interactive&lt;/code&gt; and &lt;code&gt;--tty&lt;/code&gt; flags. Useful when running headless.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DOCKER_FLAGS&lt;/code&gt;: Allows to pass custom flags (e.g., volume mounts) to "docker run" when running LocalStack in Docker.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DOCKER_CMD&lt;/code&gt;: Shell command used to run Docker containers, e.g., set to &lt;code&gt;"sudo docker"&lt;/code&gt; to run as sudo (default: &lt;code&gt;docker&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;START_WEB&lt;/code&gt;: Flag to control whether the Web API should be started in Docker (values: &lt;code&gt;0&lt;/code&gt;/&lt;code&gt;1&lt;/code&gt;; default: &lt;code&gt;1&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_FALLBACK_URL&lt;/code&gt;: Fallback URL to use when a non-existing Lambda is invoked. Either records invocations in DynamoDB (value &lt;code&gt;dynamodb://&amp;lt;table_name&amp;gt;&lt;/code&gt;) or forwards invocations as a POST request (value &lt;code&gt;http(s)://...&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;EXTRA_CORS_ALLOWED_HEADERS&lt;/code&gt;: Comma-separated list of header names to be be added to &lt;code&gt;Access-Control-Allow-Headers&lt;/code&gt; CORS header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;EXTRA_CORS_EXPOSE_HEADERS&lt;/code&gt;: Comma-separated list of header names to be be added to &lt;code&gt;Access-Control-Expose-Headers&lt;/code&gt; CORS header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LAMBDA_JAVA_OPTS&lt;/code&gt;: Allow passing custom JVM options (e.g., &lt;code&gt;-Xmx512M&lt;/code&gt;) to Java Lambdas executed in Docker. Use &lt;code&gt;_debug_port_&lt;/code&gt; placeholder to configure the debug port (e.g., &lt;code&gt;-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=_debug_port_&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, the following &lt;em&gt;read-only&lt;/em&gt; environment variables are available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LOCALSTACK_HOSTNAME&lt;/code&gt;: Name of the host where LocalStack services are available.
This is needed in order to access the services from within your Lambda functions
(e.g., to store an item to DynamoDB or S3 from Lambda).
The variable &lt;code&gt;LOCALSTACK_HOSTNAME&lt;/code&gt; is available for both, local Lambda execution
(&lt;code&gt;LAMBDA_EXECUTOR=local&lt;/code&gt;) and execution inside separate Docker containers (&lt;code&gt;LAMBDA_EXECUTOR=docker&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-dynamically-updating-configuration-at-runtime" class="anchor" aria-hidden="true" href="#dynamically-updating-configuration-at-runtime"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Dynamically updating configuration at runtime&lt;/h3&gt;
&lt;p&gt;Each of the service APIs listed &lt;a href="https://github.com/localstack/localstack#overview"&gt;above&lt;/a&gt; defines
a backdoor API under the path &lt;code&gt;/?_config_&lt;/code&gt; which allows to dynamically update configuration variables
defined in &lt;a href="https://github.com/localstack/localstack/blob/master/localstack/config.py"&gt;&lt;code&gt;config.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, to dynamically set &lt;code&gt;KINESIS_ERROR_PROBABILITY=1&lt;/code&gt; at runtime, use the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -v -d '{"variable":"KINESIS_ERROR_PROBABILITY","value":1}' 'http://localhost:4568/?_config_'
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-initializing-a-fresh-instance" class="anchor" aria-hidden="true" href="#initializing-a-fresh-instance"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Initializing a fresh instance&lt;/h3&gt;
&lt;p&gt;When a container is started for the first time, it will execute files with extensions .sh that are found in &lt;code&gt;/docker-entrypoint-initaws.d&lt;/code&gt;. Files will be executed in alphabetical order. You can easily create aws resources on localstack using &lt;code&gt;awslocal&lt;/code&gt; (or &lt;code&gt;aws&lt;/code&gt;) cli tool in the initialization scripts.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-a-note-about-using-custom-ssl-certificates-for-use_ssl1" class="anchor" aria-hidden="true" href="#a-note-about-using-custom-ssl-certificates-for-use_ssl1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;A note about using custom SSL certificates (for &lt;code&gt;USE_SSL=1&lt;/code&gt;)&lt;/h2&gt;
&lt;p&gt;If you need to use your own SSL Certificate and keep it persistent and not use the random automatic generated Certificate, you can place into the localstack temporary directory :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/tmp/localstack/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the three named files below :&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;server.test.pem
server.test.pem.crt
server.test.pem.key&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;the file &lt;code&gt;server.test.pem&lt;/code&gt; must contains your key file content, your certificate and chain certificate files contents (do a cat in this order)&lt;/li&gt;
&lt;li&gt;the file &lt;code&gt;server.test.pem.crt&lt;/code&gt; must contains your certificate and chains files contents (do a 'cat' in this order)&lt;/li&gt;
&lt;li&gt;the file server.test.pem.key must contains your key file content&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-using-use_ssl-and-own-persistent-certificate-with-docker-compose" class="anchor" aria-hidden="true" href="#using-use_ssl-and-own-persistent-certificate-with-docker-compose"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using USE_SSL and own persistent certificate with docker-compose&lt;/h3&gt;
&lt;p&gt;Typically with docker-compose you can add into docker-compose.yml this volume to the localstack services :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;volumes:
      - "${PWD}/ls_tmp:/tmp/localstack"
      - "/var/run/docker.sock:/var/run/docker.sock"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;local directory &lt;strong&gt;ls_tmp&lt;/strong&gt; must contains the three files (server.test.pem, server.test.pem.crt, server.test.pem.key)&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-accessing-the-infrastructure-via-cli-or-code" class="anchor" aria-hidden="true" href="#accessing-the-infrastructure-via-cli-or-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Accessing the infrastructure via CLI or code&lt;/h2&gt;
&lt;p&gt;You can point your &lt;code&gt;aws&lt;/code&gt; CLI to use the local infrastructure, for example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aws --endpoint-url=http://localhost:4568 kinesis list-streams
{
    "StreamNames": []
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NEW&lt;/strong&gt;: Check out &lt;a href="https://github.com/localstack/awscli-local"&gt;awslocal&lt;/a&gt;, a thin CLI wrapper
that runs commands directly against LocalStack (no need to specify &lt;code&gt;--endpoint-url&lt;/code&gt; anymore).
Install it via &lt;code&gt;pip install awscli-local&lt;/code&gt;, and then use it as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;awslocal kinesis list-streams
{
    "StreamNames": []
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;: Use the environment variable &lt;code&gt;$LOCALSTACK_HOSTNAME&lt;/code&gt; to determine the target host
inside your Lambda function. See &lt;a href="#Configurations"&gt;Configurations&lt;/a&gt; section for more details.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-client-libraries" class="anchor" aria-hidden="true" href="#client-libraries"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Client Libraries&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python: &lt;a href="https://github.com/localstack/localstack-python-client"&gt;https://github.com/localstack/localstack-python-client&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;alternatively, you can also use &lt;code&gt;boto3&lt;/code&gt; and use the &lt;code&gt;endpoint_url&lt;/code&gt; parameter when creating a connection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(more coming soon...)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-integration-with-nosetests" class="anchor" aria-hidden="true" href="#integration-with-nosetests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Integration with nosetests&lt;/h2&gt;
&lt;p&gt;If you want to use LocalStack in your integration tests (e.g., nosetests), simply fire up the
infrastructure in your test setup method and then clean up everything in your teardown method:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from localstack.services import infra

def setup():
    infra.start_infra(asynchronous=True)

def teardown():
    infra.stop_infra()

def my_app_test():
    # here goes your test logic
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See the example test file &lt;code&gt;tests/integration/test_integration.py&lt;/code&gt; for more details.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-integration-with-serverless" class="anchor" aria-hidden="true" href="#integration-with-serverless"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Integration with Serverless&lt;/h2&gt;
&lt;p&gt;You can use the &lt;a href="https://www.npmjs.com/package/serverless-localstack" rel="nofollow"&gt;&lt;code&gt;serverless-localstack&lt;/code&gt;&lt;/a&gt; plugin to easily run &lt;a href="https://serverless.com/framework/" rel="nofollow"&gt;Serverless&lt;/a&gt; applications on LocalStack.
For more information, please check out the plugin repository here:
&lt;a href="https://github.com/localstack/serverless-localstack"&gt;https://github.com/localstack/serverless-localstack&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-using-local-code-with-lambda" class="anchor" aria-hidden="true" href="#using-local-code-with-lambda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using local code with Lambda&lt;/h2&gt;
&lt;p&gt;In order to mount a local folder, ensure that &lt;code&gt;LAMBDA_REMOTE_DOCKER&lt;/code&gt; is set to &lt;code&gt;false&lt;/code&gt; then set the S3 bucket name to &lt;code&gt;__local__&lt;/code&gt; and the S3 key to your local path:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    awslocal lambda create-function --function-name myLambda \
      --code S3Bucket="__local__",S3Key="/my/local/lambda/folder" \
      --handler index.myHandler \
      --runtime nodejs8.10 \
      --role whatever
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-integration-with-javajunit" class="anchor" aria-hidden="true" href="#integration-with-javajunit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Integration with Java/JUnit&lt;/h2&gt;
&lt;p&gt;In order to use LocalStack with Java, the project ships with a simple JUnit runner and a JUnit 5 extension. Take a look
at the example JUnit test in &lt;code&gt;ext/java&lt;/code&gt;. When you run the test, all dependencies are automatically
downloaded and installed to a temporary directory in your system.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;...
import cloud.localstack.LocalstackTestRunner;
import cloud.localstack.TestUtils;

@RunWith(LocalstackTestRunner.class)
public class MyCloudAppTest {

  @Test
  public void testLocalS3API() {
    AmazonS3 s3 = TestUtils.getClientS3()
    List&amp;lt;Bucket&amp;gt; buckets = s3.listBuckets();
    ...
  }

}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or with JUnit 5 :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@ExtendWith(LocalstackExtension.class)
public class MyCloudAppTest {
   ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, there is a version of the LocalStack Test Runner which runs in a docker container
instead of installing LocalStack on the current machine. The only dependency is to have docker
installed locally. The test runner will automatically pull the image and start the container for the
duration of the test.  The container can be configured by using the @LocalstackDockerProperties annotation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@RunWith(LocalstackDockerTestRunner.class)
@LocalstackDockerProperties(services = { "sqs", "kinesis:77077" })
public class MyDockerCloudAppTest {

  @Test
  public void testKinesis() {
    AmazonKinesis kinesis = DockerTestUtils.getClientKinesis();

    ListStreamsResult streams = kinesis.listStreams();
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or with JUnit 5 :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@ExtendWith(LocalstackDockerExtension.class)
@LocalstackDockerProperties(services = { "sqs", "kinesis:77077" })
public class MyDockerCloudAppTest {
   ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The LocalStack JUnit test runner is published as an artifact in Maven Central.
Simply add the following dependency to your &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;cloud.localstack&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;localstack-utils&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.1.22&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can configure the Docker behaviour using the &lt;code&gt;@LocalstackDockerProperties&lt;/code&gt; annotation with the following parameters:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;property&lt;/th&gt;
&lt;th&gt;usage&lt;/th&gt;
&lt;th&gt;type&lt;/th&gt;
&lt;th&gt;default value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pullNewImage&lt;/td&gt;
&lt;td&gt;Determines if a new image is pulled from the docker repo before the tests are run.&lt;/td&gt;
&lt;td&gt;boolean&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;randomizePorts&lt;/td&gt;
&lt;td&gt;Determines if the container should expose the default local stack ports (4567-4583) or if it should expose randomized ports.&lt;/td&gt;
&lt;td&gt;boolean&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;services&lt;/td&gt;
&lt;td&gt;Determines which services should be run when the localstack starts.&lt;/td&gt;
&lt;td&gt;String[]&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;imageTag&lt;/td&gt;
&lt;td&gt;Use a specific image tag for docker container&lt;/td&gt;
&lt;td&gt;String&lt;/td&gt;
&lt;td&gt;latest&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hostNameResolver&lt;/td&gt;
&lt;td&gt;Used for determining the host name of the machine running the docker containers so that the containers can be addressed.&lt;/td&gt;
&lt;td&gt;IHostNameResolver&lt;/td&gt;
&lt;td&gt;localhost&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;environmentVariableProvider&lt;/td&gt;
&lt;td&gt;Used for injecting environment variables into the container.&lt;/td&gt;
&lt;td&gt;IEnvironmentVariableProvider&lt;/td&gt;
&lt;td&gt;Empty Map&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;NB : When specifying the port in the &lt;code&gt;services&lt;/code&gt; property, you cannot use &lt;code&gt;randomizePorts = true&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-troubleshooting" class="anchor" aria-hidden="true" href="#troubleshooting"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Troubleshooting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you're using AWS Java libraries with Kinesis, please, refer to &lt;a href="https://github.com/mhart/kinesalite#cbor-protocol-issues-with-the-java-sdk"&gt;CBOR protocol issues with the Java SDK guide&lt;/a&gt; how to disable CBOR protocol which is not supported by kinesalite.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accessing local S3 from Java: To avoid domain name resolution issues, you need to enable &lt;strong&gt;path style access&lt;/strong&gt; on your client:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;s3.setS3ClientOptions(S3ClientOptions.builder().setPathStyleAccess(true).build());
// There is also an option to do this if you're using any of the client builder classes:
AmazonS3ClientBuilder builder = AmazonS3ClientBuilder.standard();
builder.withPathStyleAccessEnabled(true);
...
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Mounting the temp. directory: Note that on MacOS you may have to run &lt;code&gt;TMPDIR=/private$TMPDIR docker-compose up&lt;/code&gt; if
&lt;code&gt;$TMPDIR&lt;/code&gt; contains a symbolic link that cannot be mounted by Docker.
(See details here: &lt;a href="https://bitbucket.org/atlassian/localstack/issues/40/getting-mounts-failed-on-docker-compose-up" rel="nofollow"&gt;https://bitbucket.org/atlassian/localstack/issues/40/getting-mounts-failed-on-docker-compose-up&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you run into file permission issues on &lt;code&gt;pip install&lt;/code&gt; under Mac OS (e.g., &lt;code&gt;Permission denied: '/Library/Python/2.7/site-packages/six.py'&lt;/code&gt;), then you may have to re-install &lt;code&gt;pip&lt;/code&gt; via Homebrew (see &lt;a href="https://github.com/localstack/localstack/issues/260#issuecomment-334458631"&gt;this discussion thread&lt;/a&gt;). Alternatively, try installing
with the &lt;code&gt;--user&lt;/code&gt; flag: &lt;code&gt;pip install --user localstack&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you are deploying within OpenShift, please be aware: the pod must run as &lt;code&gt;root&lt;/code&gt;, and the user must have capabilities added to the running pod, in order to allow Elasticsearch to be run as the non-root &lt;code&gt;localstack&lt;/code&gt; user.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The environment variable &lt;code&gt;no_proxy&lt;/code&gt; is rewritten by LocalStack.
(Internal requests will go straight via localhost, bypassing any proxy configuration).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For troubleshooting LocalStack start issues, you can check debug logs by running &lt;code&gt;DEBUG=1 localstack start&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In case you get errors related to node/nodejs, you may find (this issue comment: &lt;a href="https://github.com/localstack/localstack/issues/227#issuecomment-319938530"&gt;https://github.com/localstack/localstack/issues/227#issuecomment-319938530&lt;/a&gt;) helpful.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you are using AWS Java libraries and need to disable SSL certificate checking, add &lt;code&gt;-Dcom.amazonaws.sdk.disableCertChecking&lt;/code&gt; to the java invocation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-developing" class="anchor" aria-hidden="true" href="#developing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Developing&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-requirements-for-developing-or-starting-locally" class="anchor" aria-hidden="true" href="#requirements-for-developing-or-starting-locally"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements for developing or starting locally&lt;/h3&gt;
&lt;p&gt;To develop new features, or to start the stack locally (outside of Docker), the following additional tools are required:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;make&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;npm&lt;/code&gt; (node.js package manager)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;java&lt;/code&gt;/&lt;code&gt;javac&lt;/code&gt; (Java 8 runtime environment and compiler)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mvn&lt;/code&gt; (Maven, the build system for Java)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-development-environment" class="anchor" aria-hidden="true" href="#development-environment"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development Environment&lt;/h3&gt;
&lt;p&gt;If you pull the repo in order to extend/modify LocalStack, run this command to install
all the dependencies:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will install the required pip dependencies in a local Python virtualenv directory
&lt;code&gt;.venv&lt;/code&gt; (your global python packages will remain untouched), as well as some node modules
in &lt;code&gt;./localstack/node_modules/&lt;/code&gt;. Depending on your system, some pip/npm modules may require
additional native libs installed.&lt;/p&gt;
&lt;p&gt;The Makefile contains a target to conveniently run the local infrastructure for development:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make infra
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check out the
&lt;a href="https://github.com/localstack/localstack/tree/master/doc/developer_guides"&gt;developer guide&lt;/a&gt; which
contains a few instructions on how to get started with developing (and debugging) features for
LocalStack.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-testing" class="anchor" aria-hidden="true" href="#testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing&lt;/h2&gt;
&lt;p&gt;The project contains a set of unit and integration tests that can be kicked off via a make
target:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make test
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-web-dashboard" class="anchor" aria-hidden="true" href="#web-dashboard"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Web Dashboard&lt;/h2&gt;
&lt;p&gt;The projects also comes with a simple Web dashboard that allows to view the deployed AWS
components and the relationship between them.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;localstack web
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-other-ui-clients" class="anchor" aria-hidden="true" href="#other-ui-clients"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other UI Clients&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://getcommandeer.com" rel="nofollow"&gt;Commandeer desktop app&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.npmjs.com/package/dynamodb-admin" rel="nofollow"&gt;DynamoDB Admin Web UI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-change-log" class="anchor" aria-hidden="true" href="#change-log"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Change Log&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;v0.10.5: Various CloudFormation fixes: deployment of API GW method integrations, properly skip resource updates, Lambda SQS event source mapping, avoid duplicate resource creation, support for ApiGateway::GatewayResponse and Events::Rule, log groups for Lambdas; support adding Lambda policies; customize Docker registry for Lambda images; support multiple configurations in S3 notifications; fix encoding of non-ASCII results from API Gateway; allow docker-reuse to use mounted volumes; support presigned S3 URL upload notifications; fix lookup of Python Lambda handler in sub directories; upgrade kinesalite; fix duplicate CORS headers; fix mapping of Lambda versions and ARNs; fix SNS x-amz-sns-message-type header; send SNS confirmation message for HTTP(S) subscriptions; fix DynamoDB local libs for Docker Alpine; add CF support for SNS subscriptions; fix RecordId for firehose put-record-batch; fix SQS messages with multi-byte characters; avoid creating multiple SNS subscriptions; add .bat script and support running under Windows; fix S3 location constraint for CF&lt;/li&gt;
&lt;li&gt;v0.10.4: Add checks for open UDP ports; fix S3 chunked encoding uploads; fix LatestStreamLabel; fix CORS headers for SQS/SNS; set Java lambda debug port only when needed; expose default region in a util function; fix MacOS tmp folder; clear tmp supervisord logs at container startup; fix signed header requests for S3; expose Web UI via HTTPS; add Timestamp to SNS messages; fix attributes for SQS queues addressed via URL&lt;/li&gt;
&lt;li&gt;v0.10.3: Allow specifying data types for CF attributes; add API for service status and starting services at runtime; support NextShardIterator in DDB streams; add mock responses for S3 encryption and replication; fix rendering of resources in web UI; custom SQS queue attributes; fix Lambda docker command and imports; fix SQS queue physical ID in CF; allow proxy listener to define custom backend per request; support Lambda event body over stdin; exclude &lt;code&gt;ingest-geoip&lt;/code&gt; ES module to optimize image size; skip checking MD5 on S3 copy; fix DynamoDB table ARN for CF; fix CF deployment of StepFunction activities; fix uploading of Java Lambda as JAR in ZIP; fix installing libs for plugins; added &lt;code&gt;LAMBDA_JAVA_OPTS&lt;/code&gt; for Java Lambda debugging; bump Maven dependency versions; refactor Lambda API; fix boolean strings in CF templates; allow overriding AWS account id with &lt;code&gt;TEST_AWS_ACCOUNT_ID&lt;/code&gt;; fix incorrect region for API GW resources created via CF; fix permissions for cache files in &lt;code&gt;/tmp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;v0.10.2: Fix logging issue with async Lambdas; fix kinesis records processing; add basic support for &lt;code&gt;Ref&lt;/code&gt; in CloudFormation; fix ddb streams uuid generation; upgrade travis CI setup; fix DynamoDB error messages; cache server processes&lt;/li&gt;
&lt;li&gt;v0.10.0: Lazy loading of libraries; fix handling of regions; add API multiserver; improve CPU profiling; fix ES xpack installation; add basic EventBridge support; refactor Lambda API and executor; add MessageAttributes on SNS payloads; tagging for SNS; ability to customize docker command&lt;/li&gt;
&lt;li&gt;v0.9.6: Add API Gateway SQS proxy; fix command to push Docker image; fix Docker bridge IP configuration; fix SSL issue in dashboard infra; updates to README&lt;/li&gt;
&lt;li&gt;v0.9.5: Reduce Docker image size by squashing; fix response body for presigned URL S3 PUT requests; fix CreateDate returned by IAM; fix account IDs for CF and SNS; fix topic checks for SMS using SNS; improve documentation around &lt;code&gt;@LocalstackDockerProperties&lt;/code&gt;; add basic EC2 support; upgrade to ElasticSearch 6.7; set Last-Modified header in S3; preserve logic with uppercase event keys in Java; add support for nodejs 10.x Lambdas&lt;/li&gt;
&lt;li&gt;v0.9.4: Fix ARNs in CloudFormation deployments; write stderr to file in supervisord; fix Lambda invocation times; fix canonicalization of service names when running in Docker; add support for &lt;code&gt;@Nested&lt;/code&gt; in Junit5; add support for batch/transaction in DynamoDB; fix output buffering for subprocesses; assign unique ports under docker-reuse; check if topic ARN exists before publish&lt;/li&gt;
&lt;li&gt;v0.9.3: Fix output buffering of child processes; new release of Java libs; add imageTag attribute for Java annotation&lt;/li&gt;
&lt;li&gt;v0.9.2: Update to Python 3 in Dockerfile; preserve attributes when SNS Subscribe; fix event source mapping in Lambda; fix CORS ExposeHeaders; set Lambda timeout in secs; add tags support for Lambda/Firehose; add message attributes for SQS/Lambda; fix shard count support for Kinesis; fix port mappings for CloudFormation&lt;/li&gt;
&lt;li&gt;v0.9.1: Define dependent and composite services in config; forward Lambda logs to CloudWatch Logs; add SQS event deserializing for Lambda; fix AWS_PROXY for JSON list payload; add START_WEB config parameter; return correct location for S3 multipart uploads; add support for Lambda custom runtime; fix account ID for IAM responses; fix using correct SSL cert; limit memory usage for Java processes; fix unicode encoding for SNS messages; allow using &lt;code&gt;LOCALSTACK_&lt;/code&gt; prefix in Docker environment variables; enable request forwarding for non-existing Lambdas; fix large downloads for S3; add API endpoint for dynamically updating config variables; fix CloudFormation stack update&lt;/li&gt;
&lt;li&gt;v0.9.0: Enhance integration with Serverless; refactor CloudFormation implementation; add support for Step Functions, IAM, STS; fix CloudFormation integration; support mounting Lambda code locally; add &lt;code&gt;docker-entrypoint-initaws.d&lt;/code&gt; dir for initializing resources; add S3Event Parser for Lambda; fix S3 chunk encoding; fix S3 multipart upload notification; add dotnetcore2.1 and ruby2.5 Lambda runtimes; fix issues with JDK 9; install ES plugins available in AWS&lt;/li&gt;
&lt;li&gt;v0.8.10: Add kclpy to pip package; fix badges in README&lt;/li&gt;
&lt;li&gt;v0.8.9: Replace moto-ext with upstream moto; fix SNS message attributes; fix swagger; make external SQS port configurable; support for SNS DeleteTopic; S3 notifications for multipart uploads; support requestContext in AWS_PROXY integration; update docs for SSL usage&lt;/li&gt;
&lt;li&gt;v0.8.8: Support Docker network config for Lambda containers; support queryStringParameters for Lambda AWS_PROXY apigateway; add AWS SecretsManager service; add SQS/Lambda integration; add support for Firehose Kinesis source; add GetAlias to Lambda API; add function properties to LambdaContext for invocations; fix extraction of Java Lambda archives; check region headers for SNS; fix Lambda output buffering; fix S3 download of gzip; bump ElasticMQ to 0.14.5; fix Lambda response codes; fix syntax issues for Python 3.7&lt;/li&gt;
&lt;li&gt;v0.8.7: Support .Net Core 2.0 and nodejs8.10 Lambdas; refactor Java libs and integrate with JUnit 5; support tags for ES domains; add CloudFormation support for SNS topics; fix kinesis error injection; fix override of &lt;code&gt;ES_JAVA_OPTS&lt;/code&gt;; fix SQS CORS preflight response; fix S3 content md5 checks and Host header; fix ES startup issue; Bump elasticmq to 0.13.10; bump kinesalite version&lt;/li&gt;
&lt;li&gt;v0.8.6: Fixes for Windows installation; bump ES to 6.2.0; support filter policy for SNS; upgrade kinesalite; refactor JUnit runner; support Lambda PutFunctionConcurrency and GetEventSourceMapping; fixes for Terraform; add golang support to Lambda; fix file permission issue in Java Lambda tests; fix S3 bucket notification config&lt;/li&gt;
&lt;li&gt;v0.8.5: Fix DDB streams event type; implement CF Fn::GetAZs; async lambda for DDB events; fix S3 content-type; fix CF deployer for SQS; fix S3 ExposePorts; fix message subject in SNS; support for Firehose -&amp;gt; ES; pass external env vars to containers from Java; add mock for list-queue-tags; enhance docker test runner; fix Windows installation issues; new version of Java libs&lt;/li&gt;
&lt;li&gt;v0.8.4: Fix &lt;code&gt;pipenv&lt;/code&gt; dependency issue; Docker JUnit test runner; POJO type for Java Lambda RequestHandler; Java Lambda DynamoDB event; reuse Docker containers for Lambda invocations; API Gateway wildcard path segments; fix SNS RawMessageDelivery&lt;/li&gt;
&lt;li&gt;v0.8.3: Fix DDB stream events for UPDATE operations; fix DDB streams sequence numbers; fix transfer-encoding for DDB; fix requests with missing content-length header; support non-ascii content in DynamoDB items; map external port for SQS queue URLs; default to LAMBDA_REMOTE_DOCKER=true if running in Docker; S3 lifecycle support; reduce Docker image size&lt;/li&gt;
&lt;li&gt;v0.8.2: Fix S3 bucket notification configuration; CORS headers for API Gateway; fix &amp;gt;128k S3 multipart uploads; return valid ShardIDs in DynamoDB Streams; fix hardcoded "ddblocal" DynamoDB TableARN; import default service ports from localstack-client; fix S3 bucket policy response; Execute lambdas asynchronously if the source is a topic&lt;/li&gt;
&lt;li&gt;v0.8.1: Improvements in Lambda API: publish-version, list-version, function aliases; use single map with Lambda function details; workaround for SQS .fifo queues; add test for S3 upload; initial support for SSM; fix regex to replace SQS queue URL hostnames; update linter (single quotes); use &lt;code&gt;docker.for.mac.localhost&lt;/code&gt; to connect to LocalStack from Docker on Mac; fix b64 encoding for Java Lambdas; fix path of moto_server command&lt;/li&gt;
&lt;li&gt;v0.8.0: Fix request data in &lt;code&gt;GenericProxyHandler&lt;/code&gt;; add &lt;code&gt;$PORT_WEB_UI&lt;/code&gt; and &lt;code&gt;$HOSTNAME_EXTERNAL&lt;/code&gt; configs; API Gateway path parameters; enable flake8 linting; add config for service backend URLs; use ElasticMQ instead of moto for SQS; expose &lt;code&gt;$LOCALSTACK_HOSTNAME&lt;/code&gt;; custom environment variable support for Lambda; improve error logging and installation for Java/JUnit; add support for S3 REST Object POST&lt;/li&gt;
&lt;li&gt;v0.7.5: Fix issue with incomplete parallel downloads; bypass http_proxy for internal requests; use native Python code to unzip archives; download KCL client libs only for testing and not on pip install&lt;/li&gt;
&lt;li&gt;v0.7.4: Refactor CLI and enable plugins; support unicode names for S3; fix SQS names containing a dot character; execute Java Lambda functions in Docker containers; fix DynamoDB error handling; update docs&lt;/li&gt;
&lt;li&gt;v0.7.3: Extract proxy listeners into (sub-)classes; put java libs into a single "fat" jar; fix issue with non-daemonized threads; refactor code to start flask services&lt;/li&gt;
&lt;li&gt;v0.7.2: Fix DATA_DIR config when running in Docker; fix Maven dependencies; return 'ConsumedCapacity' from DynamoDB get-item; use Queue ARN instead of URL for S3 bucket notifications&lt;/li&gt;
&lt;li&gt;v0.7.1: Fix S3 API to GET bucket notifications; release Java artifacts to Maven Central; fix S3 file access from Spark; create DDB stream on UpdateTable; remove AUI dependency, optimize size of Docker image&lt;/li&gt;
&lt;li&gt;v0.7.0: Support for Kinesis in CloudFormation; extend and integrate Java tests in CI; publish Docker image under new name; update READMEs and license agreements&lt;/li&gt;
&lt;li&gt;v0.6.2: Major refactoring of installation process, lazy loading of dependencies&lt;/li&gt;
&lt;li&gt;v0.6.1: Add CORS headers; platform compatibility fixes (remove shell commands and sh module); add CloudFormation validate-template; fix Lambda execution in Docker; basic domain handling in ES API; API Gateway authorizers&lt;/li&gt;
&lt;li&gt;v0.6.0: Load services as plugins; fix service default ports; fix SQS-&amp;gt;SNS and MD5 of message attributes; fix Host header for S3&lt;/li&gt;
&lt;li&gt;v0.5.5: Enable SSL encryption for all service endpoints (&lt;code&gt;USE_SSL&lt;/code&gt; config); create Docker base image; fix issue with DATA_DIR&lt;/li&gt;
&lt;li&gt;v0.5.4: Remove hardcoded /tmp/ for Windows-compat.; update CLI and docs; fix S3/SNS notifications; disable Elasticsearch compression&lt;/li&gt;
&lt;li&gt;v0.5.3: Add CloudFormation support for serverless / API Gateway deployments; fix installation via pypi; minor fix for Java (passing of environment variables)&lt;/li&gt;
&lt;li&gt;v0.5.0: Extend DynamoDB Streams API; fix keep-alive connection for S3; fix deadlock in nested Lambda executions; add integration SNS-&amp;gt;Lambda; CloudFormation serverless example; replace dynalite with DynamoDBLocal; support Lambda execution in remote Docker container; fix CloudWatch metrics for Lambda invocation errors&lt;/li&gt;
&lt;li&gt;v0.4.3: Initial support for CloudWatch metrics (for Lambda functions); HTTP forwards for API Gateway; fix S3 message body signatures; download Lambda archive from S3 bucket; fix/extend ES tests&lt;/li&gt;
&lt;li&gt;v0.4.2: Initial support for Java Lambda functions; CloudFormation deployments; API Gateway tests&lt;/li&gt;
&lt;li&gt;v0.4.1: Python 3 compatibility; data persistence; add seq. numbers in Kinesis events; limit Elasticsearch memory&lt;/li&gt;
&lt;li&gt;v0.4.0: Execute Lambda functions in Docker containers; CORS headers for S3&lt;/li&gt;
&lt;li&gt;v0.3.11: Add Route53, SES, CloudFormation; DynamoDB fault injection; UI tweaks; refactor config&lt;/li&gt;
&lt;li&gt;v0.3.10: Add initial support for S3 bucket notifications; fix subprocess32 installation&lt;/li&gt;
&lt;li&gt;v0.3.9: Make services/ports configurable via $SERVICES; add tests for Firehose+S3&lt;/li&gt;
&lt;li&gt;v0.3.8: Fix Elasticsearch via local bind and proxy; refactoring; improve error logging&lt;/li&gt;
&lt;li&gt;v0.3.5: Fix lambda handler name; fix host name for S3 API; install web libs on pip install&lt;/li&gt;
&lt;li&gt;v0.3.4: Fix file permissions in build; fix and add UI to Docker image; add stub of ES API&lt;/li&gt;
&lt;li&gt;v0.3.3: Add version tags to Docker images&lt;/li&gt;
&lt;li&gt;v0.3.2: Add support for Redshift API; code refactoring&lt;/li&gt;
&lt;li&gt;v0.3.1: Add Dockerfile and push image to Docker Hub&lt;/li&gt;
&lt;li&gt;v0.3.0: Add simple integration for JUnit; improve process signal handling&lt;/li&gt;
&lt;li&gt;v0.2.11: Refactored the AWS assume role function&lt;/li&gt;
&lt;li&gt;v0.2.10: Added AWS assume role functionality.&lt;/li&gt;
&lt;li&gt;v0.2.9: Kinesis error response formatting&lt;/li&gt;
&lt;li&gt;v0.2.7: Throw Kinesis errors randomly&lt;/li&gt;
&lt;li&gt;v0.2.6: Decouple SNS/SQS: intercept SNS calls and forward to subscribed SQS queues&lt;/li&gt;
&lt;li&gt;v0.2.5: Return error response from Kinesis if flag is set&lt;/li&gt;
&lt;li&gt;v0.2.4: Allow Lambdas to use &lt;strong&gt;file&lt;/strong&gt; (import from file instead of exec'ing)&lt;/li&gt;
&lt;li&gt;v0.2.3: Improve Kinesis/KCL auto-checkpointing (leases in DDB)&lt;/li&gt;
&lt;li&gt;v0.2.0: Speed up installation time by lazy loading libraries&lt;/li&gt;
&lt;li&gt;v0.1.19: Pass shard_id in records sent from KCL process&lt;/li&gt;
&lt;li&gt;v0.1.16: Minor restructuring and refactoring (create separate kinesis_util.py)&lt;/li&gt;
&lt;li&gt;v0.1.14: Fix AWS tokens when creating Elasticsearch client&lt;/li&gt;
&lt;li&gt;v0.1.11: Add startup/initialization notification for KCL process&lt;/li&gt;
&lt;li&gt;v0.1.10: Bump version of amazon_kclpy to 1.4.1&lt;/li&gt;
&lt;li&gt;v0.1.9: Add initial support for SQS/SNS&lt;/li&gt;
&lt;li&gt;v0.1.8: Fix installation of JARs in amazon_kclpy if localstack is installed transitively&lt;/li&gt;
&lt;li&gt;v0.1.7: Bump version of amazon_kclpy to 1.4.0&lt;/li&gt;
&lt;li&gt;v0.1.6: Add travis-ci and coveralls configuration&lt;/li&gt;
&lt;li&gt;v0.1.5: Refactor Elasticsearch utils; fix bug in method to delete all ES indexes&lt;/li&gt;
&lt;li&gt;v0.1.4: Enhance logging; extend java KCL credentials provider (support STS assumed roles)&lt;/li&gt;
&lt;li&gt;v0.1.2: Add configurable KCL log output&lt;/li&gt;
&lt;li&gt;v0.1.0: Initial release&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributing" class="anchor" aria-hidden="true" href="#contributing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We welcome feedback, bug reports, and pull requests!&lt;/p&gt;
&lt;p&gt;For pull requests, please stick to the following guidelines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add tests for any new features and bug fixes. Ideally, each PR should increase the test coverage.&lt;/li&gt;
&lt;li&gt;Follow the existing code style (e.g., indents). A PEP8 code linting target is included in the Makefile.&lt;/li&gt;
&lt;li&gt;Put a reasonable amount of comments into the code.&lt;/li&gt;
&lt;li&gt;Separate unrelated changes into multiple pull requests.&lt;/li&gt;
&lt;li&gt;1 commit per PR: Please squash/rebase multiple commits into one single commit (to keep the history clean).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please note that by contributing any code or documentation to this repository (by
raising pull requests, or otherwise) you explicitly agree to
the &lt;a href="doc/contributor_license_agreement"&gt;&lt;strong&gt;Contributor License Agreement&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributors" class="anchor" aria-hidden="true" href="#contributors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributors&lt;/h2&gt;
&lt;p&gt;This project exists thanks to all the people who contribute.
&lt;a href="https://github.com/localstack/localstack/graphs/contributors"&gt;&lt;img src="https://camo.githubusercontent.com/70c617534022062f30a0679c0c2422a23ec605ee/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f636f6e7472696275746f72732e7376673f77696474683d383930" data-canonical-src="https://opencollective.com/localstack/contributors.svg?width=890" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-backers" class="anchor" aria-hidden="true" href="#backers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Backers&lt;/h2&gt;
&lt;p&gt;Thank you to all our backers! &lt;g-emoji class="g-emoji" alias="pray" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f64f.png"&gt;üôè&lt;/g-emoji&gt; [&lt;a href="https://opencollective.com/localstack#backer" rel="nofollow"&gt;Become a backer&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://opencollective.com/localstack#backers" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/baded82797613a9dfee852dc9b83e810dbb99e07/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f6261636b6572732e7376673f77696474683d383930" data-canonical-src="https://opencollective.com/localstack/backers.svg?width=890" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-sponsors" class="anchor" aria-hidden="true" href="#sponsors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sponsors&lt;/h2&gt;
&lt;p&gt;Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [&lt;a href="https://opencollective.com/localstack#sponsor" rel="nofollow"&gt;Become a sponsor&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;a href="https://opencollective.com/localstack/sponsor/0/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/5447958470aa26c2d09b668bf4b8e3946f94908f/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f302f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/0/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/1/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0fb41b881ff9a6f2545bf210a1b5eeefe1c2f9e0/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f312f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/1/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/2/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/105ab894cafcccc622161e8a0f070bb2e1edc38f/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f322f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/2/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/3/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/12dcc5d9654145e5609701ac986ced8a34155fd9/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f332f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/3/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/4/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/99c081e4fa906ad24a9ba4a430ce1b8f6b47f9b2/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f342f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/4/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/5/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/024967ca75aecc89725c4be25d8951eb637cc30b/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f352f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/5/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/6/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/2b266eeed387c32eb4e2c42dd5e5d005967c4024/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f362f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/6/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/7/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/c677209769bb3f28660b6ebd0c87b2e2be8d6103/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f372f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/7/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/8/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/902262e1aeccb957ef75bcbf00de78bd0ba60fb6/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f382f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/8/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://opencollective.com/localstack/sponsor/9/website" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/574b420b1fe419c38dd36d0c0a2a2d78031a3af5/68747470733a2f2f6f70656e636f6c6c6563746976652e636f6d2f6c6f63616c737461636b2f73706f6e736f722f392f6176617461722e737667" data-canonical-src="https://opencollective.com/localstack/sponsor/9/avatar.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-stargazers-over-time" class="anchor" aria-hidden="true" href="#stargazers-over-time"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Stargazers over time&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://starchart.cc/localstack/localstack" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/9d274070627cd092149bd6ed66d1e8401cd966fa/68747470733a2f2f7374617263686172742e63632f6c6f63616c737461636b2f6c6f63616c737461636b2e737667" alt="Stargazers over time" data-canonical-src="https://starchart.cc/localstack/localstack.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright (c) 2017-2019 LocalStack maintainers and contributors.&lt;/p&gt;
&lt;p&gt;Copyright (c) 2016 Atlassian and others.&lt;/p&gt;
&lt;p&gt;This version of LocalStack is released under the Apache License, Version 2.0 (see LICENSE.txt).
By downloading and using this software you agree to the
&lt;a href="doc/end_user_license_agreement"&gt;End-User License Agreement (EULA)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We build on a number of third-party software tools, including the following:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Third-Party software&lt;/th&gt;
&lt;th&gt;License&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Python/pip modules:&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;airspeed&lt;/td&gt;
&lt;td&gt;BSD License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;amazon_kclpy&lt;/td&gt;
&lt;td&gt;Amazon Software License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;boto3&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;coverage&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;docopt&lt;/td&gt;
&lt;td&gt;MIT License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;elasticsearch&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;flask&lt;/td&gt;
&lt;td&gt;BSD License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;flask_swagger&lt;/td&gt;
&lt;td&gt;MIT License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;jsonpath-rw&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;moto&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nose&lt;/td&gt;
&lt;td&gt;GNU LGPL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pep8&lt;/td&gt;
&lt;td&gt;Expat license&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;requests&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;subprocess32&lt;/td&gt;
&lt;td&gt;PSF License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Node.js/npm modules:&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kinesalite&lt;/td&gt;
&lt;td&gt;MIT License&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Other tools:&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Elasticsearch&lt;/td&gt;
&lt;td&gt;Apache License 2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>localstack</author><guid isPermaLink="false">https://github.com/localstack/localstack</guid><pubDate>Tue, 19 Nov 2019 00:16:00 GMT</pubDate></item><item><title>ownthink/KnowledgeGraphData #17 in Python, This month</title><link>https://github.com/ownthink/KnowledgeGraphData</link><description>&lt;p&gt;&lt;i&gt;Âè≤‰∏äÊúÄÂ§ßËßÑÊ®°1.4‰∫ø‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÂºÄÊ∫ê‰∏ãËΩΩ&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1 align="center"&gt;&lt;a id="user-content-Âè≤‰∏äÊúÄÂ§ßËßÑÊ®°14‰∫ø‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÂºÄÊ∫ê‰∏ãËΩΩ" class="anchor" aria-hidden="true" href="#Âè≤‰∏äÊúÄÂ§ßËßÑÊ®°14‰∫ø‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÂºÄÊ∫ê‰∏ãËΩΩ"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Âè≤‰∏äÊúÄÂ§ßËßÑÊ®°1.4‰∫ø‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÂºÄÊ∫ê‰∏ãËΩΩ&lt;/h1&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/kg.png"&gt;&lt;img src="img/kg.png" alt="Áü•ËØÜÂõæË∞±" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Áü•ËØÜÂ∞±ÊòØÂäõÈáèÔºåÁü•ËØÜÂõæË∞±ÊòØ‰∫∫Â∑•Êô∫ËÉΩÊñ∞Êó∂‰ª£ÁöÑ‰∫ßÁâ©ÔºåÁÆÄÂçïÂú∞ËØ¥Áü•ËØÜÂõæË∞±Â∞±ÊòØÈÄöËøáÂÖ≥ËÅîÂÖ≥Á≥ªÂ∞ÜÁü•ËØÜÁªÑÊàêÁΩëÁä∂ÁöÑÁªìÊûÑÔºåÁÑ∂ÂêéÊàë‰ª¨ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂèØ‰ª•ÈÄöËøáËøô‰∏™ÂõæË∞±Êù•ËÆ§ËØÜÂÖ∂‰ª£Ë°®ÁöÑËøô‰∏Ä‰∏™Áé∞ÂÆû‰∫ã‰ª∂ÔºåËøô‰∏™‰∫ã‰ª∂ÂèØ‰ª•ÊòØÁé∞ÂÆûÔºå‰πüÂèØ‰ª•ÊòØËôöÊûÑÁöÑ„ÄÇ&lt;/p&gt;
&lt;p&gt;Áü•ËØÜÂõæË∞±ÂèØ‰ª•Â∫îÁî®‰∫éÊú∫Âô®‰∫∫ÈóÆÁ≠îÁ≥ªÁªüÔºåÁü•ËØÜÊé®ËçêÁ≠âÁ≠âÔºå‰∏ãÂõæ‰∏∫Áü•ËØÜÂõæË∞±Âú®Êú∫Âô®‰∫∫‰∏äÁöÑÂ∫îÁî®„ÄÇ&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="img/bot.png"&gt;&lt;img src="img/bot.png" alt="Êú∫Âô®‰∫∫" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Êú¨Ê¨°ownthinkÂºÄÊ∫ê‰∫ÜÂè≤‰∏äÊúÄÂ§ßËßÑÊ®°ÁöÑ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÔºåÊï∞ÊçÆÊòØ‰ª•ÔºàÂÆû‰Ωì„ÄÅÂ±ûÊÄß„ÄÅÂÄºÔºâÔºåÔºàÂÆû‰Ωì„ÄÅÂÖ≥Á≥ª„ÄÅÂÆû‰ΩìÔºâÊ∑∑ÂêàÁöÑÂΩ¢ÂºèÁªÑÁªáÔºåÊï∞ÊçÆÊ†ºÂºèÈááÁî®csvÊ†ºÂºèÔºå‰∏ãËΩΩÈìæÊé•ËßÅÊñáÊú´„ÄÇ&lt;/p&gt;
&lt;p&gt;Ëß£ÂéãÂêéÊü•ÁúãÁü•ËØÜÂõæË∞±ËßÑÊ®°Ôºö&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ wc -l ownthink_v2.csv
140919781 ownthink_v2.csv&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Êü•ÁúãÁü•ËØÜÂõæË∞±Êï∞ÊçÆÔºö&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;$ head ownthink_v2.csv
ÂÆû‰Ωì,Â±ûÊÄß,ÂÄº
ËÉ∂È•¥,ÊèèËø∞,Âà´Âêç: È•¥Á≥ñ„ÄÅÁïÖÁ≥ñ„ÄÅÁïÖ„ÄÅËΩØÁ≥ñ„ÄÇ
ËØçÊù°,ÊèèËø∞,ËØçÊù°ÔºàÊãºÈü≥Ôºöc√≠ ti√°oÔºâ‰πüÂè´ËØçÁõÆÔºåÊòØËæû‰π¶Â≠¶Áî®ËØ≠ÔºåÊåáÊî∂ÂàóÁöÑËØçËØ≠ÂèäÂÖ∂ÈáäÊñá„ÄÇ
ËØçÊù°,Ê†áÁ≠æ,ÊñáÂåñ
Á∫¢Ëâ≤È£üÂìÅ,ÊèèËø∞,Á∫¢Ëâ≤È£üÂìÅÊòØÊåáÈ£üÂìÅ‰∏∫Á∫¢Ëâ≤„ÄÅÊ©ôÁ∫¢Ëâ≤ÊàñÊ£ïÁ∫¢Ëâ≤ÁöÑÈ£üÂìÅ„ÄÇ
Á∫¢Ëâ≤È£üÂìÅ,‰∏≠ÊñáÂêç,Á∫¢Ëâ≤È£üÂìÅ
Á∫¢Ëâ≤È£üÂìÅ,ÊòØÂê¶Âê´Èò≤ËÖêÂâÇ,Âê¶
Á∫¢Ëâ≤È£üÂìÅ,‰∏ªË¶ÅÈ£üÁî®ÂäüÊïà,È¢ÑÈò≤ÊÑüÂÜíÔºåÁºìËß£Áñ≤Âä≥
Á∫¢Ëâ≤È£üÂìÅ,ÈÄÇÂÆú‰∫∫Áæ§,ÂÖ®ÈÉ®‰∫∫Áæ§
Á∫¢Ëâ≤È£üÂìÅ,Áî®ÈÄî,Â¢ûÂº∫Ë°®ÁöÆÁªÜËÉûÂÜçÁîüÂíåÈò≤Ê≠¢ÁöÆËÇ§Ë°∞ËÄÅ&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;‰ΩøÁî®pythonËøõË°åËØªÂèñÊµãËØïÔºö&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; sys
&lt;span class="pl-k"&gt;import&lt;/span&gt; csv

&lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ownthink_v2.csv&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;r&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;encoding&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;utf8&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; fin:
  reader &lt;span class="pl-k"&gt;=&lt;/span&gt; csv.reader(fin)
  &lt;span class="pl-k"&gt;for&lt;/span&gt; index, read &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;enumerate&lt;/span&gt;(reader):
    &lt;span class="pl-c1"&gt;print&lt;/span&gt;(read)
    
    &lt;span class="pl-k"&gt;if&lt;/span&gt; index &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;:
      sys.exit(&lt;span class="pl-c1"&gt;0&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;ËøêË°å‰ª•‰∏äËÑöÊú¨ËæìÂá∫ÁªìÊûúÔºö&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ÂÆû‰Ωì&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Â±ûÊÄß&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ÂÄº&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ËÉ∂È•¥&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ÊèèËø∞&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Âà´Âêç: È•¥Á≥ñ„ÄÅÁïÖÁ≥ñ„ÄÅÁïÖ„ÄÅËΩØÁ≥ñ„ÄÇ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ËØçÊù°&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ÊèèËø∞&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ËØçÊù°ÔºàÊãºÈü≥Ôºöc√≠ ti√°oÔºâ‰πüÂè´ËØçÁõÆÔºåÊòØËæû‰π¶Â≠¶Áî®ËØ≠ÔºåÊåáÊî∂ÂàóÁöÑËØçËØ≠ÂèäÂÖ∂ÈáäÊñá„ÄÇ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ËØçÊù°&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Ê†áÁ≠æ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ÊñáÂåñ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Á∫¢Ëâ≤È£üÂìÅ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ÊèèËø∞&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Á∫¢Ëâ≤È£üÂìÅÊòØÊåáÈ£üÂìÅ‰∏∫Á∫¢Ëâ≤„ÄÅÊ©ôÁ∫¢Ëâ≤ÊàñÊ£ïÁ∫¢Ëâ≤ÁöÑÈ£üÂìÅ„ÄÇ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Á∫¢Ëâ≤È£üÂìÅ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;‰∏≠ÊñáÂêç&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Á∫¢Ëâ≤È£üÂìÅ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Á∫¢Ëâ≤È£üÂìÅ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ÊòØÂê¶Âê´Èò≤ËÖêÂâÇ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Âê¶&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Á∫¢Ëâ≤È£üÂìÅ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;‰∏ªË¶ÅÈ£üÁî®ÂäüÊïà&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;È¢ÑÈò≤ÊÑüÂÜíÔºåÁºìËß£Áñ≤Âä≥&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Á∫¢Ëâ≤È£üÂìÅ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ÈÄÇÂÆú‰∫∫Áæ§&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ÂÖ®ÈÉ®‰∫∫Áæ§&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Á∫¢Ëâ≤È£üÂìÅ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Áî®ÈÄî&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Â¢ûÂº∫Ë°®ÁöÆÁªÜËÉûÂÜçÁîüÂíåÈò≤Ê≠¢ÁöÆËÇ§Ë°∞ËÄÅ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Á∫¢Ëâ≤È£üÂìÅ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Ê†áÁ≠æ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ÈùûÁßëÂ≠¶&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Á∫¢Ëâ≤È£üÂìÅ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Ê†áÁ≠æ&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ÁîüÊ¥ª&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Êï∞ÊçÆ‰∏ãËΩΩÊñπÂºèÔºö&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ÂÖ≥Ê≥®ÊÄùÁü•Êú∫Âô®‰∫∫ÂõûÂ§ç„ÄêÊï∞ÊçÆ‰∏ãËΩΩ„ÄëËé∑Âèñ‰∏ãËΩΩÈìæÊé•&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ÁôæÂ∫¶ÁΩëÁõòÔºàÈìæÊé•: &lt;a href="https://pan.baidu.com/s/1LZjs9Dsta0yD9NH-1y0sAw" rel="nofollow"&gt;https://pan.baidu.com/s/1LZjs9Dsta0yD9NH-1y0sAw&lt;/a&gt; ÊèêÂèñÁ†Å: 3hpp Ôºâ&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ê≥®ÔºöËß£ÂéãÂØÜÁ†ÅÊòØÔºö&lt;a href="https://www.ownthink.com/" rel="nofollow"&gt;https://www.ownthink.com/&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ownthink</author><guid isPermaLink="false">https://github.com/ownthink/KnowledgeGraphData</guid><pubDate>Tue, 19 Nov 2019 00:17:00 GMT</pubDate></item><item><title>cornellius-gp/gpytorch #18 in Python, This month</title><link>https://github.com/cornellius-gp/gpytorch</link><description>&lt;p&gt;&lt;i&gt;A highly efficient and modular implementation of Gaussian Processes in PyTorch&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-gpytorch-beta-release" class="anchor" aria-hidden="true" href="#gpytorch-beta-release"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GPyTorch (Beta Release)&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/cornellius-gp/gpytorch" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/086e31820ce909a5325c4529fb5f9ff12a2d4ae1/68747470733a2f2f7472617669732d63692e6f72672f636f726e656c6c6975732d67702f677079746f7263682e7376673f6272616e63683d6d6173746572" alt="Build status" data-canonical-src="https://travis-ci.org/cornellius-gp/gpytorch.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://gpytorch.readthedocs.io/en/latest/?badge=latest" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/0658d8fb97af292063f7b818705b541df283c035/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f677079746f7263682f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/gpytorch/badge/?version=latest" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://forthebadge.com" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/eae52829d6125ed81803704fdbcb158c1160e528/68747470733a2f2f666f7274686562616467652e636f6d2f696d616765732f6261646765732f616765732d31322e737667" alt="forthebadge" data-canonical-src="https://forthebadge.com/images/badges/ages-12.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;News!&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Beta release is currently out! Note that it &lt;strong&gt;requires PyTorch &amp;gt;= 1.3&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;If you need to install the alpha release (we recommend you use the latest version though!), check out &lt;a href="https://github.com/cornellius-gp/gpytorch/tree/alpha"&gt;the alpha release&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GPyTorch is a Gaussian process library implemented using PyTorch. GPyTorch is designed for creating scalable, flexible, and modular Gaussian process models with ease.&lt;/p&gt;
&lt;p&gt;Internally, GPyTorch differs from many existing approaches to GP inference by performing all inference operations using modern numerical linear algebra techniques like preconditioned conjugate gradients. Implementing a scalable GP method is as simple as providing a matrix multiplication routine with the kernel matrix and its derivative via our &lt;code&gt;LazyTensor&lt;/code&gt; interface, or by composing many of our already existing &lt;code&gt;LazyTensors&lt;/code&gt;. This allows not only for easy implementation of popular scalable GP techniques, but often also for significantly improved utilization of GPU computing compared to solvers based on the Cholesky decomposition.&lt;/p&gt;
&lt;p&gt;GPyTorch provides (1) significant GPU acceleration (through MVM based inference); (2) state-of-the-art implementations of the latest algorithmic advances for scalability and flexibility (&lt;a href="http://proceedings.mlr.press/v37/wilson15.pdf" rel="nofollow"&gt;SKI/KISS-GP&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1711.03481" rel="nofollow"&gt;stochastic Lanczos expansions&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/1803.06058.pdf" rel="nofollow"&gt;LOVE&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/1802.08903.pdf" rel="nofollow"&gt;SKIP&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/1611.00336.pdf" rel="nofollow"&gt;stochastic variational&lt;/a&gt; &lt;a href="http://proceedings.mlr.press/v51/wilson16.pdf" rel="nofollow"&gt;deep kernel learning&lt;/a&gt;, ...); (3) easy integration with deep learning frameworks.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-examples-and-tutorials" class="anchor" aria-hidden="true" href="#examples-and-tutorials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Examples and Tutorials&lt;/h2&gt;
&lt;p&gt;See our numerous &lt;a href="http://github.com/cornellius-gp/gpytorch/blob/master/examples"&gt;&lt;strong&gt;examples and tutorials&lt;/strong&gt;&lt;/a&gt; on how to construct all sorts of models in GPyTorch. These example notebooks and a walk through of GPyTorch are also available at our &lt;strong&gt;ReadTheDocs page &lt;a href="https://gpytorch.readthedocs.io/en/latest/index.html" rel="nofollow"&gt;here&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python &amp;gt;= 3.6&lt;/li&gt;
&lt;li&gt;PyTorch &amp;gt;= 1.3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;N.B.&lt;/strong&gt; GPyTorch will not run on PyTorch 0.4.1 or earlier versions.&lt;/p&gt;
&lt;p&gt;First make sure that you have PyTorch (`&amp;gt;= 1.3&lt;/p&gt;
&lt;p&gt;`) installed using the appropriate command from &lt;a href="https://pytorch.org/get-started/locally/" rel="nofollow"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then install GPyTorch using pip or conda:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install gpytorch
conda install gpytorch -c gpytorch&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To use packages globally but install GPyTorch as a user-only package, use &lt;code&gt;pip install --user&lt;/code&gt; above.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-latest-unstable-version" class="anchor" aria-hidden="true" href="#latest-unstable-version"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Latest (unstable) version&lt;/h4&gt;
&lt;p&gt;To get the latest (unstable) version, run&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install git+https://github.com/cornellius-gp/gpytorch.git&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-citing-us" class="anchor" aria-hidden="true" href="#citing-us"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing Us&lt;/h2&gt;
&lt;p&gt;If you use GPyTorch, please cite the following papers:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1809.11165" rel="nofollow"&gt;Gardner, Jacob R., Geoff Pleiss, David Bindel, Kilian Q. Weinberger, and Andrew Gordon Wilson. "GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration." In Advances in Neural Information Processing Systems (2018).&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;@inproceedings{gardner2018gpytorch,
  title={GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration},
  author={Gardner, Jacob R and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q and Wilson, Andrew Gordon},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-documentation" class="anchor" aria-hidden="true" href="#documentation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Documentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;For &lt;strong&gt;tutorials and examples&lt;/strong&gt;, check out &lt;a href="https://github.com/cornellius-gp/gpytorch/tree/master/examples"&gt;the examples folder&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;For in-depth &lt;strong&gt;documentation&lt;/strong&gt;, check out our &lt;a href="http://gpytorch.readthedocs.io/" rel="nofollow"&gt;read the docs&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-development" class="anchor" aria-hidden="true" href="#development"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Development&lt;/h2&gt;
&lt;p&gt;To run the unit tests:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m unittest&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By default, the random seeds are locked down for some of the tests.
If you want to run the tests without locking down the seed, run&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;UNLOCK_SEED=true python -m unittest&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Please lint the code with &lt;code&gt;flake8&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install flake8  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; if not already installed&lt;/span&gt;
flake8&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you plan on submitting a pull request, please make use of our pre-commit hooks to ensure that your commits adhere
to the general style guidelines enforced by the repo. To do this, navigate to your local repository and run:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install pre-commit
pre-commit install&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;From then on, this will automatically run flake8, isort, black and other tools over the files you commit each time you commit to gpytorch or a fork of it.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-team" class="anchor" aria-hidden="true" href="#the-team"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Team&lt;/h2&gt;
&lt;p&gt;GPyTorch is primarily maintained by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://github.com/jacobrgardner"&gt;Jake Gardner&lt;/a&gt; (Uber AI Labs)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://github.com/gpleiss"&gt;Geoff Pleiss&lt;/a&gt; (Cornell University)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://kilian.cs.cornell.edu/" rel="nofollow"&gt;Kilian Weinberger&lt;/a&gt; (Cornell University)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://people.orie.cornell.edu/andrew/" rel="nofollow"&gt;Andrew Gordon Wilson&lt;/a&gt; (Cornell University)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://research.fb.com/people/balandat-max/" rel="nofollow"&gt;Max Balandat&lt;/a&gt; (Facebook)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0c115437df284dad869f6254be0daf42d2d32d9a/68747470733a2f2f6272616e642e636f726e656c6c2e6564752f6173736574732f696d616765732f646f776e6c6f6164732f6c6f676f732f636f726e656c6c5f6c6f676f5f73696d706c652f636f726e656c6c5f6c6f676f5f73696d706c652e737667"&gt;&lt;img width="300" src="https://camo.githubusercontent.com/0c115437df284dad869f6254be0daf42d2d32d9a/68747470733a2f2f6272616e642e636f726e656c6c2e6564752f6173736574732f696d616765732f646f776e6c6f6164732f6c6f676f732f636f726e656c6c5f6c6f676f5f73696d706c652f636f726e656c6c5f6c6f676f5f73696d706c652e737667" alt="Cornell Logo" data-canonical-src="https://brand.cornell.edu/assets/images/downloads/logos/cornell_logo_simple/cornell_logo_simple.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/cornellius-gp/cornellius-gp.github.io/master/static/media/facebook_logo.2835357a.png"&gt;&lt;img width="300" src="https://raw.githubusercontent.com/cornellius-gp/cornellius-gp.github.io/master/static/media/facebook_logo.2835357a.png" alt="Facebook Logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/13804bb567d948c1bd440eed09f5ee28ff8cd6c9/68747470733a2f2f677079746f7263682e61692f7374617469632f6d656469612f756265725f61695f686f72697a6f6e74616c2e66653961623635332e706e67"&gt;&lt;img width="300" src="https://camo.githubusercontent.com/13804bb567d948c1bd440eed09f5ee28ff8cd6c9/68747470733a2f2f677079746f7263682e61692f7374617469632f6d656469612f756265725f61695f686f72697a6f6e74616c2e66653961623635332e706e67" alt="Uber AI Logo" data-canonical-src="https://gpytorch.ai/static/media/uber_ai_horizontal.fe9ab653.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
We would like to thank our other contributors including (but not limited to)  David Arbour, Eytan Bakshy, David Eriksson, Jared Frank, Sam Stanton, Bram Wallace, Ke Alexander Wang, Ruihan Wu.
&lt;h2&gt;&lt;a id="user-content-acknowledgements" class="anchor" aria-hidden="true" href="#acknowledgements"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Development of GPyTorch is supported by funding from the &lt;a href="https://www.gatesfoundation.org/" rel="nofollow"&gt;Bill and Melinda Gates Foundation&lt;/a&gt;, the &lt;a href="https://www.nsf.gov/" rel="nofollow"&gt;National Science Foundation&lt;/a&gt;, and &lt;a href="https://www.sap.com/index.html" rel="nofollow"&gt;SAP&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>cornellius-gp</author><guid isPermaLink="false">https://github.com/cornellius-gp/gpytorch</guid><pubDate>Tue, 19 Nov 2019 00:18:00 GMT</pubDate></item><item><title>eriklindernoren/PyTorch-GAN #19 in Python, This month</title><link>https://github.com/eriklindernoren/PyTorch-GAN</link><description>&lt;p&gt;&lt;i&gt;PyTorch implementations of Generative Adversarial Networks.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;&lt;a target="_blank" rel="noopener noreferrer" href="assets/logo.png"&gt;&lt;img src="assets/logo.png" width="480" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-pytorch-gan" class="anchor" aria-hidden="true" href="#pytorch-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch-GAN&lt;/h2&gt;
&lt;p&gt;Collection of PyTorch implementations of Generative Adversarial Network varieties presented in research papers. Model architectures will not always mirror the ones proposed in the papers, but I have chosen to focus on getting the core ideas covered instead of getting every layer configuration right. Contributions and suggestions of GANs to implement are very welcomed.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;See also:&lt;/b&gt; &lt;a href="https://github.com/eriklindernoren/Keras-GAN"&gt;Keras-GAN&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#implementations"&gt;Implementations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#auxiliary-classifier-gan"&gt;Auxiliary Classifier GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#adversarial-autoencoder"&gt;Adversarial Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#began"&gt;BEGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bicyclegan"&gt;BicycleGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#boundary-seeking-gan"&gt;Boundary-Seeking GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cluster-gan"&gt;Cluster GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#conditional-gan"&gt;Conditional GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#context-conditional-gan"&gt;Context-Conditional GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#context-encoder"&gt;Context Encoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#coupled-gan"&gt;Coupled GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cyclegan"&gt;CycleGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#deep-convolutional-gan"&gt;Deep Convolutional GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#discogan"&gt;DiscoGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dragan"&gt;DRAGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dualgan"&gt;DualGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#energy-based-gan"&gt;Energy-Based GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#enhanced-super-resolution-gan"&gt;Enhanced Super-Resolution GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#gan"&gt;GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#infogan"&gt;InfoGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#least-squares-gan"&gt;Least Squares GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#munit"&gt;MUNIT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pix2pix"&gt;Pix2Pix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#pixelda"&gt;PixelDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#relativistic-gan"&gt;Relativistic GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#semi-supervised-gan"&gt;Semi-Supervised GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#softmax-gan"&gt;Softmax GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#stargan"&gt;StarGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#super-resolution-gan"&gt;Super-Resolution GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#unit"&gt;UNIT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#wasserstein-gan"&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#wasserstein-gan-gp"&gt;Wasserstein GAN GP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#wasserstein-gan-div"&gt;Wasserstein GAN DIV&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/eriklindernoren/PyTorch-GAN
$ cd PyTorch-GAN/
$ sudo pip3 install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;a id="user-content-implementations" class="anchor" aria-hidden="true" href="#implementations"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Implementations&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-auxiliary-classifier-gan" class="anchor" aria-hidden="true" href="#auxiliary-classifier-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Auxiliary Classifier GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Auxiliary Classifier Generative Adversarial Network&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Augustus Odena, Christopher Olah, Jonathon Shlens&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract" class="anchor" aria-hidden="true" href="#abstract"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1610.09585" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/acgan/acgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example" class="anchor" aria-hidden="true" href="#run-example"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/acgan/
$ python3 acgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/acgan.gif"&gt;&lt;img src="assets/acgan.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-adversarial-autoencoder" class="anchor" aria-hidden="true" href="#adversarial-autoencoder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Adversarial Autoencoder&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Adversarial Autoencoder&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-1" class="anchor" aria-hidden="true" href="#authors-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-1" class="anchor" aria-hidden="true" href="#abstract-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;n this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1511.05644" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/aae/aae.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-1" class="anchor" aria-hidden="true" href="#run-example-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/aae/
$ python3 aae.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-began" class="anchor" aria-hidden="true" href="#began"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BEGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;BEGAN: Boundary Equilibrium Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-2" class="anchor" aria-hidden="true" href="#authors-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;David Berthelot, Thomas Schumm, Luke Metz&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-2" class="anchor" aria-hidden="true" href="#abstract-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1703.10717" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/began/began.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-2" class="anchor" aria-hidden="true" href="#run-example-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/began/
$ python3 began.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-bicyclegan" class="anchor" aria-hidden="true" href="#bicyclegan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;BicycleGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Toward Multimodal Image-to-Image Translation&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-3" class="anchor" aria-hidden="true" href="#authors-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-3" class="anchor" aria-hidden="true" href="#abstract-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1711.11586" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/bicyclegan/bicyclegan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/bicyclegan_architecture.jpg"&gt;&lt;img src="assets/bicyclegan_architecture.jpg" width="800" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-3" class="anchor" aria-hidden="true" href="#run-example-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_pix2pix_dataset.sh edges2shoes
$ cd ../implementations/bicyclegan/
$ python3 bicyclegan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/bicyclegan.png"&gt;&lt;img src="assets/bicyclegan.png" width="480" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Various style translations by varying the latent code.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-boundary-seeking-gan" class="anchor" aria-hidden="true" href="#boundary-seeking-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Boundary-Seeking GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Boundary-Seeking Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-4" class="anchor" aria-hidden="true" href="#authors-4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;R Devon Hjelm, Athul Paul Jacob, Tong Che, Adam Trischler, Kyunghyun Cho, Yoshua Bengio&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-4" class="anchor" aria-hidden="true" href="#abstract-4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Generative adversarial networks (GANs) are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training, and we demonstrate this on Celeba, Large-scale Scene Understanding (LSUN) bedrooms, and Imagenet without conditioning.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1702.08431" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/bgan/bgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-4" class="anchor" aria-hidden="true" href="#run-example-4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/bgan/
$ python3 bgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-cluster-gan" class="anchor" aria-hidden="true" href="#cluster-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Cluster GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;ClusterGAN: Latent Space Clustering in Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-5" class="anchor" aria-hidden="true" href="#authors-5"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Sudipto Mukherjee, Himanshu Asnani, Eugene Lin, Sreeram Kannan&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-5" class="anchor" aria-hidden="true" href="#abstract-5"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Generative Adversarial networks (GANs) have obtained remarkable success in many unsupervised learning tasks and
unarguably, clustering is an important unsupervised learning problem. While one can potentially exploit the
latent-space back-projection in GANs to cluster, we demonstrate that the cluster structure is not retained in the
GAN latent space.  In this paper, we propose ClusterGAN as a new mechanism for clustering using GANs. By sampling
latent variables from a mixture of one-hot encoded variables and continuous latent variables, coupled with an
inverse network (which projects the data to the latent space) trained jointly with a clustering specific loss, we
are able to achieve clustering in the latent space. Our results show a remarkable phenomenon that GANs can preserve
latent space interpolation across categories, even though the discriminator is never exposed to such vectors. We
compare our results with various clustering baselines and demonstrate superior performance on both synthetic and
real datasets.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1809.03627" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/cluster_gan/clustergan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code based on a full PyTorch &lt;a href="https://github.com/zhampel/clusterGAN"&gt;[implementation]&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-5" class="anchor" aria-hidden="true" href="#run-example-5"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/cluster_gan/
$ python3 clustergan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/cluster_gan.gif"&gt;&lt;img src="assets/cluster_gan.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-conditional-gan" class="anchor" aria-hidden="true" href="#conditional-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Conditional GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Conditional Generative Adversarial Nets&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-6" class="anchor" aria-hidden="true" href="#authors-6"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Mehdi Mirza, Simon Osindero&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-6" class="anchor" aria-hidden="true" href="#abstract-6"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1411.1784" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/cgan/cgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-6" class="anchor" aria-hidden="true" href="#run-example-6"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/cgan/
$ python3 cgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/cgan.gif"&gt;&lt;img src="assets/cgan.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-context-conditional-gan" class="anchor" aria-hidden="true" href="#context-conditional-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Context-Conditional GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-7" class="anchor" aria-hidden="true" href="#authors-7"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Emily Denton, Sam Gross, Rob Fergus&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-7" class="anchor" aria-hidden="true" href="#abstract-7"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1611.06430" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/ccgan/ccgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-7" class="anchor" aria-hidden="true" href="#run-example-7"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/ccgan/
$ python3 ccgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-context-encoder" class="anchor" aria-hidden="true" href="#context-encoder"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Context Encoder&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Context Encoders: Feature Learning by Inpainting&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-8" class="anchor" aria-hidden="true" href="#authors-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-8" class="anchor" aria-hidden="true" href="#abstract-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1604.07379" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/context_encoder/context_encoder.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-8" class="anchor" aria-hidden="true" href="#run-example-8"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/context_encoder/
&amp;lt;follow steps at the top of context_encoder.py&amp;gt;
$ python3 context_encoder.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/context_encoder.png"&gt;&lt;img src="assets/context_encoder.png" width="640" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Rows: Masked | Inpainted | Original | Masked | Inpainted | Original
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-coupled-gan" class="anchor" aria-hidden="true" href="#coupled-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Coupled GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Coupled Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-9" class="anchor" aria-hidden="true" href="#authors-9"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Ming-Yu Liu, Oncel Tuzel&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-9" class="anchor" aria-hidden="true" href="#abstract-9"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1606.07536" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/cogan/cogan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-9" class="anchor" aria-hidden="true" href="#run-example-9"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/cogan/
$ python3 cogan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/cogan.gif"&gt;&lt;img src="assets/cogan.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Generated MNIST and MNIST-M images
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-cyclegan" class="anchor" aria-hidden="true" href="#cyclegan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;CycleGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-10" class="anchor" aria-hidden="true" href="#authors-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-10" class="anchor" aria-hidden="true" href="#abstract-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G:X‚ÜíY such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F:Y‚ÜíX and introduce a cycle consistency loss to push F(G(X))‚âàX (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1703.10593" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/cyclegan/cyclegan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/c653ddc55471557b851a7059540e80799fad7e29/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6379636c6567616e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/c653ddc55471557b851a7059540e80799fad7e29/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f6379636c6567616e2e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/cyclegan.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-10" class="anchor" aria-hidden="true" href="#run-example-10"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_cyclegan_dataset.sh monet2photo
$ cd ../implementations/cyclegan/
$ python3 cyclegan.py --dataset_name monet2photo
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/cyclegan.png"&gt;&lt;img src="assets/cyclegan.png" width="900" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Monet to photo translations.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-deep-convolutional-gan" class="anchor" aria-hidden="true" href="#deep-convolutional-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deep Convolutional GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Deep Convolutional Generative Adversarial Network&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-11" class="anchor" aria-hidden="true" href="#authors-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Alec Radford, Luke Metz, Soumith Chintala&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-11" class="anchor" aria-hidden="true" href="#abstract-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1511.06434" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/dcgan/dcgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-11" class="anchor" aria-hidden="true" href="#run-example-11"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/dcgan/
$ python3 dcgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/dcgan.gif"&gt;&lt;img src="assets/dcgan.gif" width="240" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-discogan" class="anchor" aria-hidden="true" href="#discogan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DiscoGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Learning to Discover Cross-Domain Relations with Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-12" class="anchor" aria-hidden="true" href="#authors-12"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, Jiwon Kim&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-12" class="anchor" aria-hidden="true" href="#abstract-12"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1703.05192" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/discogan/discogan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e004d49943b2b1264496d5db1a9bff807afa8a68/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f646973636f67616e5f6172636869746563747572652e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/e004d49943b2b1264496d5db1a9bff807afa8a68/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f646973636f67616e5f6172636869746563747572652e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/discogan_architecture.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-12" class="anchor" aria-hidden="true" href="#run-example-12"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_pix2pix_dataset.sh edges2shoes
$ cd ../implementations/discogan/
$ python3 discogan.py --dataset_name edges2shoes
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/discogan.png"&gt;&lt;img src="assets/discogan.png" width="480" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Rows from top to bottom: (1) Real image from domain A (2) Translated image from &lt;br&gt;
    domain A (3) Reconstructed image from domain A (4) Real image from domain B (5) &lt;br&gt;
    Translated image from domain B (6) Reconstructed image from domain B
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-dragan" class="anchor" aria-hidden="true" href="#dragan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DRAGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;On Convergence and Stability of GANs&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-13" class="anchor" aria-hidden="true" href="#authors-13"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Naveen Kodali, Jacob Abernethy, James Hays, Zsolt Kira&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-13" class="anchor" aria-hidden="true" href="#abstract-13"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We propose studying GAN training dynamics as regret minimization, which is in contrast to the popular view that there is consistent minimization of a divergence between real and generated distributions. We analyze the convergence of GAN training from this new point of view to understand why mode collapse happens. We hypothesize the existence of undesirable local equilibria in this non-convex game to be responsible for mode collapse. We observe that these local equilibria often exhibit sharp gradients of the discriminator function around some real data points. We demonstrate that these degenerate local equilibria can be avoided with a gradient penalty scheme called DRAGAN. We show that DRAGAN enables faster training, achieves improved stability with fewer mode collapses, and leads to generator networks with better modeling performance across a variety of architectures and objective functions.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1705.07215" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/dragan/dragan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-13" class="anchor" aria-hidden="true" href="#run-example-13"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/dragan/
$ python3 dragan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-dualgan" class="anchor" aria-hidden="true" href="#dualgan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DualGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;DualGAN: Unsupervised Dual Learning for Image-to-Image Translation&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-14" class="anchor" aria-hidden="true" href="#authors-14"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Zili Yi, Hao Zhang, Ping Tan, Minglun Gong&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-14" class="anchor" aria-hidden="true" href="#abstract-14"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1704.02510" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/dualgan/dualgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-14" class="anchor" aria-hidden="true" href="#run-example-14"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_pix2pix_dataset.sh facades
$ cd ../implementations/dualgan/
$ python3 dualgan.py --dataset_name facades
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-energy-based-gan" class="anchor" aria-hidden="true" href="#energy-based-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Energy-Based GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Energy-based Generative Adversarial Network&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-15" class="anchor" aria-hidden="true" href="#authors-15"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Junbo Zhao, Michael Mathieu, Yann LeCun&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-15" class="anchor" aria-hidden="true" href="#abstract-15"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We introduce the "Energy-based Generative Adversarial Network" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1609.03126" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/ebgan/ebgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-15" class="anchor" aria-hidden="true" href="#run-example-15"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/ebgan/
$ python3 ebgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-enhanced-super-resolution-gan" class="anchor" aria-hidden="true" href="#enhanced-super-resolution-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Enhanced Super-Resolution GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-16" class="anchor" aria-hidden="true" href="#authors-16"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-16" class="anchor" aria-hidden="true" href="#abstract-16"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at &lt;a href="https://github.com/xinntao/ESRGAN"&gt;this https URL&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1809.00219" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/esrgan/esrgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-16" class="anchor" aria-hidden="true" href="#run-example-16"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/esrgan/
&amp;lt;follow steps at the top of esrgan.py&amp;gt;
$ python3 esrgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/enhanced_superresgan.png"&gt;&lt;img src="assets/enhanced_superresgan.png" width="320" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Nearest Neighbor Upsampling | ESRGAN
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-gan" class="anchor" aria-hidden="true" href="#gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Generative Adversarial Network&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-17" class="anchor" aria-hidden="true" href="#authors-17"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-17" class="anchor" aria-hidden="true" href="#abstract-17"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1406.2661" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/gan/gan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-17" class="anchor" aria-hidden="true" href="#run-example-17"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/gan/
$ python3 gan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/gan.gif"&gt;&lt;img src="assets/gan.gif" width="240" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-infogan" class="anchor" aria-hidden="true" href="#infogan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;InfoGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-18" class="anchor" aria-hidden="true" href="#authors-18"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-18" class="anchor" aria-hidden="true" href="#abstract-18"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1606.03657" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/infogan/infogan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-18" class="anchor" aria-hidden="true" href="#run-example-18"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/infogan/
$ python3 infogan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/infogan.gif"&gt;&lt;img src="assets/infogan.gif" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Result of varying categorical latent variable by column.
&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/infogan.png"&gt;&lt;img src="assets/infogan.png" width="360" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Result of varying continuous latent variable by row.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-least-squares-gan" class="anchor" aria-hidden="true" href="#least-squares-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Least Squares GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Least Squares Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-19" class="anchor" aria-hidden="true" href="#authors-19"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, Stephen Paul Smolley&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-19" class="anchor" aria-hidden="true" href="#abstract-19"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson œá2 divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1611.04076" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/lsgan/lsgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-19" class="anchor" aria-hidden="true" href="#run-example-19"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/lsgan/
$ python3 lsgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-munit" class="anchor" aria-hidden="true" href="#munit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MUNIT&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Multimodal Unsupervised Image-to-Image Translation&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-20" class="anchor" aria-hidden="true" href="#authors-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-20" class="anchor" aria-hidden="true" href="#abstract-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at &lt;a href="https://github.com/nvlabs/MUNIT"&gt;this https URL&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1804.04732" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/munit/munit.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-20" class="anchor" aria-hidden="true" href="#run-example-20"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_pix2pix_dataset.sh edges2shoes
$ cd ../implementations/munit/
$ python3 munit.py --dataset_name edges2shoes
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/munit.png"&gt;&lt;img src="assets/munit.png" width="480" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Results by varying the style code.
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pix2pix" class="anchor" aria-hidden="true" href="#pix2pix"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pix2Pix&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Unpaired Image-to-Image Translation with Conditional Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-21" class="anchor" aria-hidden="true" href="#authors-21"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-21" class="anchor" aria-hidden="true" href="#abstract-21"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1611.07004" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/pix2pix/pix2pix.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/e8c023b62678aa244f1a474bf643c66c45ef0feb/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f706978327069785f6172636869746563747572652e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/e8c023b62678aa244f1a474bf643c66c45ef0feb/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f706978327069785f6172636869746563747572652e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/pix2pix_architecture.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-21" class="anchor" aria-hidden="true" href="#run-example-21"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_pix2pix_dataset.sh facades
$ cd ../implementations/pix2pix/
$ python3 pix2pix.py --dataset_name facades
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/pix2pix.png"&gt;&lt;img src="assets/pix2pix.png" width="480" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Rows from top to bottom: (1) The condition for the generator (2) Generated image &lt;br&gt;
    based of condition (3) The true corresponding image to the condition
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pixelda" class="anchor" aria-hidden="true" href="#pixelda"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PixelDA&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-22" class="anchor" aria-hidden="true" href="#authors-22"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-22" class="anchor" aria-hidden="true" href="#abstract-22"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images often fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that attempt to map representations between the two domains or learn to extract features that are domain-invariant. In this work, we present a new approach that learns, in an unsupervised manner, a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1612.05424" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/pixelda/pixelda.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-mnist-to-mnist-m-classification" class="anchor" aria-hidden="true" href="#mnist-to-mnist-m-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;MNIST to MNIST-M Classification&lt;/h4&gt;
&lt;p&gt;Trains a classifier on images that have been translated from the source domain (MNIST) to the target domain (MNIST-M) using the annotations of the source domain images. The classification network is trained jointly with the generator network to optimize the generator for both providing a proper domain translation and also for preserving the semantics of the source domain image. The classification network trained on translated images is compared to the naive solution of training a classifier on MNIST and evaluating it on MNIST-M. The naive model manages a 55% classification accuracy on MNIST-M while the one trained during domain adaptation achieves a 95% classification accuracy.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/pixelda/
$ python3 pixelda.py
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th align="center"&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Naive&lt;/td&gt;
&lt;td align="center"&gt;55%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PixelDA&lt;/td&gt;
&lt;td align="center"&gt;95%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/pixelda.png"&gt;&lt;img src="assets/pixelda.png" width="480" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Rows from top to bottom: (1) Real images from MNIST (2) Translated images from &lt;br&gt;
    MNIST to MNIST-M (3) Examples of images from MNIST-M
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-relativistic-gan" class="anchor" aria-hidden="true" href="#relativistic-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Relativistic GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;The relativistic discriminator: a key element missing from standard GAN&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-23" class="anchor" aria-hidden="true" href="#authors-23"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Alexia Jolicoeur-Martineau&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-23" class="anchor" aria-hidden="true" href="#abstract-23"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs.
We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function.
Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1807.00734" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/relativistic_gan/relativistic_gan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-22" class="anchor" aria-hidden="true" href="#run-example-22"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/relativistic_gan/
$ python3 relativistic_gan.py                 # Relativistic Standard GAN
$ python3 relativistic_gan.py --rel_avg_gan   # Relativistic Average GAN
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-semi-supervised-gan" class="anchor" aria-hidden="true" href="#semi-supervised-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Semi-Supervised GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Semi-Supervised Generative Adversarial Network&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-24" class="anchor" aria-hidden="true" href="#authors-24"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Augustus Odena&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-24" class="anchor" aria-hidden="true" href="#abstract-24"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels. We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes. At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G. We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1606.01583" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/sgan/sgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-23" class="anchor" aria-hidden="true" href="#run-example-23"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/sgan/
$ python3 sgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-softmax-gan" class="anchor" aria-hidden="true" href="#softmax-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Softmax GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Softmax GAN&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-25" class="anchor" aria-hidden="true" href="#authors-25"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Min Lin&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-25" class="anchor" aria-hidden="true" href="#abstract-25"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Softmax GAN is a novel variant of Generative Adversarial Network (GAN). The key idea of Softmax GAN is to replace the classification loss in the original GAN with a softmax cross-entropy loss in the sample space of one single batch. In the adversarial learning of N real training samples and M generated samples, the target of discriminator training is to distribute all the probability mass to the real samples, each with probability 1M, and distribute zero probability to generated data. In the generator training phase, the target is to assign equal probability to all data points in the batch, each with probability 1M+N. While the original GAN is closely related to Noise Contrastive Estimation (NCE), we show that Softmax GAN is the Importance Sampling version of GAN. We futher demonstrate with experiments that this simple change stabilizes GAN training.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1704.06191" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/softmax_gan/softmax_gan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-24" class="anchor" aria-hidden="true" href="#run-example-24"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/softmax_gan/
$ python3 softmax_gan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-stargan" class="anchor" aria-hidden="true" href="#stargan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;StarGAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-26" class="anchor" aria-hidden="true" href="#authors-26"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-26" class="anchor" aria-hidden="true" href="#abstract-26"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1711.09020" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/stargan/stargan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-25" class="anchor" aria-hidden="true" href="#run-example-25"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/stargan/
&amp;lt;follow steps at the top of stargan.py&amp;gt;
$ python3 stargan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/stargan.png"&gt;&lt;img src="assets/stargan.png" width="640" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Original | Black Hair | Blonde Hair | Brown Hair | Gender Flip | Aged
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-super-resolution-gan" class="anchor" aria-hidden="true" href="#super-resolution-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Super-Resolution GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-27" class="anchor" aria-hidden="true" href="#authors-27"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-27" class="anchor" aria-hidden="true" href="#abstract-27"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1609.04802" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/srgan/srgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/07288b4b467fbf547c6757a448f8e786bf20f295/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f737570657272657367616e2e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/07288b4b467fbf547c6757a448f8e786bf20f295/687474703a2f2f6572696b6c696e6465726e6f72656e2e73652f696d616765732f737570657272657367616e2e706e67" width="640" data-canonical-src="http://eriklindernoren.se/images/superresgan.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-26" class="anchor" aria-hidden="true" href="#run-example-26"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/srgan/
&amp;lt;follow steps at the top of srgan.py&amp;gt;
$ python3 srgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/superresgan.png"&gt;&lt;img src="assets/superresgan.png" width="320" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align="center"&gt;
    Nearest Neighbor Upsampling | SRGAN
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-unit" class="anchor" aria-hidden="true" href="#unit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;UNIT&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Unsupervised Image-to-Image Translation Networks&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-28" class="anchor" aria-hidden="true" href="#authors-28"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Ming-Yu Liu, Thomas Breuel, Jan Kautz&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-28" class="anchor" aria-hidden="true" href="#abstract-28"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in this &lt;a href="https://github.com/mingyuliutw/unit"&gt;https URL&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1703.00848" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/unit/unit.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-27" class="anchor" aria-hidden="true" href="#run-example-27"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd data/
$ bash download_cyclegan_dataset.sh apple2orange
$ cd implementations/unit/
$ python3 unit.py --dataset_name apple2orange
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-wasserstein-gan" class="anchor" aria-hidden="true" href="#wasserstein-gan"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wasserstein GAN&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Wasserstein GAN&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-29" class="anchor" aria-hidden="true" href="#authors-29"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Martin Arjovsky, Soumith Chintala, L√©on Bottou&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-29" class="anchor" aria-hidden="true" href="#abstract-29"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1701.07875" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/wgan/wgan.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-28" class="anchor" aria-hidden="true" href="#run-example-28"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/wgan/
$ python3 wgan.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-wasserstein-gan-gp" class="anchor" aria-hidden="true" href="#wasserstein-gan-gp"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wasserstein GAN GP&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Improved Training of Wasserstein GANs&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-30" class="anchor" aria-hidden="true" href="#authors-30"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-30" class="anchor" aria-hidden="true" href="#abstract-30"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1704.00028" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/wgan_gp/wgan_gp.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-29" class="anchor" aria-hidden="true" href="#run-example-29"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/wgan_gp/
$ python3 wgan_gp.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/wgan_gp.gif"&gt;&lt;img src="assets/wgan_gp.gif" width="240" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-wasserstein-gan-div" class="anchor" aria-hidden="true" href="#wasserstein-gan-div"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wasserstein GAN DIV&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Wasserstein Divergence for GANs&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-authors-31" class="anchor" aria-hidden="true" href="#authors-31"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h4&gt;
&lt;p&gt;Jiqing Wu, Zhiwu Huang, Janine Thoma, Dinesh Acharya, Luc Van Gool&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-abstract-31" class="anchor" aria-hidden="true" href="#abstract-31"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h4&gt;
&lt;p&gt;In many domains of computer vision, generative adversarial networks (GANs) have achieved great success, among which the fam-
ily of Wasserstein GANs (WGANs) is considered to be state-of-the-art due to the theoretical contributions and competitive qualitative performance. However, it is very challenging to approximate the k-Lipschitz constraint required by the Wasserstein-1 metric (W-met). In this paper, we propose a novel Wasserstein divergence (W-div), which is a relaxed version of W-met and does not require the k-Lipschitz constraint.As a concrete application, we introduce a Wasserstein divergence objective for GANs (WGAN-div), which can faithfully approximate W-div through optimization. Under various settings, including progressive growing training, we demonstrate the stability of the proposed WGAN-div owing to its theoretical and practical advantages over WGANs. Also, we study the quantitative and visual performance of WGAN-div on standard image synthesis benchmarks, showing the superior performance of WGAN-div compared to the state-of-the-art methods.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1712.01026" rel="nofollow"&gt;[Paper]&lt;/a&gt; &lt;a href="implementations/wgan_div/wgan_div.py"&gt;[Code]&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-run-example-30" class="anchor" aria-hidden="true" href="#run-example-30"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Run Example&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;$ cd implementations/wgan_div/
$ python3 wgan_div.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p align="center"&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="assets/wgan_div.png"&gt;&lt;img src="assets/wgan_div.png" width="240" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>eriklindernoren</author><guid isPermaLink="false">https://github.com/eriklindernoren/PyTorch-GAN</guid><pubDate>Tue, 19 Nov 2019 00:19:00 GMT</pubDate></item><item><title>NVIDIA/DeepLearningExamples #20 in Python, This month</title><link>https://github.com/NVIDIA/DeepLearningExamples</link><description>&lt;p&gt;&lt;i&gt;Deep Learning Examples&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-nvidia-deep-learning-examples-for-tensor-cores" class="anchor" aria-hidden="true" href="#nvidia-deep-learning-examples-for-tensor-cores"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA Deep Learning Examples for Tensor Cores&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This repository provides the latest deep learning example networks for training.  These examples focus on achieving the best performance and convergence from NVIDIA Volta Tensor Cores.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-nvidia-gpu-cloud-ngc-container-registry" class="anchor" aria-hidden="true" href="#nvidia-gpu-cloud-ngc-container-registry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA GPU Cloud (NGC) Container Registry&lt;/h2&gt;
&lt;p&gt;These examples, along with our NVIDIA deep learning software stack, are provided in a monthly updated Docker container on the NGC container registry (&lt;a href="https://ngc.nvidia.com" rel="nofollow"&gt;https://ngc.nvidia.com&lt;/a&gt;). These containers include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The latest NVIDIA examples from this repository&lt;/li&gt;
&lt;li&gt;The latest NVIDIA contributions shared upstream to the respective framework&lt;/li&gt;
&lt;li&gt;The latest NVIDIA Deep Learning software libraries, such as cuDNN, NCCL, cuBLAS, etc. which have all been through a rigorous monthly quality assurance process to ensure that they provide the best possible performance&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/dgx/index.html#nvidia-optimized-frameworks-release-notes" rel="nofollow"&gt;Monthly release notes&lt;/a&gt; for each of the NVIDIA optimized containers&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-directory-structure" class="anchor" aria-hidden="true" href="#directory-structure"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Directory structure&lt;/h2&gt;
&lt;p&gt;The examples are organized first by framework, such as TensorFlow, PyTorch, etc. and second by use case, such as computer vision, natural language processing, etc. We hope this structure enables you to quickly locate the example networks that best suit your needs. Here are the currently supported models:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-computer-vision" class="anchor" aria-hidden="true" href="#computer-vision"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Computer Vision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ResNet-50&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/MxNet/Classification/RN50v1.5"&gt;MXNet&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/RN50v1.5"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Classification/RN50v1.5"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSD&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Detection/SSD"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mask R-CNN&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Segmentation/MaskRCNN"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;U-Net(industrial)&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/UNet_Industrial"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;U-Net(medical)&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Segmentation/UNet_Medical"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-natural-language-processing" class="anchor" aria-hidden="true" href="#natural-language-processing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Natural Language Processing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GNMT&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Translation/GNMT"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Translation/GNMT"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Translation/Transformer"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BERT&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT"&gt;PyTorch&lt;/a&gt;][&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-recommender-systems" class="anchor" aria-hidden="true" href="#recommender-systems"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recommender Systems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NCF&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/NCF"&gt;PyTorch&lt;/a&gt;] [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/Recommendation/NCF"&gt;TensorFlow&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-text-to-speech" class="anchor" aria-hidden="true" href="#text-to-speech"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Text to Speech&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tacotron &amp;amp; WaveGlow&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/Tacotron2"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-speech-recognition" class="anchor" aria-hidden="true" href="#speech-recognition"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Speech Recognition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jasper&lt;/strong&gt; [&lt;a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechRecognition/Jasper"&gt;PyTorch&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-nvidia-support" class="anchor" aria-hidden="true" href="#nvidia-support"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NVIDIA support&lt;/h2&gt;
&lt;p&gt;In each of the network READMEs, we indicate the level of support that will be provided. The range is from ongoing updates and improvements to a point-in-time release for thought leadership.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-feedback--contributions" class="anchor" aria-hidden="true" href="#feedback--contributions"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Feedback / Contributions&lt;/h2&gt;
&lt;p&gt;We're posting these examples on GitHub to better support the community, facilitate feedback, as well as collect and implement contributions using GitHub Issues and pull requests. We welcome all contributions!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-known-issues" class="anchor" aria-hidden="true" href="#known-issues"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Known issues&lt;/h2&gt;
&lt;p&gt;In each of the network READMEs, we indicate any known issues and encourage the community to provide feedback.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>NVIDIA</author><guid isPermaLink="false">https://github.com/NVIDIA/DeepLearningExamples</guid><pubDate>Tue, 19 Nov 2019 00:20:00 GMT</pubDate></item><item><title>huggingface/transformers #21 in Python, This month</title><link>https://github.com/huggingface/transformers</link><description>&lt;p&gt;&lt;i&gt;ü§ó Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
    &lt;br&gt;
    &lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png"&gt;&lt;img src="https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png" width="400" style="max-width:100%;"&gt;&lt;/a&gt;
    &lt;br&gt;
&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;&lt;p align="center"&gt;
    &lt;a href="https://circleci.com/gh/huggingface/transformers" rel="nofollow"&gt;
        &lt;img alt="Build" src="https://camo.githubusercontent.com/045b8639882280ff5cd38c403499977386c25134/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f6275696c642f6769746875622f68756767696e67666163652f7472616e73666f726d6572732f6d6173746572" data-canonical-src="https://img.shields.io/circleci/build/github/huggingface/transformers/master" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://github.com/huggingface/transformers/blob/master/LICENSE"&gt;
        &lt;img alt="GitHub" src="https://camo.githubusercontent.com/440e73b137335cc0088bb06e6c90cc7b503b14a2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f68756767696e67666163652f7472616e73666f726d6572732e7376673f636f6c6f723d626c7565" data-canonical-src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://huggingface.co/transformers/index.html" rel="nofollow"&gt;
        &lt;img alt="Documentation" src="https://camo.githubusercontent.com/b104c21f478c4d4a37f63292ab2898047f19ee24/68747470733a2f2f696d672e736869656c64732e696f2f776562736974652f687474702f68756767696e67666163652e636f2f7472616e73666f726d6572732f696e6465782e68746d6c2e7376673f646f776e5f636f6c6f723d72656426646f776e5f6d6573736167653d6f66666c696e652675705f6d6573736167653d6f6e6c696e65" data-canonical-src="https://img.shields.io/website/http/huggingface.co/transformers/index.html.svg?down_color=red&amp;amp;down_message=offline&amp;amp;up_message=online" style="max-width:100%;"&gt;
    &lt;/a&gt;
    &lt;a href="https://github.com/huggingface/transformers/releases"&gt;
        &lt;img alt="GitHub release" src="https://camo.githubusercontent.com/8409fd8716dd1a11afa7ab38e1218b34918164eb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f68756767696e67666163652f7472616e73666f726d6572732e737667" data-canonical-src="https://img.shields.io/github/release/huggingface/transformers.svg" style="max-width:100%;"&gt;
    &lt;/a&gt;
&lt;/p&gt;
&lt;h3 align="center"&gt;&lt;a id="user-content-state-of-the-art-natural-language-processing-for-tensorflow-20-and-pytorch" class="anchor" aria-hidden="true" href="#state-of-the-art-natural-language-processing-for-tensorflow-20-and-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;
&lt;p&gt;State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch
&lt;/p&gt;&lt;/h3&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;ü§ó&lt;/g-emoji&gt; Transformers (formerly known as &lt;code&gt;pytorch-transformers&lt;/code&gt; and &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt;) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-features" class="anchor" aria-hidden="true" href="#features"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;As easy to use as pytorch-transformers&lt;/li&gt;
&lt;li&gt;As powerful and concise as Keras&lt;/li&gt;
&lt;li&gt;High performance on NLU and NLG tasks&lt;/li&gt;
&lt;li&gt;Low barrier to entry for educators and practitioners&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;State-of-the-art NLP for everyone&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep learning researchers&lt;/li&gt;
&lt;li&gt;Hands-on practitioners&lt;/li&gt;
&lt;li&gt;AI/ML/NLP teachers and educators&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lower compute costs, smaller carbon footprint&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Researchers can share trained models instead of always retraining&lt;/li&gt;
&lt;li&gt;Practitioners can reduce compute time and production costs&lt;/li&gt;
&lt;li&gt;10 architectures with over 30 pretrained models, some in more than 100 languages&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Choose the right framework for every part of a model's lifetime&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train state-of-the-art models in 3 lines of code&lt;/li&gt;
&lt;li&gt;Deep interoperability between TensorFlow 2.0 and PyTorch models&lt;/li&gt;
&lt;li&gt;Move a single model between TF2.0/PyTorch frameworks at will&lt;/li&gt;
&lt;li&gt;Seamlessly pick the right framework for training, evaluation, production&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Section&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#installation"&gt;Installation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;How to install the package&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#model-architectures"&gt;Model architectures&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Architectures (with pretrained weights)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#online-demo"&gt;Online demo&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Experimenting with this repo‚Äôs text generation capabilities&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#quick-tour"&gt;Quick tour: Usage&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Tokenizers &amp;amp; models usage: Bert and GPT-2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Quick-tour-TF-20-training-and-PyTorch-interoperability"&gt;Quick tour: TF 2.0 and PyTorch &lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Train a TF 2.0 model in 10 lines of code, load it in PyTorch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#quick-tour-of-the-fine-tuningusage-scripts"&gt;Quick tour: Fine-tuning/usage scripts&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Using provided scripts: GLUE, SQuAD and Text generation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Migrating-from-pytorch-transformers-to-transformers"&gt;Migrating from pytorch-transformers to transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Migrating your code from pytorch-transformers to transformers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="#Migrating-from-pytorch-pretrained-bert-to-transformers"&gt;Migrating from pytorch-pretrained-bert to pytorch-transformers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Migrating your code from pytorch-pretrained-bert to transformers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;Documentation&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v2.1.1" rel="nofollow"&gt;(v2.1.1)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v2.0.0" rel="nofollow"&gt;(v2.0.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.2.0" rel="nofollow"&gt;(v1.2.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.1.0" rel="nofollow"&gt;(v1.1.0)&lt;/a&gt; &lt;a href="https://huggingface.co/transformers/v1.0.0" rel="nofollow"&gt;(v1.0.0)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Full API documentation and more&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;p&gt;This repo is tested on Python 2.7 and 3.5+ (examples are tested only on python 3.5+), PyTorch 1.0.0+ and TensorFlow 2.0.0-rc1&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-with-pip" class="anchor" aria-hidden="true" href="#with-pip"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;With pip&lt;/h3&gt;
&lt;p&gt;First you need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to &lt;a href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available" rel="nofollow"&gt;TensorFlow installation page&lt;/a&gt; and/or &lt;a href="https://pytorch.org/get-started/locally/#start-locally" rel="nofollow"&gt;PyTorch installation page&lt;/a&gt; regarding the specific install command for your platform.&lt;/p&gt;
&lt;p&gt;When TensorFlow 2.0 and/or PyTorch has been installed, &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;ü§ó&lt;/g-emoji&gt; Transformers can be installed using pip as follows:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install transformers&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-from-source" class="anchor" aria-hidden="true" href="#from-source"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;From source&lt;/h3&gt;
&lt;p&gt;Here also, you first need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to &lt;a href="https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available" rel="nofollow"&gt;TensorFlow installation page&lt;/a&gt; and/or &lt;a href="https://pytorch.org/get-started/locally/#start-locally" rel="nofollow"&gt;PyTorch installation page&lt;/a&gt; regarding the specific install command for your platform.&lt;/p&gt;
&lt;p&gt;When TensorFlow 2.0 and/or PyTorch has been installed, you can install from source by cloning the repository and running:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install [--editable] &lt;span class="pl-c1"&gt;.&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-tests" class="anchor" aria-hidden="true" href="#tests"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Tests&lt;/h3&gt;
&lt;p&gt;A series of tests are included for the library and the example scripts. Library tests can be found in the &lt;a href="https://github.com/huggingface/transformers/tree/master/transformers/tests"&gt;tests folder&lt;/a&gt; and examples tests in the &lt;a href="https://github.com/huggingface/transformers/tree/master/examples"&gt;examples folder&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These tests can be run using &lt;code&gt;pytest&lt;/code&gt; (install pytest if needed with &lt;code&gt;pip install pytest&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Depending on which framework is installed (TensorFlow 2.0 and/or PyTorch), the irrelevant tests will be skipped. Ensure that both frameworks are installed if you want to execute all tests.&lt;/p&gt;
&lt;p&gt;You can run the tests from the root of the cloned repository with the commands:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m pytest -sv ./transformers/tests/
python -m pytest -sv ./examples/&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-do-you-want-to-run-a-transformer-model-on-a-mobile-device" class="anchor" aria-hidden="true" href="#do-you-want-to-run-a-transformer-model-on-a-mobile-device"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Do you want to run a Transformer model on a mobile device?&lt;/h3&gt;
&lt;p&gt;You should check out our &lt;a href="https://github.com/huggingface/swift-coreml-transformers"&gt;&lt;code&gt;swift-coreml-transformers&lt;/code&gt;&lt;/a&gt; repo.&lt;/p&gt;
&lt;p&gt;It contains a set of tools to convert PyTorch or TensorFlow 2.0 trained Transformer models (currently contains &lt;code&gt;GPT-2&lt;/code&gt;, &lt;code&gt;DistilGPT-2&lt;/code&gt;, &lt;code&gt;BERT&lt;/code&gt;, and &lt;code&gt;DistilBERT&lt;/code&gt;) to CoreML models that run on iOS devices.&lt;/p&gt;
&lt;p&gt;At some point in the future, you'll be able to seamlessly move from pre-training or fine-tuning models to productizing them in CoreML, or prototype a model or an app in CoreML then research its hyperparameters or architecture from TensorFlow 2.0 and/or PyTorch. Super exciting!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-architectures" class="anchor" aria-hidden="true" href="#model-architectures"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model architectures&lt;/h2&gt;
&lt;p&gt;&lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;ü§ó&lt;/g-emoji&gt; Transformers currently provides 10 NLU/NLG architectures:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/google-research/bert"&gt;BERT&lt;/a&gt;&lt;/strong&gt; (from Google) released with the paper &lt;a href="https://arxiv.org/abs/1810.04805" rel="nofollow"&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt; by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/openai/finetune-transformer-lm"&gt;GPT&lt;/a&gt;&lt;/strong&gt; (from OpenAI) released with the paper &lt;a href="https://blog.openai.com/language-unsupervised/" rel="nofollow"&gt;Improving Language Understanding by Generative Pre-Training&lt;/a&gt; by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;GPT-2&lt;/a&gt;&lt;/strong&gt; (from OpenAI) released with the paper &lt;a href="https://blog.openai.com/better-language-models/" rel="nofollow"&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt; by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/kimiyoung/transformer-xl"&gt;Transformer-XL&lt;/a&gt;&lt;/strong&gt; (from Google/CMU) released with the paper &lt;a href="https://arxiv.org/abs/1901.02860" rel="nofollow"&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/a&gt; by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/zihangdai/xlnet/"&gt;XLNet&lt;/a&gt;&lt;/strong&gt; (from Google/CMU) released with the paper &lt;a href="https://arxiv.org/abs/1906.08237" rel="nofollow"&gt;‚ÄãXLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;/a&gt; by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/facebookresearch/XLM/"&gt;XLM&lt;/a&gt;&lt;/strong&gt; (from Facebook) released together with the paper &lt;a href="https://arxiv.org/abs/1901.07291" rel="nofollow"&gt;Cross-lingual Language Model Pretraining&lt;/a&gt; by Guillaume Lample and Alexis Conneau.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta"&gt;RoBERTa&lt;/a&gt;&lt;/strong&gt; (from Facebook), released together with the paper a &lt;a href="https://arxiv.org/abs/1907.11692" rel="nofollow"&gt;Robustly Optimized BERT Pretraining Approach&lt;/a&gt; by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/huggingface/transformers/tree/master/examples/distillation"&gt;DistilBERT&lt;/a&gt;&lt;/strong&gt; (from HuggingFace), released together with the paper &lt;a href="https://arxiv.org/abs/1910.01108" rel="nofollow"&gt;DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter&lt;/a&gt; by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into &lt;a href="https://github.com/huggingface/transformers/tree/master/examples/distillation"&gt;DistilGPT2&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/salesforce/ctrl/"&gt;CTRL&lt;/a&gt;&lt;/strong&gt; (from Salesforce) released with the paper &lt;a href="https://arxiv.org/abs/1909.05858" rel="nofollow"&gt;CTRL: A Conditional Transformer Language Model for Controllable Generation&lt;/a&gt; by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://camembert-model.fr" rel="nofollow"&gt;CamemBERT&lt;/a&gt;&lt;/strong&gt; (from Inria/Facebook/Sorbonne) released with the paper &lt;a href="https://arxiv.org/abs/1911.03894" rel="nofollow"&gt;CamemBERT: a Tasty French Language Model&lt;/a&gt; by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su√°rez*, Yoann Dupont, Laurent Romary, √âric Villemonte de la Clergerie, Djam√© Seddah and Beno√Æt Sagot.&lt;/li&gt;
&lt;li&gt;Want to contribute a new model? We have added a &lt;strong&gt;detailed guide and templates&lt;/strong&gt; to guide you in the process of adding a new model. You can find them in the &lt;a href="./templates"&gt;&lt;code&gt;templates&lt;/code&gt;&lt;/a&gt; folder of the repository. Be sure to check the &lt;a href="./CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; and contact the maintainers or open an issue to collect feedbacks before starting your PR.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the &lt;a href="https://huggingface.co/transformers/examples.html" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-online-demo" class="anchor" aria-hidden="true" href="#online-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Online demo&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://transformer.huggingface.co" rel="nofollow"&gt;Write With Transformer&lt;/a&gt;&lt;/strong&gt;, built by the Hugging Face team at transformer.huggingface.co, is the official demo of this repo‚Äôs text generation capabilities.
You can use it to experiment with completions generated by &lt;code&gt;GPT2Model&lt;/code&gt;, &lt;code&gt;TransfoXLModel&lt;/code&gt;, and &lt;code&gt;XLNetModel&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚Äú&lt;g-emoji class="g-emoji" alias="unicorn" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f984.png"&gt;ü¶Ñ&lt;/g-emoji&gt; Write with transformer is to writing what calculators are to calculus.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/ba91bf4a35939363eca4ca83f3ad3f83248bbc60/68747470733a2f2f7472616e73666f726d65722e68756767696e67666163652e636f2f66726f6e742f6173736574732f7468756d626e61696c2d6c617267652e706e67"&gt;&lt;img src="https://camo.githubusercontent.com/ba91bf4a35939363eca4ca83f3ad3f83248bbc60/68747470733a2f2f7472616e73666f726d65722e68756767696e67666163652e636f2f66726f6e742f6173736574732f7468756d626e61696c2d6c617267652e706e67" alt="write_with_transformer" data-canonical-src="https://transformer.huggingface.co/front/assets/thumbnail-large.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour" class="anchor" aria-hidden="true" href="#quick-tour"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour&lt;/h2&gt;
&lt;p&gt;Let's do a very quick overview of the model architectures in &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;ü§ó&lt;/g-emoji&gt; Transformers. Detailed examples for each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the &lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;full documentation&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; torch
&lt;span class="pl-k"&gt;from&lt;/span&gt; transformers &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Transformers has a unified API&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; for 8 transformer architectures and 30 pretrained weights.&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;          Model          | Tokenizer          | Pretrained weights shortcut&lt;/span&gt;
&lt;span class="pl-c1"&gt;MODELS&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; [(BertModel,       BertTokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (OpenAIGPTModel,  OpenAIGPTTokenizer,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;openai-gpt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (GPT2Model,       GPT2Tokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;gpt2&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (CTRLModel,       CTRLTokenizer,       &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;ctrl&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (TransfoXLModel,  TransfoXLTokenizer,  &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;transfo-xl-wt103&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (XLNetModel,      XLNetTokenizer,      &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xlnet-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (XLMModel,        XLMTokenizer,        &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;xlm-mlm-enfr-1024&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (DistilBertModel, DistilBertTokenizer, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;distilbert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;),
          (RobertaModel,    RobertaTokenizer,    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;roberta-base&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Let's encode some text in a sequence of hidden-states using each model:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; model_class, tokenizer_class, pretrained_weights &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;MODELS&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load pretrained model/tokenizer&lt;/span&gt;
    tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer_class.from_pretrained(pretrained_weights)
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Encode text&lt;/span&gt;
    input_ids &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.tensor([tokenizer.encode(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Here is some text to encode&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)])  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Add special tokens takes care of adding [CLS], [SEP], &amp;lt;s&amp;gt;... tokens in the right way for each model.&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; torch.no_grad():
        last_hidden_states &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids)[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models outputs are now tuples&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.&lt;/span&gt;
&lt;span class="pl-c1"&gt;BERT_MODEL_CLASSES&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,
                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; All the classes for an architecture can be initiated from pretrained weights for this architecture&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Note that additional weights added for fine-tuning are only initialized&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; and need to be trained on the down-stream task&lt;/span&gt;
pretrained_weights &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(pretrained_weights)
&lt;span class="pl-k"&gt;for&lt;/span&gt; model_class &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;BERT_MODEL_CLASSES&lt;/span&gt;:
    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load pretrained model/tokenizer&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights)

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models can return full list of hidden-states &amp;amp; attentions weights at each layer&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights,
                                        &lt;span class="pl-v"&gt;output_hidden_states&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;,
                                        &lt;span class="pl-v"&gt;output_attentions&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
    input_ids &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.tensor([tokenizer.encode(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;Let's see all hidden-states and attentions on this text&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)])
    all_hidden_states, all_attentions &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids)[&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;:]

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Models are compatible with Torchscript&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(pretrained_weights, &lt;span class="pl-v"&gt;torchscript&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
    traced_model &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.jit.trace(model, (input_ids,))

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Simple serialization for models and tokenizers&lt;/span&gt;
    model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; save&lt;/span&gt;
    model &lt;span class="pl-k"&gt;=&lt;/span&gt; model_class.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; re-load&lt;/span&gt;
    tokenizer.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; save&lt;/span&gt;
    tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./directory/to/save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; re-load&lt;/span&gt;

    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; SOTA examples for GLUE, SQUAD, text generation...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour-tf-20-training-and-pytorch-interoperability" class="anchor" aria-hidden="true" href="#quick-tour-tf-20-training-and-pytorch-interoperability"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour TF 2.0 training and PyTorch interoperability&lt;/h2&gt;
&lt;p&gt;Let's do a quick example of how a TensorFlow 2.0 model can be trained in 12 lines of code with &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;ü§ó&lt;/g-emoji&gt; Transformers and then loaded in PyTorch for fast inspection/tests.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow &lt;span class="pl-k"&gt;as&lt;/span&gt; tf
&lt;span class="pl-k"&gt;import&lt;/span&gt; tensorflow_datasets
&lt;span class="pl-k"&gt;from&lt;/span&gt; transformers &lt;span class="pl-k"&gt;import&lt;/span&gt; &lt;span class="pl-k"&gt;*&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load dataset, tokenizer, model from pretrained model/vocabulary&lt;/span&gt;
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
model &lt;span class="pl-k"&gt;=&lt;/span&gt; TFBertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-cased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
data &lt;span class="pl-k"&gt;=&lt;/span&gt; tensorflow_datasets.load(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;glue/mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Prepare dataset for GLUE as a tf.data.Dataset instance&lt;/span&gt;
train_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; glue_convert_examples_to_features(data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;train&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], tokenizer, &lt;span class="pl-v"&gt;max_length&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-v"&gt;task&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
valid_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; glue_convert_examples_to_features(data[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;validation&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], tokenizer, &lt;span class="pl-v"&gt;max_length&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;128&lt;/span&gt;, &lt;span class="pl-v"&gt;task&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;mrpc&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
train_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; train_dataset.shuffle(&lt;span class="pl-c1"&gt;100&lt;/span&gt;).batch(&lt;span class="pl-c1"&gt;32&lt;/span&gt;).repeat(&lt;span class="pl-c1"&gt;2&lt;/span&gt;)
valid_dataset &lt;span class="pl-k"&gt;=&lt;/span&gt; valid_dataset.batch(&lt;span class="pl-c1"&gt;64&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule &lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.optimizers.Adam(&lt;span class="pl-v"&gt;learning_rate&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;3e-5&lt;/span&gt;, &lt;span class="pl-v"&gt;epsilon&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1e-08&lt;/span&gt;, &lt;span class="pl-v"&gt;clipnorm&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;1.0&lt;/span&gt;)
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.losses.SparseCategoricalCrossentropy(&lt;span class="pl-v"&gt;from_logits&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
metric &lt;span class="pl-k"&gt;=&lt;/span&gt; tf.keras.metrics.SparseCategoricalAccuracy(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;accuracy&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
model.compile(&lt;span class="pl-v"&gt;optimizer&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;optimizer, &lt;span class="pl-v"&gt;loss&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;loss, &lt;span class="pl-v"&gt;metrics&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;[metric])

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train and evaluate using tf.keras.Model.fit()&lt;/span&gt;
history &lt;span class="pl-k"&gt;=&lt;/span&gt; model.fit(train_dataset, &lt;span class="pl-v"&gt;epochs&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-v"&gt;steps_per_epoch&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;115&lt;/span&gt;,
                    &lt;span class="pl-v"&gt;validation_data&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;valid_dataset, &lt;span class="pl-v"&gt;validation_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;7&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Load the TensorFlow model in PyTorch for inspection&lt;/span&gt;
model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
pytorch_model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./save/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;from_tf&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task&lt;/span&gt;
sentence_0 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;This research was consistent with his findings.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
sentence_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;His findings were compatible with this research.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
sentence_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;His findings were not compatible with this research.&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;
inputs_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer.encode_plus(sentence_0, sentence_1, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;return_tensors&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
inputs_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; tokenizer.encode_plus(sentence_0, sentence_2, &lt;span class="pl-v"&gt;add_special_tokens&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-v"&gt;return_tensors&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;pt&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

pred_1 &lt;span class="pl-k"&gt;=&lt;/span&gt; pytorch_model(inputs_1[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;token_type_ids&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;inputs_1[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;token_type_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].argmax().item()
pred_2 &lt;span class="pl-k"&gt;=&lt;/span&gt; pytorch_model(inputs_2[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;input_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;], &lt;span class="pl-v"&gt;token_type_ids&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;inputs_2[&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;token_type_ids&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])[&lt;span class="pl-c1"&gt;0&lt;/span&gt;].argmax().item()

&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sentence_1 is&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; pred_1 &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;not a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;of sentence_0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)
&lt;span class="pl-c1"&gt;print&lt;/span&gt;(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;sentence_2 is&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt; &lt;span class="pl-k"&gt;if&lt;/span&gt; pred_2 &lt;span class="pl-k"&gt;else&lt;/span&gt; &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;not a paraphrase&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;of sentence_0&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-quick-tour-of-the-fine-tuningusage-scripts" class="anchor" aria-hidden="true" href="#quick-tour-of-the-fine-tuningusage-scripts"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Quick tour of the fine-tuning/usage scripts&lt;/h2&gt;
&lt;p&gt;The library comprises several example scripts with SOTA performances for NLU and NLG tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;run_glue.py&lt;/code&gt;: an example fine-tuning Bert, XLNet and XLM on nine different GLUE tasks (&lt;em&gt;sequence-level classification&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run_squad.py&lt;/code&gt;: an example fine-tuning Bert, XLNet and XLM on the question answering dataset SQuAD 2.0 (&lt;em&gt;token-level classification&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run_generation.py&lt;/code&gt;: an example using GPT, GPT-2, CTRL, Transformer-XL and XLNet for conditional language generation&lt;/li&gt;
&lt;li&gt;other model-specific examples (see the documentation).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are three quick usage examples for these scripts:&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run_gluepy-fine-tuning-on-glue-tasks-for-sequence-classification" class="anchor" aria-hidden="true" href="#run_gluepy-fine-tuning-on-glue-tasks-for-sequence-classification"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_glue.py&lt;/code&gt;: Fine-tuning on GLUE tasks for sequence classification&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://gluebenchmark.com/" rel="nofollow"&gt;General Language Understanding Evaluation (GLUE) benchmark&lt;/a&gt; is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.&lt;/p&gt;
&lt;p&gt;Before running anyone of these GLUE tasks you should download the
&lt;a href="https://gluebenchmark.com/tasks" rel="nofollow"&gt;GLUE data&lt;/a&gt; by running
&lt;a href="https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"&gt;this script&lt;/a&gt;
and unpack it to some directory &lt;code&gt;$GLUE_DIR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You should also install the additional packages required by the examples:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;pip install -r ./examples/requirements.txt&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue
&lt;span class="pl-k"&gt;export&lt;/span&gt; TASK_NAME=MRPC

python ./examples/run_glue.py \
    --model_type bert \
    --model_name_or_path bert-base-uncased \
    --task_name &lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt; \
    --do_train \
    --do_eval \
    --do_lower_case \
    --data_dir &lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/&lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt; \
    --max_seq_length 128 \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --output_dir /tmp/&lt;span class="pl-smi"&gt;$TASK_NAME&lt;/span&gt;/&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.&lt;/p&gt;
&lt;p&gt;The dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called '/tmp/MNLI-MM/' in addition to '/tmp/MNLI/'.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-fine-tuning-xlnet-model-on-the-sts-b-regression-task" class="anchor" aria-hidden="true" href="#fine-tuning-xlnet-model-on-the-sts-b-regression-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning XLNet model on the STS-B regression task&lt;/h4&gt;
&lt;p&gt;This example code fine-tunes XLNet on the STS-B corpus using parallel training on a server with 4 V100 GPUs.
Parallel training is a simple way to use several GPUs (but is slower and less flexible than distributed training, see below).&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;export&lt;/span&gt; GLUE_DIR=/path/to/glue

python ./examples/run_glue.py \
    --model_type xlnet \
    --model_name_or_path xlnet-large-cased \
    --do_train  \
    --do_eval   \
    --task_name=sts-b     \
    --data_dir=&lt;span class="pl-smi"&gt;${GLUE_DIR}&lt;/span&gt;/STS-B  \
    --output_dir=./proc_data/sts-b-110   \
    --max_seq_length=128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --gradient_accumulation_steps=1 \
    --max_steps=1200  \
    --model_name=xlnet-large-cased   \
    --overwrite_output_dir   \
    --overwrite_cache \
    --warmup_steps=120&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On this machine we thus have a batch size of 32, please increase &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; to reach the same batch size if you have a smaller machine. These hyper-parameters should result in a Pearson correlation coefficient of &lt;code&gt;+0.917&lt;/code&gt; on the development set.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-fine-tuning-bert-model-on-the-mrpc-classification-task" class="anchor" aria-hidden="true" href="#fine-tuning-bert-model-on-the-mrpc-classification-task"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Fine-tuning Bert model on the MRPC classification task&lt;/h4&gt;
&lt;p&gt;This example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 &amp;gt; 92.&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m torch.distributed.launch --nproc_per_node 8 ./examples/run_glue.py   \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --task_name MRPC \
    --do_train   \
    --do_eval   \
    --do_lower_case   \
    --data_dir &lt;span class="pl-smi"&gt;$GLUE_DIR&lt;/span&gt;/MRPC/   \
    --max_seq_length 128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5   \
    --num_train_epochs 3.0  \
    --output_dir /tmp/mrpc_output/ \
    --overwrite_output_dir   \
    --overwrite_cache \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Training with these hyper-parameters gave us the following results:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;  acc = 0.8823529411764706
  acc_and_f1 = 0.901702786377709
  eval_loss = 0.3418912578906332
  f1 = 0.9210526315789473
  global_step = 174
  loss = 0.07231863956341798&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-run_squadpy-fine-tuning-on-squad-for-question-answering" class="anchor" aria-hidden="true" href="#run_squadpy-fine-tuning-on-squad-for-question-answering"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_squad.py&lt;/code&gt;: Fine-tuning on SQuAD for question-answering&lt;/h3&gt;
&lt;p&gt;This example code fine-tunes BERT on the SQuAD dataset using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 &amp;gt; 93 on SQuAD:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python -m torch.distributed.launch --nproc_per_node=8 ./examples/run_squad.py \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --do_train \
    --do_eval \
    --do_lower_case \
    --train_file &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/train-v1.1.json \
    --predict_file &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json \
    --learning_rate 3e-5 \
    --num_train_epochs 2 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir ../models/wwm_uncased_finetuned_squad/ \
    --per_gpu_eval_batch_size=3   \
    --per_gpu_train_batch_size=3   \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Training with these hyper-parameters gave us the following results:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/evaluate-v1.1.py &lt;span class="pl-smi"&gt;$SQUAD_DIR&lt;/span&gt;/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json
{&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;exact_match&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 86.91579943235573, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;f1&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;: 93.1532499015869}&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the model provided as &lt;code&gt;bert-large-uncased-whole-word-masking-finetuned-squad&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-run_generationpy-text-generation-with-gpt-gpt-2-ctrl-transformer-xl-and-xlnet" class="anchor" aria-hidden="true" href="#run_generationpy-text-generation-with-gpt-gpt-2-ctrl-transformer-xl-and-xlnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;code&gt;run_generation.py&lt;/code&gt;: Text generation with GPT, GPT-2, CTRL, Transformer-XL and XLNet&lt;/h3&gt;
&lt;p&gt;A conditional generation script is also included to generate text from a prompt.
The generation script includes the &lt;a href="https://github.com/rusiaaman/XLNet-gen#methodology"&gt;tricks&lt;/a&gt; proposed by Aman Rusia to get high-quality generation with memory models like Transformer-XL and XLNet (include a predefined text to make short inputs longer).&lt;/p&gt;
&lt;p&gt;Here is how to run the script with the small version of OpenAI GPT-2 model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python ./examples/run_generation.py \
    --model_type=gpt2 \
    --length=20 \
    --model_name_or_path=gpt2 \&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and from the Salesforce CTRL model:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;python ./examples/run_generation.py \
    --model_type=ctrl \
    --length=20 \
    --model_name_or_path=ctrl \
    --temperature=0 \
    --repetition_penalty=1.2 \&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-migrating-from-pytorch-transformers-to-transformers" class="anchor" aria-hidden="true" href="#migrating-from-pytorch-transformers-to-transformers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Migrating from pytorch-transformers to transformers&lt;/h2&gt;
&lt;p&gt;Here is a quick summary of what you should take care of when migrating from &lt;code&gt;pytorch-transformers&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-positional-order-of-some-models-keywords-inputs-attention_mask-token_type_ids-changed" class="anchor" aria-hidden="true" href="#positional-order-of-some-models-keywords-inputs-attention_mask-token_type_ids-changed"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Positional order of some models' keywords inputs (&lt;code&gt;attention_mask&lt;/code&gt;, &lt;code&gt;token_type_ids&lt;/code&gt;...) changed&lt;/h3&gt;
&lt;p&gt;To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models &lt;strong&gt;keywords inputs&lt;/strong&gt; (&lt;code&gt;attention_mask&lt;/code&gt;, &lt;code&gt;token_type_ids&lt;/code&gt;...) has been changed.&lt;/p&gt;
&lt;p&gt;If you used to call the models with keyword names for keyword arguments, e.g. &lt;code&gt;model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)&lt;/code&gt;, this should not cause any change.&lt;/p&gt;
&lt;p&gt;If you used to call the models with positional inputs for keyword arguments, e.g. &lt;code&gt;model(inputs_ids, attention_mask, token_type_ids)&lt;/code&gt;, you may have to double check the exact order of input arguments.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-migrating-from-pytorch-pretrained-bert-to-transformers" class="anchor" aria-hidden="true" href="#migrating-from-pytorch-pretrained-bert-to-transformers"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Migrating from pytorch-pretrained-bert to transformers&lt;/h2&gt;
&lt;p&gt;Here is a quick summary of what you should take care of when migrating from &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-models-always-output-tuples" class="anchor" aria-hidden="true" href="#models-always-output-tuples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Models always output &lt;code&gt;tuples&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The main breaking change when migrating from &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt; is that every model's forward method always outputs a &lt;code&gt;tuple&lt;/code&gt; with various elements depending on the model and the configuration parameters.&lt;/p&gt;
&lt;p&gt;The exact content of the tuples for each model is detailed in the models' docstrings and the &lt;a href="https://huggingface.co/transformers/" rel="nofollow"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is a &lt;code&gt;pytorch-pretrained-bert&lt;/code&gt; to &lt;code&gt;transformers&lt;/code&gt; conversion example for a &lt;code&gt;BertForSequenceClassification&lt;/code&gt; classification model:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Let's load our model&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; If you used to have this line in pytorch-pretrained-bert:&lt;/span&gt;
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Now just use this line in transformers to extract the loss from the output tuple:&lt;/span&gt;
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)
loss &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs[&lt;span class="pl-c1"&gt;0&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; In transformers you can also have access to the logits:&lt;/span&gt;
loss, logits &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs[:&lt;span class="pl-c1"&gt;2&lt;/span&gt;]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;output_attentions&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;True&lt;/span&gt;)
outputs &lt;span class="pl-k"&gt;=&lt;/span&gt; model(input_ids, &lt;span class="pl-v"&gt;labels&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;labels)
loss, logits, attentions &lt;span class="pl-k"&gt;=&lt;/span&gt; outputs&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-using-hidden-states" class="anchor" aria-hidden="true" href="#using-hidden-states"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Using hidden states&lt;/h3&gt;
&lt;p&gt;By enabling the configuration option &lt;code&gt;output_hidden_states&lt;/code&gt;, it was possible to retrieve the last hidden states of the encoder. In &lt;code&gt;pytorch-transformers&lt;/code&gt; as well as &lt;code&gt;transformers&lt;/code&gt; the return value has changed slightly: &lt;code&gt;all_hidden_states&lt;/code&gt; now also includes the hidden state of the embeddings in addition to those of the encoding layers. This allows users to easily access the embeddings final state.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-serialization" class="anchor" aria-hidden="true" href="#serialization"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Serialization&lt;/h3&gt;
&lt;p&gt;Breaking change in the &lt;code&gt;from_pretrained()&lt;/code&gt; method:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Models are now set in evaluation mode by default when instantiated with the &lt;code&gt;from_pretrained()&lt;/code&gt; method. To train them, don't forget to set them back in training mode (&lt;code&gt;model.train()&lt;/code&gt;) to activate the dropout modules.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The additional &lt;code&gt;*input&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt; arguments supplied to the &lt;code&gt;from_pretrained()&lt;/code&gt; method used to be directly passed to the underlying model's class &lt;code&gt;__init__()&lt;/code&gt; method. They are now used to update the model configuration attribute instead, which can break derived model classes built based on the previous &lt;code&gt;BertForSequenceClassification&lt;/code&gt; examples. We are working on a way to mitigate this breaking change in &lt;a href="https://github.com/huggingface/transformers/pull/866"&gt;#866&lt;/a&gt; by forwarding the the model's &lt;code&gt;__init__()&lt;/code&gt; method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method &lt;code&gt;save_pretrained(save_directory)&lt;/code&gt; if you were using any other serialization method before.&lt;/p&gt;
&lt;p&gt;Here is an example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Let's load a model and tokenizer&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;bert-base-uncased&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Do some stuff to our model and tokenizer&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Ex: add new tokens to the vocabulary and embeddings of our model&lt;/span&gt;
tokenizer.add_tokens([&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;[SPECIAL_TOKEN_1]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;[SPECIAL_TOKEN_2]&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;])
model.resize_token_embeddings(&lt;span class="pl-c1"&gt;len&lt;/span&gt;(tokenizer))
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Train our model&lt;/span&gt;
train(model)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Now let's save our model and tokenizer to a directory&lt;/span&gt;
model.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer.save_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Reload the model and the tokenizer&lt;/span&gt;
model &lt;span class="pl-k"&gt;=&lt;/span&gt; BertForSequenceClassification.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)
tokenizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertTokenizer.from_pretrained(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;./my_saved_model_directory/&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-optimizers-bertadam--openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules" class="anchor" aria-hidden="true" href="#optimizers-bertadam--openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Optimizers: BertAdam &amp;amp; OpenAIAdam are now AdamW, schedules are standard PyTorch schedules&lt;/h3&gt;
&lt;p&gt;The two optimizers previously included, &lt;code&gt;BertAdam&lt;/code&gt; and &lt;code&gt;OpenAIAdam&lt;/code&gt;, have been replaced by a single &lt;code&gt;AdamW&lt;/code&gt; optimizer which has a few differences:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it only implements weights decay correction,&lt;/li&gt;
&lt;li&gt;schedules are now externals (see below),&lt;/li&gt;
&lt;li&gt;gradient clipping is now also external (see below).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The new optimizer &lt;code&gt;AdamW&lt;/code&gt; matches PyTorch &lt;code&gt;Adam&lt;/code&gt; optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping.&lt;/p&gt;
&lt;p&gt;The schedules are now standard &lt;a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate" rel="nofollow"&gt;PyTorch learning rate schedulers&lt;/a&gt; and not part of the optimizer anymore.&lt;/p&gt;
&lt;p&gt;Here is a conversion examples from &lt;code&gt;BertAdam&lt;/code&gt; with a linear warmup and decay schedule to &lt;code&gt;AdamW&lt;/code&gt; and the same schedule:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Parameters:&lt;/span&gt;
lr &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1e-3&lt;/span&gt;
max_grad_norm &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1.0&lt;/span&gt;
num_training_steps &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;1000&lt;/span&gt;
num_warmup_steps &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt;
warmup_proportion &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;float&lt;/span&gt;(num_warmup_steps) &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;float&lt;/span&gt;(num_training_steps)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; 0.1&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## Previously BertAdam optimizer was instantiated like this:&lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; BertAdam(model.parameters(), &lt;span class="pl-v"&gt;lr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;lr, &lt;span class="pl-v"&gt;schedule&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;warmup_linear&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-v"&gt;warmup&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;warmup_proportion, &lt;span class="pl-v"&gt;t_total&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_training_steps)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## and used like this:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; batch &lt;span class="pl-k"&gt;in&lt;/span&gt; train_data:
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(batch)
    loss.backward()
    optimizer.step()

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## In Transformers, optimizer and schedules are splitted and instantiated like this:&lt;/span&gt;
optimizer &lt;span class="pl-k"&gt;=&lt;/span&gt; AdamW(model.parameters(), &lt;span class="pl-v"&gt;lr&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;lr, &lt;span class="pl-v"&gt;correct_bias&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;&lt;span class="pl-c1"&gt;False&lt;/span&gt;)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; To reproduce BertAdam specific behavior set correct_bias=False&lt;/span&gt;
scheduler &lt;span class="pl-k"&gt;=&lt;/span&gt; get_linear_schedule_with_warmup(optimizer, &lt;span class="pl-v"&gt;num_warmup_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_warmup_steps, &lt;span class="pl-v"&gt;num_training_steps&lt;/span&gt;&lt;span class="pl-k"&gt;=&lt;/span&gt;num_training_steps)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; PyTorch scheduler&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt;## and used like this:&lt;/span&gt;
&lt;span class="pl-k"&gt;for&lt;/span&gt; batch &lt;span class="pl-k"&gt;in&lt;/span&gt; train_data:
    model.train()
    loss &lt;span class="pl-k"&gt;=&lt;/span&gt; model(batch)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; Gradient clipping is not in AdamW anymore (so you can use amp without issue)&lt;/span&gt;
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;We now have a paper you can cite for the &lt;g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png"&gt;ü§ó&lt;/g-emoji&gt; Transformers library:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>huggingface</author><guid isPermaLink="false">https://github.com/huggingface/transformers</guid><pubDate>Tue, 19 Nov 2019 00:21:00 GMT</pubDate></item><item><title>DrDonk/unlocker #22 in Python, This month</title><link>https://github.com/DrDonk/unlocker</link><description>&lt;p&gt;&lt;i&gt;VMware Workstation macOS &lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body txt" data-path="readme.txt"&gt;&lt;div class="plain"&gt;&lt;pre style="white-space: pre-wrap"&gt;macOS Unlocker V3.0 for VMware Workstation
==========================================

+-----------------------------------------------------------------------------+
| IMPORTANT:                                                                  |
| ==========                                                                  |
|                                                                             |
| Always uninstall the previous version of the Unlocker before using a new    |
| version. Failure to do this could render VMware unusable.                   |
|                                                                             |
+-----------------------------------------------------------------------------+

1. Introduction
---------------

Unlocker 3 is designed for VMware Workstation 11-15 and Player 7-15.

If you are using an earlier product please continue using Unlocker 1.

Version 3 has been tested against:

* Workstation 11/12/14/15 on Windows and Linux
* Workstation Player 7/12/14/15 on Windows and Linux

The patch code carries out the following modifications dependent on the product
being patched:

* Fix vmware-vmx and derivatives to allow macOS to boot
* Fix vmwarebase .dll or .so to allow Apple to be selected during VM creation
* Download a copy of the latest VMware Tools for macOS

Note that not all products recognise the darwin.iso via install tools menu item.
You will have to manually mount the darwin.iso for example on Workstation 11 and Player 7.

In all cases make sure VMware is not running, and any background guests have
been shutdown.

The code is written in Python.

2. Prerequisites
----------------

The code requires Python 2.7 to work. Most Linux distros ship with a compatible
Python interpreter and should work without requiring any additional software.

Windows Unlocker has a packaged version of the Python script using PyInstaller, 
and so does not require Python to be installed.

3. Limitations
--------------

If you are using VMware Player or Workstation on Windows you may get a core dump.

Latest Linux products are OK and do not show this problem.

+-----------------------------------------------------------------------------+
| IMPORTANT:                                                                  |
| ==========                                                                  |
|                                                                             |
| If you create a new VM VMware may stop and create a core dump.              |
| There are two options to work around this issue:                            |
|                                                                             |
| 1. Change the VM to be HW 10 - this does not affect performance.            |
| 2. Edit the VMX file and add:                                               |
|    smc.version = "0"                                                        |
|                                                                             |
+-----------------------------------------------------------------------------+

4. Windows
----------
On Windows you will need to either run cmd.exe as Administrator or using
Explorer right click on the command file and select "Run as administrator".

win-install.cmd   - patches VMware
win-uninstall.cmd - restores VMware
win-update-tools.cmd - retrieves latest macOS guest tools

5. Linux
---------
On Linux you will need to be either root or use sudo to run the scripts.

You may need to ensure the Linux scripts have execute permissions
by running chmod +x against the 2 files.

lnx-install.sh   - patches VMware
lnx-uninstall.sh - restores VMware
lnx-update-tools.sh - retrieves latest macOS guest tools
   
6. Thanks
---------

Thanks to Zenith432 for originally building the C++ unlocker and Mac Son of Knife
(MSoK) for all the testing and support.

Thanks also to Sam B for finding the solution for ESXi 6 and helping me with
debugging expertise. Sam also wrote the code for patching ESXi ELF files and
modified the unlocker code to run on Python 3 in the ESXi 6.5 environment.


History
-------
27/09/18 3.0.0 - First release
02/10/18 3.0.1 - Fixed gettools.py to work with Python 3 and correctly download darwinPre15.iso
10/10/18 3.0.2 - Fixed false positives from anti-virus software with Windows executables
               - Allow Python 2 and 3 to run the Python code from Bash scripts


(c) 2011-2018 Dave Parsons&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</description><author>DrDonk</author><guid isPermaLink="false">https://github.com/DrDonk/unlocker</guid><pubDate>Tue, 19 Nov 2019 00:22:00 GMT</pubDate></item><item><title>getsentry/sentry #23 in Python, This month</title><link>https://github.com/getsentry/sentry</link><description>&lt;p&gt;&lt;i&gt;Sentry is cross-platform application monitoring, with a focus on error reporting.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body rst" data-path="README.rst"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
  &lt;/p&gt;&lt;p align="center"&gt;
    &lt;a href="https://sentry.io/?utm_source=github&amp;amp;utm_medium=logo" rel="nofollow"&gt;
      &lt;img src="https://camo.githubusercontent.com/2dfeafbee0904d6df16ddf7200993dace1629e60/68747470733a2f2f73656e7472792d6272616e642e73746f726167652e676f6f676c65617069732e636f6d2f73656e7472792d6c6f676f2d626c61636b2e706e67" alt="Sentry" height="72" data-canonical-src="https://sentry-brand.storage.googleapis.com/sentry-logo-black.png" style="max-width:100%;"&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p align="center"&gt;
    Users and logs provide clues. Sentry provides answers.
  &lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;a name="user-content-what-s-sentry"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-whats-sentry" class="anchor" aria-hidden="true" href="#whats-sentry"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What's Sentry?&lt;/h2&gt;
&lt;p&gt;Sentry fundamentally is a service that helps you monitor and fix crashes in realtime.
The server is in Python, but it contains a full API for sending events from any
language, in any application.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-1.png"&gt;&lt;img src="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-1.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-2.png"&gt;&lt;img src="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-2.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
  &lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-3.png"&gt;&lt;img src="https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/product/thumb-3.png" width="290" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;&lt;a name="user-content-official-sentry-sdks"&gt;&lt;/a&gt;
&lt;h3&gt;&lt;a id="user-content-official-sentry-sdks" class="anchor" aria-hidden="true" href="#official-sentry-sdks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Official Sentry SDKs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-javascript"&gt;JavaScript&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/react-native-sentry"&gt;React-Native&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-python"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/raven-ruby"&gt;Ruby&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-php"&gt;PHP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-go"&gt;Go&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-java"&gt;Java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-cocoa"&gt;Objective-C/Swift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-dotnet"&gt;C#&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/perl-raven"&gt;Perl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-elixir"&gt;Elixir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry-laravel"&gt;Laravel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;a name="user-content-resources"&gt;&lt;/a&gt;
&lt;h2&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.sentry.io/" rel="nofollow"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://forum.sentry.io/" rel="nofollow"&gt;Community&lt;/a&gt; (Bugs, feature requests, general questions)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.sentry.io/internal/contributing/" rel="nofollow"&gt;Contributing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry/issues"&gt;Bug Tracker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/getsentry/sentry"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://discord.gg/ez5KZN7" rel="nofollow"&gt;Discord&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.transifex.com/getsentry/sentry/" rel="nofollow"&gt;Transifex&lt;/a&gt; (Translate Sentry!)&lt;/li&gt;
&lt;/ul&gt;

&lt;/article&gt;&lt;/div&gt;</description><author>getsentry</author><guid isPermaLink="false">https://github.com/getsentry/sentry</guid><pubDate>Tue, 19 Nov 2019 00:23:00 GMT</pubDate></item><item><title>psf/black #24 in Python, This month</title><link>https://github.com/psf/black</link><description>&lt;p&gt;&lt;i&gt;The uncompromising Python code formatter&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/psf/black/master/docs/_static/logo2-readme.png"&gt;&lt;img src="https://raw.githubusercontent.com/psf/black/master/docs/_static/logo2-readme.png" alt="Black Logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 align="center"&gt;&lt;a id="user-content-the-uncompromising-code-formatter" class="anchor" aria-hidden="true" href="#the-uncompromising-code-formatter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The Uncompromising Code Formatter&lt;/h2&gt;
&lt;p align="center"&gt;
&lt;a href="https://travis-ci.com/psf/black" rel="nofollow"&gt;&lt;img alt="Build Status" src="https://camo.githubusercontent.com/0d62c6ce125db151bb0bc13cbc834c0d0522ed88/68747470733a2f2f7472617669732d63692e636f6d2f7073662f626c61636b2e7376673f6272616e63683d6d6173746572" data-canonical-src="https://travis-ci.com/psf/black.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://black.readthedocs.io/en/stable/?badge=stable" rel="nofollow"&gt;&lt;img alt="Documentation Status" src="https://camo.githubusercontent.com/ab197284ad0cd9cae552157b75f815b640eba827/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f626c61636b2f62616467652f3f76657273696f6e3d737461626c65" data-canonical-src="https://readthedocs.org/projects/black/badge/?version=stable" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://coveralls.io/github/psf/black?branch=master" rel="nofollow"&gt;&lt;img alt="Coverage Status" src="https://camo.githubusercontent.com/b77842b75f031fbf9bbf690eb55585ae55a8321d/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f7073662f626c61636b2f62616467652e7376673f6272616e63683d6d6173746572" data-canonical-src="https://coveralls.io/repos/github/psf/black/badge.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/black/blob/master/LICENSE"&gt;&lt;img alt="License: MIT" src="https://camo.githubusercontent.com/14a9abb7e83098f2949f26d2190e04fb1bd52c06/68747470733a2f2f626c61636b2e72656164746865646f63732e696f2f656e2f737461626c652f5f7374617469632f6c6963656e73652e737667" data-canonical-src="https://black.readthedocs.io/en/stable/_static/license.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pypi.org/project/black/" rel="nofollow"&gt;&lt;img alt="PyPI" src="https://camo.githubusercontent.com/5f6551edb3716d9b0a5659caf441d987bb1527a6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f626c61636b" data-canonical-src="https://img.shields.io/pypi/v/black" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://pepy.tech/project/black" rel="nofollow"&gt;&lt;img alt="Downloads" src="https://camo.githubusercontent.com/7b3026c6e1fecc9574cb4b93a3925e392bee087d/68747470733a2f2f706570792e746563682f62616467652f626c61636b" data-canonical-src="https://pepy.tech/badge/black" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://github.com/psf/black"&gt;&lt;img alt="Code style: black" src="https://camo.githubusercontent.com/28a51fe3a2c05048d8ca8ecd039d6b1619037326/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ÄúAny color you like.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is the uncompromising Python code formatter. By using it, you agree to cede
control over minutiae of hand-formatting. In return, &lt;em&gt;Black&lt;/em&gt; gives you speed,
determinism, and freedom from &lt;code&gt;pycodestyle&lt;/code&gt; nagging about formatting. You will save time
and mental energy for more important matters.&lt;/p&gt;
&lt;p&gt;Blackened code looks the same regardless of the project you're reading. Formatting
becomes transparent after a while and you can focus on the content instead.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; makes code review faster by producing the smallest diffs possible.&lt;/p&gt;
&lt;p&gt;Try it out now using the &lt;a href="https://black.now.sh" rel="nofollow"&gt;Black Playground&lt;/a&gt;. Watch the
&lt;a href="https://youtu.be/esZLCuWs_2Y" rel="nofollow"&gt;PyCon 2019 talk&lt;/a&gt; to learn more.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Contents:&lt;/em&gt; &lt;strong&gt;&lt;a href="#installation-and-usage"&gt;Installation and usage&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#the-black-code-style"&gt;Code style&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#pyprojecttoml"&gt;pyproject.toml&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#editor-integration"&gt;Editor integration&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#blackd"&gt;blackd&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#version-control-integration"&gt;Version control integration&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#ignoring-unmodified-files"&gt;Ignoring unmodified files&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#used-by"&gt;Used by&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#testimonials"&gt;Testimonials&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#show-your-style"&gt;Show your style&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#contributing-to-black"&gt;Contributing&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="#change-log"&gt;Change Log&lt;/a&gt;&lt;/strong&gt; |
&lt;strong&gt;&lt;a href="#authors"&gt;Authors&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;a id="user-content-installation-and-usage" class="anchor" aria-hidden="true" href="#installation-and-usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation and usage&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; can be installed by running &lt;code&gt;pip install black&lt;/code&gt;. It requires Python 3.6.0+ to
run but you can reformat Python 2 code with it, too.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-usage" class="anchor" aria-hidden="true" href="#usage"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h3&gt;
&lt;p&gt;To get started right away with sensible defaults:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;black {source_file_or_directory}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-command-line-options" class="anchor" aria-hidden="true" href="#command-line-options"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Command line options&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; doesn't provide many options. You can list them by running &lt;code&gt;black --help&lt;/code&gt;:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;black [OPTIONS] [SRC]...

Options:
  -c, --code TEXT                 Format the code passed in as a string.
  -l, --line-length INTEGER       How many characters per line to allow.
                                  [default: 88]
  -t, --target-version [py27|py33|py34|py35|py36|py37|py38]
                                  Python versions that should be supported by
                                  Black's output. [default: per-file auto-
                                  detection]
  --py36                          Allow using Python 3.6-only syntax on all
                                  input files.  This will put trailing commas
                                  in function signatures and calls also after
                                  *args and **kwargs. Deprecated; use
                                  --target-version instead. [default: per-file
                                  auto-detection]
  --pyi                           Format all input files like typing stubs
                                  regardless of file extension (useful when
                                  piping source on standard input).
  -S, --skip-string-normalization
                                  Don't normalize string quotes or prefixes.
  --check                         Don't write the files back, just return the
                                  status.  Return code 0 means nothing would
                                  change.  Return code 1 means some files
                                  would be reformatted.  Return code 123 means
                                  there was an internal error.
  --diff                          Don't write the files back, just output a
                                  diff for each file on stdout.
  --fast / --safe                 If --fast given, skip temporary sanity
                                  checks. [default: --safe]
  --include TEXT                  A regular expression that matches files and
                                  directories that should be included on
                                  recursive searches.  An empty value means
                                  all files are included regardless of the
                                  name.  Use forward slashes for directories
                                  on all platforms (Windows, too).  Exclusions
                                  are calculated first, inclusions later.
                                  [default: \.pyi?$]
  --exclude TEXT                  A regular expression that matches files and
                                  directories that should be excluded on
                                  recursive searches.  An empty value means no
                                  paths are excluded. Use forward slashes for
                                  directories on all platforms (Windows, too).
                                  Exclusions are calculated first, inclusions
                                  later.  [default: /(\.eggs|\.git|\.hg|\.mypy
                                  _cache|\.nox|\.tox|\.venv|_build|buck-
                                  out|build|dist)/]
  -q, --quiet                     Don't emit non-error messages to stderr.
                                  Errors are still emitted, silence those with
                                  2&amp;gt;/dev/null.
  -v, --verbose                   Also emit messages to stderr about files
                                  that were not changed or were ignored due to
                                  --exclude=.
  --version                       Show the version and exit.
  --config PATH                   Read configuration from PATH.
  -h, --help                      Show this message and exit.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is a well-behaved Unix-style command-line tool:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it does nothing if no sources are passed to it;&lt;/li&gt;
&lt;li&gt;it will read from standard input and write to standard output if &lt;code&gt;-&lt;/code&gt; is used as the
filename;&lt;/li&gt;
&lt;li&gt;it only outputs messages to users on standard error;&lt;/li&gt;
&lt;li&gt;exits with code 0 unless an internal error occurred (or &lt;code&gt;--check&lt;/code&gt; was used).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-note-this-is-a-beta-product" class="anchor" aria-hidden="true" href="#note-this-is-a-beta-product"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;NOTE: This is a beta product&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is already &lt;a href="#used-by"&gt;successfully used&lt;/a&gt; by many projects, small and big. It
also sports a decent test suite. However, it is still very new. Things will probably be
wonky for a while. This is made explicit by the "Beta" trove classifier, as well as by
the "b" in the version number. What this means for you is that &lt;strong&gt;until the formatter
becomes stable, you should expect some formatting to change in the future&lt;/strong&gt;. That being
said, no drastic stylistic changes are planned, mostly responses to bug reports.&lt;/p&gt;
&lt;p&gt;Also, as a temporary safety measure, &lt;em&gt;Black&lt;/em&gt; will check that the reformatted code still
produces a valid AST that is equivalent to the original. This slows it down. If you're
feeling confident, use &lt;code&gt;--fast&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-the-black-code-style" class="anchor" aria-hidden="true" href="#the-black-code-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;The &lt;em&gt;Black&lt;/em&gt; code style&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; reformats entire files in place. It is not configurable. It doesn't take
previous formatting into account. It doesn't reformat blocks that start with
&lt;code&gt;# fmt: off&lt;/code&gt; and end with &lt;code&gt;# fmt: on&lt;/code&gt;. &lt;code&gt;# fmt: on/off&lt;/code&gt; have to be on the same level of
indentation. It also recognizes &lt;a href="https://github.com/google/yapf"&gt;YAPF&lt;/a&gt;'s block comments
to the same effect, as a courtesy for straddling code.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-black-wraps-lines" class="anchor" aria-hidden="true" href="#how-black-wraps-lines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How &lt;em&gt;Black&lt;/em&gt; wraps lines&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; ignores previous formatting and applies uniform horizontal and vertical
whitespace to your code. The rules for horizontal whitespace can be summarized as: do
whatever makes &lt;code&gt;pycodestyle&lt;/code&gt; happy. The coding style used by &lt;em&gt;Black&lt;/em&gt; can be viewed as a
strict subset of PEP 8.&lt;/p&gt;
&lt;p&gt;As for vertical whitespace, &lt;em&gt;Black&lt;/em&gt; tries to render one full expression or simple
statement per line. If this fits the allotted line length, great.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

j &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;,
     &lt;span class="pl-c1"&gt;2&lt;/span&gt;,
     &lt;span class="pl-c1"&gt;3&lt;/span&gt;,
]

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

j &lt;span class="pl-k"&gt;=&lt;/span&gt; [&lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;, &lt;span class="pl-c1"&gt;3&lt;/span&gt;]&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If not, &lt;em&gt;Black&lt;/em&gt; will look at the contents of the first outer matching brackets and put
that in a separate indented line.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

ImportantClass.important_method(exc, limit, lookup_lines, capture_locals, extra_argument)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

ImportantClass.important_method(
    exc, limit, lookup_lines, capture_locals, extra_argument
)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If that still doesn't fit the bill, it will decompose the internal expression further
using the same rule, indenting matching brackets every time. If the contents of the
matching brackets pair are comma-separated (like an argument list, or a dict literal,
and so on) then &lt;em&gt;Black&lt;/em&gt; will first try to keep them on the same line with the matching
brackets. If that doesn't work, it will put all of them in separate lines.&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; in:&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;very_important_function&lt;/span&gt;(&lt;span class="pl-smi"&gt;template&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;, &lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;variables&lt;/span&gt;, &lt;span class="pl-smi"&gt;file&lt;/span&gt;: os.PathLike, &lt;span class="pl-smi"&gt;engine&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;, &lt;span class="pl-smi"&gt;header&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;, &lt;span class="pl-smi"&gt;debug&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;False&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Applies `variables` to the `template` and writes to `file`.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-v"&gt;file&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;w&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        &lt;span class="pl-c1"&gt;...&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;#&lt;/span&gt; out:&lt;/span&gt;

&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;very_important_function&lt;/span&gt;(
    &lt;span class="pl-smi"&gt;template&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;,
    &lt;span class="pl-k"&gt;*&lt;/span&gt;&lt;span class="pl-smi"&gt;variables&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;file&lt;/span&gt;: os.PathLike,
    &lt;span class="pl-smi"&gt;engine&lt;/span&gt;: &lt;span class="pl-c1"&gt;str&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;header&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;True&lt;/span&gt;,
    &lt;span class="pl-smi"&gt;debug&lt;/span&gt;: &lt;span class="pl-c1"&gt;bool&lt;/span&gt; &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;False&lt;/span&gt;,
):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"""&lt;/span&gt;Applies `variables` to the `template` and writes to `file`.&lt;span class="pl-pds"&gt;"""&lt;/span&gt;&lt;/span&gt;
    &lt;span class="pl-k"&gt;with&lt;/span&gt; &lt;span class="pl-c1"&gt;open&lt;/span&gt;(&lt;span class="pl-v"&gt;file&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;"&lt;/span&gt;w&lt;span class="pl-pds"&gt;"&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-k"&gt;as&lt;/span&gt; f:
        &lt;span class="pl-c1"&gt;...&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You might have noticed that closing brackets are always dedented and that a trailing
comma is always added. Such formatting produces smaller diffs; when you add or remove an
element, it's always just one line. Also, having the closing bracket dedented provides a
clear delimiter between two distinct sections of the code that otherwise share the same
indentation level (like the arguments list and the docstring in the example above).&lt;/p&gt;
&lt;p&gt;If a data structure literal (tuple, list, set, dict) or a line of "from" imports cannot
fit in the allotted length, it's always split into one element per line. This minimizes
diffs as well as enables readers of code to find which commit introduced a particular
entry. This also makes &lt;em&gt;Black&lt;/em&gt; compatible with &lt;a href="https://pypi.org/p/isort/" rel="nofollow"&gt;isort&lt;/a&gt; with
the following configuration.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;A compatible `.isort.cfg`&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;[settings]
multi_line_output=3
include_trailing_comma=True
force_grid_wrap=0
use_parentheses=True
line_length=88
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The equivalent command line is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ isort --multi-line=3 --trailing-comma --force-grid-wrap=0 --use-parentheses --line-width=88 [ file.py ]
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-line-length" class="anchor" aria-hidden="true" href="#line-length"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Line length&lt;/h3&gt;
&lt;p&gt;You probably noticed the peculiar default line length. &lt;em&gt;Black&lt;/em&gt; defaults to 88 characters
per line, which happens to be 10% over 80. This number was found to produce
significantly shorter files than sticking with 80 (the most popular), or even 79 (used
by the standard library). In general,
&lt;a href="https://youtu.be/wf-BqAjZb8M?t=260" rel="nofollow"&gt;90-ish seems like the wise choice&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you're paid by the line of code you write, you can pass &lt;code&gt;--line-length&lt;/code&gt; with a lower
number. &lt;em&gt;Black&lt;/em&gt; will try to respect that. However, sometimes it won't be able to without
breaking other rules. In those rare cases, auto-formatted code will exceed your allotted
limit.&lt;/p&gt;
&lt;p&gt;You can also increase it, but remember that people with sight disabilities find it
harder to work with line lengths exceeding 100 characters. It also adversely affects
side-by-side diff review on typical screen resolutions. Long lines also make it harder
to present code neatly in documentation or talk slides.&lt;/p&gt;
&lt;p&gt;If you're using Flake8, you can bump &lt;code&gt;max-line-length&lt;/code&gt; to 88 and forget about it.
Alternatively, use &lt;a href="https://github.com/PyCQA/flake8-bugbear"&gt;Bugbear&lt;/a&gt;'s B950 warning
instead of E501 and keep the max line length at 80 which you are probably already using.
You'd do it like this:&lt;/p&gt;
&lt;div class="highlight highlight-source-ini"&gt;&lt;pre&gt;&lt;span class="pl-en"&gt;[flake8]&lt;/span&gt;
&lt;span class="pl-k"&gt;max-line-length&lt;/span&gt; = 80
...
&lt;span class="pl-k"&gt;select&lt;/span&gt; = C,E,F,W,B,B950
&lt;span class="pl-k"&gt;ignore&lt;/span&gt; = E203, E501, W503&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You'll find &lt;em&gt;Black&lt;/em&gt;'s own .flake8 config file is configured like this. Explanation of
why W503 and E203 are disabled can be found further in this documentation. And if you're
curious about the reasoning behind B950,
&lt;a href="https://github.com/PyCQA/flake8-bugbear#opinionated-warnings"&gt;Bugbear's documentation&lt;/a&gt;
explains it. The tl;dr is "it's like highway speed limits, we won't bother you if you
overdo it by a few km/h".&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-empty-lines" class="anchor" aria-hidden="true" href="#empty-lines"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Empty lines&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; avoids spurious vertical whitespace. This is in the spirit of PEP 8 which says
that in-function vertical whitespace should only be used sparingly.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will allow single empty lines inside functions, and single and double empty
lines on module level left by the original editors, except when they're within
parenthesized expressions. Since such expressions are always reformatted to fit minimal
space, this whitespace is lost.&lt;/p&gt;
&lt;p&gt;It will also insert proper spacing before and after function definitions. It's one line
before and after inner functions and two lines before and after module-level functions
and classes. &lt;em&gt;Black&lt;/em&gt; will not put empty lines between function/class definitions and
standalone comments that immediately precede the given function/class.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will enforce single empty lines between a class-level docstring and the first
following field or method. This conforms to
&lt;a href="https://www.python.org/dev/peps/pep-0257/#multi-line-docstrings" rel="nofollow"&gt;PEP 257&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; won't insert empty lines after function docstrings unless that empty line is
required due to an inner function starting immediately after.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-trailing-commas" class="anchor" aria-hidden="true" href="#trailing-commas"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Trailing commas&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will add trailing commas to expressions that are split by comma where each
element is on its own line. This includes function signatures.&lt;/p&gt;
&lt;p&gt;Unnecessary trailing commas are removed if an expression fits in one line. This makes it
1% more likely that your line won't exceed the allotted line length limit. Moreover, in
this scenario, if you added another argument to your call, you'd probably fit it in the
same line anyway. That doesn't make diffs any larger.&lt;/p&gt;
&lt;p&gt;One exception to removing trailing commas is tuple expressions with just one element. In
this case &lt;em&gt;Black&lt;/em&gt; won't touch the single trailing comma as this would unexpectedly
change the underlying data type. Note that this is also the case when commas are used
while indexing. This is a tuple in disguise: &lt;code&gt;numpy_array[3, ]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One exception to adding trailing commas is function signatures containing &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;*args&lt;/code&gt;,
or &lt;code&gt;**kwargs&lt;/code&gt;. In this case a trailing comma is only safe to use on Python 3.6. &lt;em&gt;Black&lt;/em&gt;
will detect if your file is already 3.6+ only and use trailing commas in this situation.
If you wonder how it knows, it looks for f-strings and existing use of trailing commas
in function signatures that have stars in them. In other words, if you'd like a trailing
comma in this situation and &lt;em&gt;Black&lt;/em&gt; didn't recognize it was safe to do so, put it there
manually and &lt;em&gt;Black&lt;/em&gt; will keep it.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-strings" class="anchor" aria-hidden="true" href="#strings"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Strings&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; prefers double quotes (&lt;code&gt;"&lt;/code&gt; and &lt;code&gt;"""&lt;/code&gt;) over single quotes (&lt;code&gt;'&lt;/code&gt; and &lt;code&gt;'''&lt;/code&gt;). It
will replace the latter with the former as long as it does not result in more backslash
escapes than before.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; also standardizes string prefixes, making them always lowercase. On top of that,
if your code is already Python 3.6+ only or it's using the &lt;code&gt;unicode_literals&lt;/code&gt; future
import, &lt;em&gt;Black&lt;/em&gt; will remove &lt;code&gt;u&lt;/code&gt; from the string prefix as it is meaningless in those
scenarios.&lt;/p&gt;
&lt;p&gt;The main reason to standardize on a single form of quotes is aesthetics. Having one kind
of quotes everywhere reduces reader distraction. It will also enable a future version of
&lt;em&gt;Black&lt;/em&gt; to merge consecutive string literals that ended up on the same line (see
&lt;a href="https://github.com/psf/black/issues/26"&gt;#26&lt;/a&gt; for details).&lt;/p&gt;
&lt;p&gt;Why settle on double quotes? They anticipate apostrophes in English text. They match the
docstring standard described in
&lt;a href="https://www.python.org/dev/peps/pep-0257/#what-is-a-docstring" rel="nofollow"&gt;PEP 257&lt;/a&gt;. An empty
string in double quotes (&lt;code&gt;""&lt;/code&gt;) is impossible to confuse with a one double-quote
regardless of fonts and syntax highlighting used. On top of this, double quotes for
strings are consistent with C which Python interacts a lot with.&lt;/p&gt;
&lt;p&gt;On certain keyboard layouts like US English, typing single quotes is a bit easier than
double quotes. The latter requires use of the Shift key. My recommendation here is to
keep using whatever is faster to type and let &lt;em&gt;Black&lt;/em&gt; handle the transformation.&lt;/p&gt;
&lt;p&gt;If you are adopting &lt;em&gt;Black&lt;/em&gt; in a large project with pre-existing string conventions
(like the popular
&lt;a href="https://stackoverflow.com/a/56190" rel="nofollow"&gt;"single quotes for data, double quotes for human-readable strings"&lt;/a&gt;),
you can pass &lt;code&gt;--skip-string-normalization&lt;/code&gt; on the command line. This is meant as an
adoption helper, avoid using this for new projects.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-numeric-literals" class="anchor" aria-hidden="true" href="#numeric-literals"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Numeric literals&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; standardizes most numeric literals to use lowercase letters for the syntactic
parts and uppercase letters for the digits themselves: &lt;code&gt;0xAB&lt;/code&gt; instead of &lt;code&gt;0XAB&lt;/code&gt; and
&lt;code&gt;1e10&lt;/code&gt; instead of &lt;code&gt;1E10&lt;/code&gt;. Python 2 long literals are styled as &lt;code&gt;2L&lt;/code&gt; instead of &lt;code&gt;2l&lt;/code&gt; to
avoid confusion between &lt;code&gt;l&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-line-breaks--binary-operators" class="anchor" aria-hidden="true" href="#line-breaks--binary-operators"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Line breaks &amp;amp; binary operators&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will break a line before a binary operator when splitting a block of code over
multiple lines. This is so that &lt;em&gt;Black&lt;/em&gt; is compliant with the recent changes in the
&lt;a href="https://www.python.org/dev/peps/pep-0008/#should-a-line-break-before-or-after-a-binary-operator" rel="nofollow"&gt;PEP 8&lt;/a&gt;
style guide, which emphasizes that this approach improves readability.&lt;/p&gt;
&lt;p&gt;This behaviour may raise &lt;code&gt;W503 line break before binary operator&lt;/code&gt; warnings in style
guide enforcement tools like Flake8. Since &lt;code&gt;W503&lt;/code&gt; is not PEP 8 compliant, you should
tell Flake8 to ignore these warnings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-slices" class="anchor" aria-hidden="true" href="#slices"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Slices&lt;/h3&gt;
&lt;p&gt;PEP 8
&lt;a href="https://www.python.org/dev/peps/pep-0008/#whitespace-in-expressions-and-statements" rel="nofollow"&gt;recommends&lt;/a&gt;
to treat &lt;code&gt;:&lt;/code&gt; in slices as a binary operator with the lowest priority, and to leave an
equal amount of space on either side, except if a parameter is omitted (e.g.
&lt;code&gt;ham[1 + 1 :]&lt;/code&gt;). It also states that for extended slices, both &lt;code&gt;:&lt;/code&gt; operators have to
have the same amount of spacing, except if a parameter is omitted (&lt;code&gt;ham[1 + 1 ::]&lt;/code&gt;).
&lt;em&gt;Black&lt;/em&gt; enforces these rules consistently.&lt;/p&gt;
&lt;p&gt;This behaviour may raise &lt;code&gt;E203 whitespace before ':'&lt;/code&gt; warnings in style guide
enforcement tools like Flake8. Since &lt;code&gt;E203&lt;/code&gt; is not PEP 8 compliant, you should tell
Flake8 to ignore these warnings.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-parentheses" class="anchor" aria-hidden="true" href="#parentheses"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Parentheses&lt;/h3&gt;
&lt;p&gt;Some parentheses are optional in the Python grammar. Any expression can be wrapped in a
pair of parentheses to form an atom. There are a few interesting cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;if (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;while (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;for (...) in (...):&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;assert (...), (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;from X import (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;assignments like:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;target = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;target: type = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;some, *un, packing = (...)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;augmented += (...)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In those cases, parentheses are removed when the entire statement fits in one line, or
if the inner expression doesn't have any delimiters to further split on. If there is
only a single delimiter and the expression starts or ends with a bracket, the
parenthesis can also be successfully omitted since the existing bracket pair will
organize the expression neatly anyway. Otherwise, the parentheses are added.&lt;/p&gt;
&lt;p&gt;Please note that &lt;em&gt;Black&lt;/em&gt; does not add or remove any additional nested parentheses that
you might want to have for clarity or further code organization. For example those
parentheses are not going to be removed:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;return&lt;/span&gt; &lt;span class="pl-k"&gt;not&lt;/span&gt; (this &lt;span class="pl-k"&gt;or&lt;/span&gt; that)
decision &lt;span class="pl-k"&gt;=&lt;/span&gt; (maybe.this() &lt;span class="pl-k"&gt;and&lt;/span&gt; values &lt;span class="pl-k"&gt;&amp;gt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;) &lt;span class="pl-k"&gt;or&lt;/span&gt; (maybe.that() &lt;span class="pl-k"&gt;and&lt;/span&gt; values &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0&lt;/span&gt;)&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-call-chains" class="anchor" aria-hidden="true" href="#call-chains"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Call chains&lt;/h3&gt;
&lt;p&gt;Some popular APIs, like ORMs, use call chaining. This API style is known as a
&lt;a href="https://en.wikipedia.org/wiki/Fluent_interface" rel="nofollow"&gt;fluent interface&lt;/a&gt;. &lt;em&gt;Black&lt;/em&gt; formats
those by treating dots that follow a call or an indexing operation like a very low
priority delimiter. It's easier to show the behavior than to explain it. Look at the
example:&lt;/p&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;example&lt;/span&gt;(&lt;span class="pl-smi"&gt;session&lt;/span&gt;):
    result &lt;span class="pl-k"&gt;=&lt;/span&gt; (
        session.query(models.Customer.id)
        .filter(
            models.Customer.account_id &lt;span class="pl-k"&gt;==&lt;/span&gt; account_id,
            models.Customer.email &lt;span class="pl-k"&gt;==&lt;/span&gt; email_address,
        )
        .order_by(models.Customer.id.asc())
        .all()
    )&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;a id="user-content-typing-stub-files" class="anchor" aria-hidden="true" href="#typing-stub-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Typing stub files&lt;/h3&gt;
&lt;p&gt;PEP 484 describes the syntax for type hints in Python. One of the use cases for typing
is providing type annotations for modules which cannot contain them directly (they might
be written in C, or they might be third-party, or their implementation may be overly
dynamic, and so on).&lt;/p&gt;
&lt;p&gt;To solve this,
&lt;a href="https://www.python.org/dev/peps/pep-0484/#stub-files" rel="nofollow"&gt;stub files with the &lt;code&gt;.pyi&lt;/code&gt; file extension&lt;/a&gt;
can be used to describe typing information for an external module. Those stub files omit
the implementation of classes and functions they describe, instead they only contain the
structure of the file (listing globals, functions, and classes with their members). The
recommended code style for those files is more terse than PEP 8:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prefer &lt;code&gt;...&lt;/code&gt; on the same line as the class/function signature;&lt;/li&gt;
&lt;li&gt;avoid vertical whitespace between consecutive module-level functions, names, or
methods and fields within a single class;&lt;/li&gt;
&lt;li&gt;use a single blank line between top-level class definitions, or none if the classes
are very small.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; enforces the above rules. There are additional guidelines for formatting &lt;code&gt;.pyi&lt;/code&gt;
file that are not enforced yet but might be in a future version of the formatter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;all function bodies should be empty (contain &lt;code&gt;...&lt;/code&gt; instead of the body);&lt;/li&gt;
&lt;li&gt;do not use docstrings;&lt;/li&gt;
&lt;li&gt;prefer &lt;code&gt;...&lt;/code&gt; over &lt;code&gt;pass&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;for arguments with a default, use &lt;code&gt;...&lt;/code&gt; instead of the actual default;&lt;/li&gt;
&lt;li&gt;avoid using string literals in type annotations, stub files support forward references
natively (like Python 3.7 code with &lt;code&gt;from __future__ import annotations&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;use variable annotations instead of type comments, even for stubs that target older
versions of Python;&lt;/li&gt;
&lt;li&gt;for arguments that default to &lt;code&gt;None&lt;/code&gt;, use &lt;code&gt;Optional[]&lt;/code&gt; explicitly;&lt;/li&gt;
&lt;li&gt;use &lt;code&gt;float&lt;/code&gt; instead of &lt;code&gt;Union[int, float]&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pyprojecttoml" class="anchor" aria-hidden="true" href="#pyprojecttoml"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;pyproject.toml&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is able to read project-specific default values for its command line options
from a &lt;code&gt;pyproject.toml&lt;/code&gt; file. This is especially useful for specifying custom
&lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt; patterns for your project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pro-tip&lt;/strong&gt;: If you're asking yourself "Do I need to configure anything?" the answer is
"No". &lt;em&gt;Black&lt;/em&gt; is all about sensible defaults.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-what-on-earth-is-a-pyprojecttoml-file" class="anchor" aria-hidden="true" href="#what-on-earth-is-a-pyprojecttoml-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;What on Earth is a &lt;code&gt;pyproject.toml&lt;/code&gt; file?&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.python.org/dev/peps/pep-0518/" rel="nofollow"&gt;PEP 518&lt;/a&gt; defines &lt;code&gt;pyproject.toml&lt;/code&gt; as a
configuration file to store build system requirements for Python projects. With the help
of tools like &lt;a href="https://poetry.eustace.io/" rel="nofollow"&gt;Poetry&lt;/a&gt; or
&lt;a href="https://flit.readthedocs.io/en/latest/" rel="nofollow"&gt;Flit&lt;/a&gt; it can fully replace the need for
&lt;code&gt;setup.py&lt;/code&gt; and &lt;code&gt;setup.cfg&lt;/code&gt; files.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-where-black-looks-for-the-file" class="anchor" aria-hidden="true" href="#where-black-looks-for-the-file"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Where &lt;em&gt;Black&lt;/em&gt; looks for the file&lt;/h3&gt;
&lt;p&gt;By default &lt;em&gt;Black&lt;/em&gt; looks for &lt;code&gt;pyproject.toml&lt;/code&gt; starting from the common base directory of
all files and directories passed on the command line. If it's not there, it looks in
parent directories. It stops looking when it finds the file, or a &lt;code&gt;.git&lt;/code&gt; directory, or a
&lt;code&gt;.hg&lt;/code&gt; directory, or the root of the file system, whichever comes first.&lt;/p&gt;
&lt;p&gt;If you're formatting standard input, &lt;em&gt;Black&lt;/em&gt; will look for configuration starting from
the current working directory.&lt;/p&gt;
&lt;p&gt;You can also explicitly specify the path to a particular file that you want with
&lt;code&gt;--config&lt;/code&gt;. In this situation &lt;em&gt;Black&lt;/em&gt; will not look for any other file.&lt;/p&gt;
&lt;p&gt;If you're running with &lt;code&gt;--verbose&lt;/code&gt;, you will see a blue message if a file was found and
used.&lt;/p&gt;
&lt;p&gt;Please note &lt;code&gt;blackd&lt;/code&gt; will not use &lt;code&gt;pyproject.toml&lt;/code&gt; configuration.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-configuration-format" class="anchor" aria-hidden="true" href="#configuration-format"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Configuration format&lt;/h3&gt;
&lt;p&gt;As the file extension suggests, &lt;code&gt;pyproject.toml&lt;/code&gt; is a
&lt;a href="https://github.com/toml-lang/toml"&gt;TOML&lt;/a&gt; file. It contains separate sections for
different tools. &lt;em&gt;Black&lt;/em&gt; is using the &lt;code&gt;[tool.black]&lt;/code&gt; section. The option keys are the
same as long names of options on the command line.&lt;/p&gt;
&lt;p&gt;Note that you have to use single-quoted strings in TOML for regular expressions. It's
the equivalent of r-strings in Python. Multiline strings are treated as verbose regular
expressions by Black. Use &lt;code&gt;[ ]&lt;/code&gt; to denote a significant space character.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Example `pyproject.toml`&lt;/summary&gt;
&lt;div class="highlight highlight-source-toml"&gt;&lt;pre&gt;[&lt;span class="pl-en"&gt;tool&lt;/span&gt;.&lt;span class="pl-en"&gt;black&lt;/span&gt;]
&lt;span class="pl-smi"&gt;line-length&lt;/span&gt; = &lt;span class="pl-c1"&gt;88&lt;/span&gt;
&lt;span class="pl-smi"&gt;target-version&lt;/span&gt; = [&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;py37&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;]
&lt;span class="pl-smi"&gt;include&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;\.pyi?$&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-smi"&gt;exclude&lt;/span&gt; = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'''&lt;/span&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;/span&gt;
&lt;span class="pl-s"&gt;(&lt;/span&gt;
&lt;span class="pl-s"&gt;  /(&lt;/span&gt;
&lt;span class="pl-s"&gt;      \.eggs         # exclude a few common directories in the&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.git          # root of the project&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.hg&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.mypy_cache&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.tox&lt;/span&gt;
&lt;span class="pl-s"&gt;    | \.venv&lt;/span&gt;
&lt;span class="pl-s"&gt;    | _build&lt;/span&gt;
&lt;span class="pl-s"&gt;    | buck-out&lt;/span&gt;
&lt;span class="pl-s"&gt;    | build&lt;/span&gt;
&lt;span class="pl-s"&gt;    | dist&lt;/span&gt;
&lt;span class="pl-s"&gt;  )/&lt;/span&gt;
&lt;span class="pl-s"&gt;  | foo.py           # also separately exclude a file named foo.py in&lt;/span&gt;
&lt;span class="pl-s"&gt;                     # the root of the project&lt;/span&gt;
&lt;span class="pl-s"&gt;)&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'''&lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/details&gt;
&lt;h3&gt;&lt;a id="user-content-lookup-hierarchy" class="anchor" aria-hidden="true" href="#lookup-hierarchy"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Lookup hierarchy&lt;/h3&gt;
&lt;p&gt;Command-line options have defaults that you can see in &lt;code&gt;--help&lt;/code&gt;. A &lt;code&gt;pyproject.toml&lt;/code&gt; can
override those defaults. Finally, options provided by the user on the command line
override both.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; will only ever use one &lt;code&gt;pyproject.toml&lt;/code&gt; file during an entire run. It doesn't
look for multiple files, and doesn't compose configuration from different levels of the
file hierarchy.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-editor-integration" class="anchor" aria-hidden="true" href="#editor-integration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Editor integration&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-emacs" class="anchor" aria-hidden="true" href="#emacs"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Emacs&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/proofit404/blacken"&gt;proofit404/blacken&lt;/a&gt; or
&lt;a href="https://github.com/jorgenschaefer/elpy"&gt;Elpy&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-pycharmintellij-idea" class="anchor" aria-hidden="true" href="#pycharmintellij-idea"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyCharm/IntelliJ IDEA&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;code&gt;black&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;pip install black&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Locate your &lt;code&gt;black&lt;/code&gt; installation folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On macOS / Linux / BSD:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;which black&lt;/span&gt;
&lt;span class="pl-c1"&gt;/usr/local/bin/black  # possible location&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On Windows:&lt;/p&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;where black&lt;/span&gt;
&lt;span class="pl-c1"&gt;%LocalAppData%\Programs\Python\Python36-32\Scripts\black.exe  # possible location&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Open External tools in PyCharm/IntelliJ IDEA&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On macOS:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PyCharm -&amp;gt; Preferences -&amp;gt; Tools -&amp;gt; External Tools&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;On Windows / Linux / BSD:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;File -&amp;gt; Settings -&amp;gt; Tools -&amp;gt; External Tools&lt;/code&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;
&lt;p&gt;Click the + icon to add a new external tool with the following values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Name: Black&lt;/li&gt;
&lt;li&gt;Description: Black is the uncompromising Python code formatter.&lt;/li&gt;
&lt;li&gt;Program: &amp;lt;install_location_from_step_2&amp;gt;&lt;/li&gt;
&lt;li&gt;Arguments: &lt;code&gt;"$FilePath$"&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Format the currently opened file by selecting &lt;code&gt;Tools -&amp;gt; External Tools -&amp;gt; black&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Alternatively, you can set a keyboard shortcut by navigating to
&lt;code&gt;Preferences or Settings -&amp;gt; Keymap -&amp;gt; External Tools -&amp;gt; External Tools - Black&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optionally, run &lt;em&gt;Black&lt;/em&gt; on every file save:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure you have the
&lt;a href="https://plugins.jetbrains.com/plugin/7177-file-watchers" rel="nofollow"&gt;File Watcher&lt;/a&gt; plugin
installed.&lt;/li&gt;
&lt;li&gt;Go to &lt;code&gt;Preferences or Settings -&amp;gt; Tools -&amp;gt; File Watchers&lt;/code&gt; and click &lt;code&gt;+&lt;/code&gt; to add a
new watcher:
&lt;ul&gt;
&lt;li&gt;Name: Black&lt;/li&gt;
&lt;li&gt;File type: Python&lt;/li&gt;
&lt;li&gt;Scope: Project Files&lt;/li&gt;
&lt;li&gt;Program: &amp;lt;install_location_from_step_2&amp;gt;&lt;/li&gt;
&lt;li&gt;Arguments: &lt;code&gt;$FilePath$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Output paths to refresh: &lt;code&gt;$FilePath$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Working directory: &lt;code&gt;$ProjectFileDir$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Uncheck "Auto-save edited files to trigger the watcher"&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-wing-ide" class="anchor" aria-hidden="true" href="#wing-ide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Wing IDE&lt;/h3&gt;
&lt;p&gt;Wing supports black via the OS Commands tool, as explained in the Wing documentation on
&lt;a href="https://wingware.com/doc/edit/pep8" rel="nofollow"&gt;pep8 formatting&lt;/a&gt;. The detailed procedure is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;code&gt;black&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;pip install black&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Make sure it runs from the command line, e.g.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;$ &lt;span class="pl-s1"&gt;black --help&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;In Wing IDE, activate the &lt;strong&gt;OS Commands&lt;/strong&gt; panel and define the command &lt;strong&gt;black&lt;/strong&gt; to
execute black on the currently selected file:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Use the Tools -&amp;gt; OS Commands menu selection&lt;/li&gt;
&lt;li&gt;click on &lt;strong&gt;+&lt;/strong&gt; in &lt;strong&gt;OS Commands&lt;/strong&gt; -&amp;gt; New: Command line..
&lt;ul class="contains-task-list"&gt;
&lt;li&gt;Title: black&lt;/li&gt;
&lt;li&gt;Command Line: black %s&lt;/li&gt;
&lt;li&gt;I/O Encoding: Use Default&lt;/li&gt;
&lt;li&gt;Key Binding: F1&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Raise OS Commands when executed&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Auto-save files before execution&lt;/li&gt;
&lt;li class="task-list-item"&gt;&lt;input type="checkbox" id="" disabled="" class="task-list-item-checkbox" checked=""&gt; Line mode&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Select a file in the editor and press &lt;strong&gt;F1&lt;/strong&gt; , or whatever key binding you selected
in step 3, to reformat the file.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-vim" class="anchor" aria-hidden="true" href="#vim"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Vim&lt;/h3&gt;
&lt;p&gt;Commands and shortcuts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;:Black&lt;/code&gt; to format the entire file (ranges not supported);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:BlackUpgrade&lt;/code&gt; to upgrade &lt;em&gt;Black&lt;/em&gt; inside the virtualenv;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:BlackVersion&lt;/code&gt; to get the current version of &lt;em&gt;Black&lt;/em&gt; inside the virtualenv.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Configuration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;g:black_fast&lt;/code&gt; (defaults to &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_linelength&lt;/code&gt; (defaults to &lt;code&gt;88&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_skip_string_normalization&lt;/code&gt; (defaults to &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;g:black_virtualenv&lt;/code&gt; (defaults to &lt;code&gt;~/.vim/black&lt;/code&gt; or &lt;code&gt;~/.local/share/nvim/black&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To install with &lt;a href="https://github.com/junegunn/vim-plug"&gt;vim-plug&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Plug 'psf/black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or with &lt;a href="https://github.com/VundleVim/Vundle.vim"&gt;Vundle&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Plugin 'psf/black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or you can copy the plugin from
&lt;a href="https://github.com/psf/black/tree/master/plugin/black.vim"&gt;plugin/black.vim&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p ~/.vim/pack/python/start/black/plugin
curl https://raw.githubusercontent.com/psf/black/master/plugin/black.vim -o ~/.vim/pack/python/start/black/plugin/black.vim
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me know if this requires any changes to work with Vim 8's builtin &lt;code&gt;packadd&lt;/code&gt;, or
Pathogen, and so on.&lt;/p&gt;
&lt;p&gt;This plugin &lt;strong&gt;requires Vim 7.0+ built with Python 3.6+ support&lt;/strong&gt;. It needs Python 3.6 to
be able to run &lt;em&gt;Black&lt;/em&gt; inside the Vim process which is much faster than calling an
external command.&lt;/p&gt;
&lt;p&gt;On first run, the plugin creates its own virtualenv using the right Python version and
automatically installs &lt;em&gt;Black&lt;/em&gt;. You can upgrade it later by calling &lt;code&gt;:BlackUpgrade&lt;/code&gt; and
restarting Vim.&lt;/p&gt;
&lt;p&gt;If you need to do anything special to make your virtualenv work and install &lt;em&gt;Black&lt;/em&gt; (for
example you want to run a version from master), create a virtualenv manually and point
&lt;code&gt;g:black_virtualenv&lt;/code&gt; to it. The plugin will use it.&lt;/p&gt;
&lt;p&gt;To run &lt;em&gt;Black&lt;/em&gt; on save, add the following line to &lt;code&gt;.vimrc&lt;/code&gt; or &lt;code&gt;init.vim&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autocmd BufWritePre *.py execute ':Black'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run &lt;em&gt;Black&lt;/em&gt; on a key press (e.g. F9 below), add this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nnoremap &amp;lt;F9&amp;gt; :Black&amp;lt;CR&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;How to get Vim with Python 3.6?&lt;/strong&gt; On Ubuntu 17.10 Vim comes with Python 3.6 by
default. On macOS with Homebrew run: &lt;code&gt;brew install vim --with-python3&lt;/code&gt;. When building
Vim from source, use: &lt;code&gt;./configure --enable-python3interp=yes&lt;/code&gt;. There's many guides
online how to do this.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-visual-studio-code" class="anchor" aria-hidden="true" href="#visual-studio-code"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Visual Studio Code&lt;/h3&gt;
&lt;p&gt;Use the
&lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-python.python" rel="nofollow"&gt;Python extension&lt;/a&gt;
(&lt;a href="https://code.visualstudio.com/docs/python/editing#_formatting" rel="nofollow"&gt;instructions&lt;/a&gt;).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-sublimetext-3" class="anchor" aria-hidden="true" href="#sublimetext-3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;SublimeText 3&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/jgirardet/sublack"&gt;sublack plugin&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-jupyter-notebook-magic" class="anchor" aria-hidden="true" href="#jupyter-notebook-magic"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Jupyter Notebook Magic&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://github.com/csurfer/blackcellmagic"&gt;blackcellmagic&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-python-language-server" class="anchor" aria-hidden="true" href="#python-language-server"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Python Language Server&lt;/h3&gt;
&lt;p&gt;If your editor supports the &lt;a href="https://langserver.org/" rel="nofollow"&gt;Language Server Protocol&lt;/a&gt; (Atom,
Sublime Text, Visual Studio Code and many more), you can use the
&lt;a href="https://github.com/palantir/python-language-server"&gt;Python Language Server&lt;/a&gt; with the
&lt;a href="https://github.com/rupert/pyls-black"&gt;pyls-black&lt;/a&gt; plugin.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-atomnuclide" class="anchor" aria-hidden="true" href="#atomnuclide"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Atom/Nuclide&lt;/h3&gt;
&lt;p&gt;Use &lt;a href="https://atom.io/packages/python-black" rel="nofollow"&gt;python-black&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-kakoune" class="anchor" aria-hidden="true" href="#kakoune"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Kakoune&lt;/h3&gt;
&lt;p&gt;Add the following hook to your kakrc, then run black with &lt;code&gt;:format&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hook global WinSetOption filetype=python %{
    set-option window formatcmd 'black -q  -'
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-other-editors" class="anchor" aria-hidden="true" href="#other-editors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other editors&lt;/h3&gt;
&lt;p&gt;Other editors will require external contributions.&lt;/p&gt;
&lt;p&gt;Patches welcome! &lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;‚ú®&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f370.png"&gt;üç∞&lt;/g-emoji&gt; &lt;g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png"&gt;‚ú®&lt;/g-emoji&gt;&lt;/p&gt;
&lt;p&gt;Any tool that can pipe code through &lt;em&gt;Black&lt;/em&gt; using its stdio mode (just
&lt;a href="https://www.tldp.org/LDP/abs/html/special-chars.html#DASHREF2" rel="nofollow"&gt;use &lt;code&gt;-&lt;/code&gt; as the file name&lt;/a&gt;).
The formatted code will be returned on stdout (unless &lt;code&gt;--check&lt;/code&gt; was passed). &lt;em&gt;Black&lt;/em&gt;
will still emit messages on stderr but that shouldn't affect your use case.&lt;/p&gt;
&lt;p&gt;This can be used for example with PyCharm's or IntelliJ's
&lt;a href="https://www.jetbrains.com/help/pycharm/file-watchers.html" rel="nofollow"&gt;File Watchers&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-blackd" class="anchor" aria-hidden="true" href="#blackd"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;blackd&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; is a small HTTP server that exposes &lt;em&gt;Black&lt;/em&gt;'s functionality over a simple
protocol. The main benefit of using it is to avoid paying the cost of starting up a new
&lt;em&gt;Black&lt;/em&gt; process every time you want to blacken a file.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-usage-1" class="anchor" aria-hidden="true" href="#usage-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Usage&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; is not packaged alongside &lt;em&gt;Black&lt;/em&gt; by default because it has additional
dependencies. You will need to do &lt;code&gt;pip install black[d]&lt;/code&gt; to install it.&lt;/p&gt;
&lt;p&gt;You can start the server on the default port, binding only to the local interface by
running &lt;code&gt;blackd&lt;/code&gt;. You will see a single line mentioning the server's version, and the
host and port it's listening on. &lt;code&gt;blackd&lt;/code&gt; will then print an access log similar to most
web servers on standard output, merged with any exception traces caused by invalid
formatting requests.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; provides even less options than &lt;em&gt;Black&lt;/em&gt;. You can see them by running
&lt;code&gt;blackd --help&lt;/code&gt;:&lt;/p&gt;
&lt;pre lang="text"&gt;&lt;code&gt;Usage: blackd [OPTIONS]

Options:
  --bind-host TEXT                Address to bind the server to.
  --bind-port INTEGER             Port to listen on
  --version                       Show the version and exit.
  -h, --help                      Show this message and exit.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no official blackd client tool (yet!). You can test that blackd is working
using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;blackd --bind-port 9090 &amp;amp;  # or let blackd choose a port
curl -s -XPOST "localhost:9090" -d "print('valid')"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-protocol" class="anchor" aria-hidden="true" href="#protocol"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Protocol&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; only accepts &lt;code&gt;POST&lt;/code&gt; requests at the &lt;code&gt;/&lt;/code&gt; path. The body of the request should
contain the python source code to be formatted, encoded according to the &lt;code&gt;charset&lt;/code&gt; field
in the &lt;code&gt;Content-Type&lt;/code&gt; request header. If no &lt;code&gt;charset&lt;/code&gt; is specified, &lt;code&gt;blackd&lt;/code&gt; assumes
&lt;code&gt;UTF-8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are a few HTTP headers that control how the source is formatted. These correspond
to command line flags for &lt;em&gt;Black&lt;/em&gt;. There is one exception to this: &lt;code&gt;X-Protocol-Version&lt;/code&gt;
which if present, should have the value &lt;code&gt;1&lt;/code&gt;, otherwise the request is rejected with
&lt;code&gt;HTTP 501&lt;/code&gt; (Not Implemented).&lt;/p&gt;
&lt;p&gt;The headers controlling how code is formatted are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;X-Line-Length&lt;/code&gt;: corresponds to the &lt;code&gt;--line-length&lt;/code&gt; command line flag.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Skip-String-Normalization&lt;/code&gt;: corresponds to the &lt;code&gt;--skip-string-normalization&lt;/code&gt;
command line flag. If present and its value is not the empty string, no string
normalization will be performed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Fast-Or-Safe&lt;/code&gt;: if set to &lt;code&gt;fast&lt;/code&gt;, &lt;code&gt;blackd&lt;/code&gt; will act as &lt;em&gt;Black&lt;/em&gt; does when passed the
&lt;code&gt;--fast&lt;/code&gt; command line flag.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Python-Variant&lt;/code&gt;: if set to &lt;code&gt;pyi&lt;/code&gt;, &lt;code&gt;blackd&lt;/code&gt; will act as &lt;em&gt;Black&lt;/em&gt; does when passed the
&lt;code&gt;--pyi&lt;/code&gt; command line flag. Otherwise, its value must correspond to a Python version or
a set of comma-separated Python versions, optionally prefixed with &lt;code&gt;py&lt;/code&gt;. For example,
to request code that is compatible with Python 3.5 and 3.6, set the header to
&lt;code&gt;py3.5,py3.6&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X-Diff&lt;/code&gt;: corresponds to the &lt;code&gt;--diff&lt;/code&gt; command line flag. If present, a diff of the
formats will be output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of these headers are set to invalid values, &lt;code&gt;blackd&lt;/code&gt; returns a &lt;code&gt;HTTP 400&lt;/code&gt; error
response, mentioning the name of the problematic header in the message body.&lt;/p&gt;
&lt;p&gt;Apart from the above, &lt;code&gt;blackd&lt;/code&gt; can produce the following response codes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;HTTP 204&lt;/code&gt;: If the input is already well-formatted. The response body is empty.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 200&lt;/code&gt;: If formatting was needed on the input. The response body contains the
blackened Python code, and the &lt;code&gt;Content-Type&lt;/code&gt; header is set accordingly.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 400&lt;/code&gt;: If the input contains a syntax error. Details of the error are returned in
the response body.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HTTP 500&lt;/code&gt;: If there was any kind of error while trying to format the input. The
response body contains a textual representation of the error.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The response headers include a &lt;code&gt;X-Black-Version&lt;/code&gt; header containing the version of
&lt;em&gt;Black&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-version-control-integration" class="anchor" aria-hidden="true" href="#version-control-integration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Version control integration&lt;/h2&gt;
&lt;p&gt;Use &lt;a href="https://pre-commit.com/" rel="nofollow"&gt;pre-commit&lt;/a&gt;. Once you
&lt;a href="https://pre-commit.com/#install" rel="nofollow"&gt;have it installed&lt;/a&gt;, add this to the
&lt;code&gt;.pre-commit-config.yaml&lt;/code&gt; in your repository:&lt;/p&gt;
&lt;div class="highlight highlight-source-yaml"&gt;&lt;pre&gt;&lt;span class="pl-ent"&gt;repos&lt;/span&gt;:
  - &lt;span class="pl-ent"&gt;repo&lt;/span&gt;: &lt;span class="pl-s"&gt;https://github.com/psf/black&lt;/span&gt;
    &lt;span class="pl-ent"&gt;rev&lt;/span&gt;: &lt;span class="pl-s"&gt;stable&lt;/span&gt;
    &lt;span class="pl-ent"&gt;hooks&lt;/span&gt;:
      - &lt;span class="pl-ent"&gt;id&lt;/span&gt;: &lt;span class="pl-s"&gt;black&lt;/span&gt;
        &lt;span class="pl-ent"&gt;language_version&lt;/span&gt;: &lt;span class="pl-s"&gt;python3.6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then run &lt;code&gt;pre-commit install&lt;/code&gt; and you're ready to go.&lt;/p&gt;
&lt;p&gt;Avoid using &lt;code&gt;args&lt;/code&gt; in the hook. Instead, store necessary configuration in
&lt;code&gt;pyproject.toml&lt;/code&gt; so that editors and command-line usage of Black all behave consistently
for your project. See &lt;em&gt;Black&lt;/em&gt;'s own &lt;a href="/pyproject.toml"&gt;pyproject.toml&lt;/a&gt; for an example.&lt;/p&gt;
&lt;p&gt;If you're already using Python 3.7, switch the &lt;code&gt;language_version&lt;/code&gt; accordingly. Finally,
&lt;code&gt;stable&lt;/code&gt; is a tag that is pinned to the latest release on PyPI. If you'd rather run on
master, this is also an option.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-ignoring-unmodified-files" class="anchor" aria-hidden="true" href="#ignoring-unmodified-files"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Ignoring unmodified files&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; remembers files it has already formatted, unless the &lt;code&gt;--diff&lt;/code&gt; flag is used or
code is passed via standard input. This information is stored per-user. The exact
location of the file depends on the &lt;em&gt;Black&lt;/em&gt; version and the system on which &lt;em&gt;Black&lt;/em&gt; is
run. The file is non-portable. The standard location on common operating systems is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Windows:
&lt;code&gt;C:\\Users\&amp;lt;username&amp;gt;\AppData\Local\black\black\Cache\&amp;lt;version&amp;gt;\cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;macOS:
&lt;code&gt;/Users/&amp;lt;username&amp;gt;/Library/Caches/black/&amp;lt;version&amp;gt;/cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Linux:
&lt;code&gt;/home/&amp;lt;username&amp;gt;/.cache/black/&amp;lt;version&amp;gt;/cache.&amp;lt;line-length&amp;gt;.&amp;lt;file-mode&amp;gt;.pickle&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;file-mode&lt;/code&gt; is an int flag that determines whether the file was formatted as 3.6+ only,
as .pyi, and whether string normalization was omitted.&lt;/p&gt;
&lt;p&gt;To override the location of these files on macOS or Linux, set the environment variable
&lt;code&gt;XDG_CACHE_HOME&lt;/code&gt; to your preferred location. For example, if you want to put the cache
in the directory you're running &lt;em&gt;Black&lt;/em&gt; from, set &lt;code&gt;XDG_CACHE_HOME=.cache&lt;/code&gt;. &lt;em&gt;Black&lt;/em&gt; will
then write the above files to &lt;code&gt;.cache/black/&amp;lt;version&amp;gt;/&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-used-by" class="anchor" aria-hidden="true" href="#used-by"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Used by&lt;/h2&gt;
&lt;p&gt;The following notable open-source projects trust &lt;em&gt;Black&lt;/em&gt; with enforcing a consistent
code style: pytest, tox, Pyramid, Django Channels, Hypothesis, attrs, SQLAlchemy,
Poetry, PyPA applications (Warehouse, Pipenv, virtualenv), pandas, Pillow, every Datadog
Agent Integration.&lt;/p&gt;
&lt;p&gt;Are we missing anyone? Let us know.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-testimonials" class="anchor" aria-hidden="true" href="#testimonials"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testimonials&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Dusty Phillips&lt;/strong&gt;,
&lt;a href="https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;amp;field-keywords=dusty+phillips" rel="nofollow"&gt;writer&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is opinionated so you don't have to be.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hynek Schlawack&lt;/strong&gt;, &lt;a href="https://www.attrs.org/" rel="nofollow"&gt;creator of &lt;code&gt;attrs&lt;/code&gt;&lt;/a&gt;, core developer of
Twisted and CPython:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An auto-formatter that doesn't suck is all I want for Xmas!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Carl Meyer&lt;/strong&gt;, &lt;a href="https://www.djangoproject.com/" rel="nofollow"&gt;Django&lt;/a&gt; core developer:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At least the name is good.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kenneth Reitz&lt;/strong&gt;, creator of &lt;a href="http://python-requests.org/" rel="nofollow"&gt;&lt;code&gt;requests&lt;/code&gt;&lt;/a&gt; and
&lt;a href="https://docs.pipenv.org/" rel="nofollow"&gt;&lt;code&gt;pipenv&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This vastly improves the formatting of our code. Thanks a ton!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;&lt;a id="user-content-show-your-style" class="anchor" aria-hidden="true" href="#show-your-style"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Show your style&lt;/h2&gt;
&lt;p&gt;Use the badge in your project's README.md:&lt;/p&gt;
&lt;div class="highlight highlight-source-gfm"&gt;&lt;pre&gt;[![&lt;span class="pl-e"&gt;Code style: black&lt;/span&gt;](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the badge in README.rst:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks like this:
&lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://camo.githubusercontent.com/28a51fe3a2c05048d8ca8ecd039d6b1619037326/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;MIT&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contributing-to-black" class="anchor" aria-hidden="true" href="#contributing-to-black"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing to &lt;em&gt;Black&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;In terms of inspiration, &lt;em&gt;Black&lt;/em&gt; is about as configurable as &lt;em&gt;gofmt&lt;/em&gt;. This is
deliberate.&lt;/p&gt;
&lt;p&gt;Bug reports and fixes are always welcome! However, before you suggest a new feature or
configuration knob, ask yourself why you want it. If it enables better integration with
some workflow, fixes an inconsistency, speeds things up, and so on - go for it! On the
other hand, if your answer is "because I don't like a particular formatting" then you're
not ready to embrace &lt;em&gt;Black&lt;/em&gt; yet. Such changes are unlikely to get accepted. You can
still try but prepare to be disappointed.&lt;/p&gt;
&lt;p&gt;More details can be found in &lt;a href="CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-change-log" class="anchor" aria-hidden="true" href="#change-log"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Change Log&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-1910b0" class="anchor" aria-hidden="true" href="#1910b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;19.10b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added support for PEP 572 assignment expressions (#711)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for PEP 570 positional-only arguments (#943)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for async generators (#593)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added support for pre-splitting collections by putting an explicit trailing comma
inside (#826)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;black -c&lt;/code&gt; as a way to format code passed from the command line (#761)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;--safe now works with Python 2 code (#840)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed grammar selection for Python 2-specific code (#765)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed feature detection for trailing commas in function definitions and call sites
(#763)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt;/&lt;code&gt;# fmt: on&lt;/code&gt; comment pairs placed multiple times within the same block of
code now behave correctly (#1005)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on Windows machines with more than 61 cores (#838)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on standalone comments prepended with a backslash (#767)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on &lt;code&gt;from&lt;/code&gt; ... &lt;code&gt;import&lt;/code&gt; blocks with comments (#829)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer crashes on Python 3.7 on some platform configurations (#494)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer fails on comments in from-imports (#671)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer fails when the file starts with a backslash (#922)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer merges regular comments with type comments (#1027)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer splits long lines that contain type comments (#997)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;removed unnecessary parentheses around &lt;code&gt;yield&lt;/code&gt; expressions (#834)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added parentheses around long tuples in unpacking assignments (#832)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added parentheses around complex powers when they are prefixed by a unary operator
(#646)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed bug that led &lt;em&gt;Black&lt;/em&gt; format some code with a line length target of 1 (#762)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer introduces quotes in f-string subexpressions on string boundaries
(#863)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if &lt;em&gt;Black&lt;/em&gt; puts parenthesis around a single expression, it moves comments to the
wrapped expression instead of after the brackets (#872)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; now returns the version of &lt;em&gt;Black&lt;/em&gt; in the response headers (#1013)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; can now output the diff of formats on source code when the &lt;code&gt;X-Diff&lt;/code&gt; header is
provided (#969)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-193b0" class="anchor" aria-hidden="true" href="#193b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;19.3b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;new option &lt;code&gt;--target-version&lt;/code&gt; to control which Python versions &lt;em&gt;Black&lt;/em&gt;-formatted code
should target (#618)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;deprecated &lt;code&gt;--py36&lt;/code&gt; (use &lt;code&gt;--target-version=py36&lt;/code&gt; instead) (#724)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer normalizes numeric literals to include &lt;code&gt;_&lt;/code&gt; separators (#696)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;long &lt;code&gt;del&lt;/code&gt; statements are now split into multiple lines (#698)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;type comments are no longer mangled in function signatures&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;improved performance of formatting deeply nested data structures (#509)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now properly formats multiple files in parallel on Windows (#632)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now creates cache files atomically which allows it to be used in parallel
pipelines (like &lt;code&gt;xargs -P8&lt;/code&gt;) (#673)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now correctly indents comments in files that were previously formatted with
tabs (#262)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blackd&lt;/code&gt; now supports CORS (#622)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-189b0" class="anchor" aria-hidden="true" href="#189b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.9b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;numeric literals are now formatted by &lt;em&gt;Black&lt;/em&gt; (#452, #461, #464, #469):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;numeric literals are normalized to include &lt;code&gt;_&lt;/code&gt; separators on Python 3.6+ code&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--skip-numeric-underscore-normalization&lt;/code&gt; to disable the above behavior and
leave numeric underscores as they were in the input&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code with &lt;code&gt;_&lt;/code&gt; in numeric literals is recognized as Python 3.6+&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;most letters in numeric literals are lowercased (e.g., in &lt;code&gt;1e10&lt;/code&gt;, &lt;code&gt;0x01&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hexadecimal digits are always uppercased (e.g. &lt;code&gt;0xBADC0DE&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;blackd&lt;/code&gt;, see &lt;a href="#blackd"&gt;its documentation&lt;/a&gt; for more info (#349)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;adjacent string literals are now correctly split into multiple lines (#463)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;trailing comma is now added to single imports that don't fit on a line (#250)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cache is now populated when &lt;code&gt;--check&lt;/code&gt; is successful for a file which speeds up
consecutive checks of properly formatted unmodified files (#448)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;whitespace at the beginning of the file is now removed (#399)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed mangling &lt;a href="http://mpastell.com/pweave/" rel="nofollow"&gt;pweave&lt;/a&gt; and
&lt;a href="https://pythonhosted.org/spyder/" rel="nofollow"&gt;Spyder IDE&lt;/a&gt; special comments (#532)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when unpacking big tuples (#267)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of &lt;code&gt;__future__&lt;/code&gt; imports with renames (#389)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed scope of &lt;code&gt;# fmt: off&lt;/code&gt; when directly preceding &lt;code&gt;yield&lt;/code&gt; and other nodes (#385)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed formatting of lambda expressions with default arguments (#468)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed &lt;code&gt;async for&lt;/code&gt; statements: &lt;em&gt;Black&lt;/em&gt; no longer breaks them into separate lines (#372)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;note: the Vim plugin stopped registering &lt;code&gt;,=&lt;/code&gt; as a default chord as it turned out to
be a bad idea (#415)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b4" class="anchor" aria-hidden="true" href="#186b4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;hotfix: don't freeze when multiple comments directly precede &lt;code&gt;# fmt: off&lt;/code&gt; (#371)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b3" class="anchor" aria-hidden="true" href="#186b3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;typing stub files (&lt;code&gt;.pyi&lt;/code&gt;) now have blank lines added after constants (#340)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt; and &lt;code&gt;# fmt: on&lt;/code&gt; are now much more dependable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;they now work also within bracket pairs (#329)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they now correctly work across function/class boundaries (#335)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they now work when an indentation block starts with empty lines or misaligned
comments (#334)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;made Click not fail on invalid environments; note that Click is right but the
likelihood we'll need to access non-ASCII file paths when dealing with Python source
code is low (#277)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed improper formatting of f-strings with quotes inside interpolated expressions
(#322)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown when long list literals where found in a file&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown on AST nodes with very many siblings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed cannibalizing backslashes during string normalization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed a crash due to symbolic links pointing outside of the project directory (#338)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b2" class="anchor" aria-hidden="true" href="#186b2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--config&lt;/code&gt; (#65)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;-h&lt;/code&gt; equivalent to &lt;code&gt;--help&lt;/code&gt; (#316)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed improper unmodified file caching when &lt;code&gt;-S&lt;/code&gt; was used&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra space in string unpacking (#305)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed formatting of empty triple quoted strings (#313)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary slowdown in comment placement calculation on lines without comments&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b1" class="anchor" aria-hidden="true" href="#186b1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;hotfix: don't output human-facing information on stdout (#299)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hotfix: don't output cake emoji on non-zero return code (#300)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-186b0" class="anchor" aria-hidden="true" href="#186b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.6b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt; (#270)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--skip-string-normalization&lt;/code&gt; (#118)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--verbose&lt;/code&gt; (#283)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the header output in &lt;code&gt;--diff&lt;/code&gt; now actually conforms to the unified diff spec&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed long trivial assignments being wrapped in unnecessary parentheses (#273)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unnecessary parentheses when a line contained multiline strings (#232)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed stdin handling not working correctly if an old version of Click was used (#276)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now preserves line endings when formatting a file in place (#258)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-185b1" class="anchor" aria-hidden="true" href="#185b1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.5b1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--pyi&lt;/code&gt; (#249)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--py36&lt;/code&gt; (#249)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Python grammar pickle caches are stored with the formatting caches, making &lt;em&gt;Black&lt;/em&gt;
work in environments where site-packages is not user-writable (#192)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now enforces a PEP 257 empty line after a class-level docstring (and/or
fields) and the first method&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid code produced when standalone comments were present in a trailer that
was omitted from line splitting on a large expression (#237)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed optional parentheses being removed within &lt;code&gt;# fmt: off&lt;/code&gt; sections (#224)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid code produced when stars in very long imports were incorrectly wrapped
in optional parentheses (#234)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when inline comments were moved around in a trailer that was
omitted from line splitting on a large expression (#238)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra empty line between a class declaration and the first method if no class
docstring or fields are present (#219)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed extra empty line between a function signature and an inner function or inner
class (#196)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-185b0" class="anchor" aria-hidden="true" href="#185b0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.5b0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;call chains are now formatted according to the
&lt;a href="https://en.wikipedia.org/wiki/Fluent_interface" rel="nofollow"&gt;fluent interfaces&lt;/a&gt; style (#67)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;data structure literals (tuples, lists, dictionaries, and sets) are now also always
exploded like imports when they don't fit in a single line (#152)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;slices are now formatted according to PEP 8 (#178)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;parentheses are now also managed automatically on the right-hand side of assignments
and return statements (#140)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;math operators now use their respective priorities for delimiting multiline
expressions (#148)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;optional parentheses are now omitted on expressions that start or end with a bracket
and only contain a single operator (#177)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;empty parentheses in a class definition are now removed (#145, #180)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;string prefixes are now standardized to lowercase and &lt;code&gt;u&lt;/code&gt; is removed on Python 3.6+
only code and Python 2.7+ code with the &lt;code&gt;unicode_literals&lt;/code&gt; future import (#188, #198,
#199)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;typing stub files (&lt;code&gt;.pyi&lt;/code&gt;) are now formatted in a style that is consistent with PEP
484 (#207, #210)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;progress when reformatting many files is now reported incrementally&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed trailers (content with brackets) being unnecessarily exploded into their own
lines after a dedented closing bracket (#119)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed an invalid trailing comma sometimes left in imports (#185)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed non-deterministic formatting when multiple pairs of removable parentheses were
used (#183)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed multiline strings being unnecessarily wrapped in optional parentheses in long
assignments (#215)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed not splitting long from-imports with only a single name&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed Python 3.6+ file discovery by also looking at function calls with unpacking.
This fixed non-deterministic formatting if trailing commas where used both in function
signatures with stars and function calls with stars but the former would be
reformatted to a single line.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed crash on dealing with optional parentheses (#193)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed "is", "is not", "in", and "not in" not considered operators for splitting
purposes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed crash when dead symlinks where encountered&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a4" class="anchor" aria-hidden="true" href="#184a4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;don't populate the cache on &lt;code&gt;--check&lt;/code&gt; (#175)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a3" class="anchor" aria-hidden="true" href="#184a3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added a "cache"; files already reformatted that haven't changed on disk won't be
reformatted again (#109)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;--check&lt;/code&gt; and &lt;code&gt;--diff&lt;/code&gt; are no longer mutually exclusive (#149)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;generalized star expression handling, including double stars; this fixes
multiplication making expressions "unsafe" for trailing commas (#132)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; no longer enforces putting empty lines behind control flow statements (#90)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; now splits imports like "Mode 3 + trailing comma" of isort (#127)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed comment indentation when a standalone comment closes a block (#16, #32)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed standalone comments receiving extra empty lines if immediately preceding a
class, def, or decorator (#56, #154)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed &lt;code&gt;--diff&lt;/code&gt; not showing entire path (#130)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of complex expressions after star and double stars in function calls
(#2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid splitting on comma in lambda arguments (#133)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed missing splits of ternary expressions (#141)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a2" class="anchor" aria-hidden="true" href="#184a2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;fixed parsing of unaligned standalone comments (#99, #112)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed placement of dictionary unpacking inside dictionary literals (#111)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vim plugin now works on Windows, too&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting when encountering unnecessarily escaped quotes in a string
(#120)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a1" class="anchor" aria-hidden="true" href="#184a1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--quiet&lt;/code&gt; (#78)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added automatic parentheses management (#4)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added &lt;a href="https://pre-commit.com" rel="nofollow"&gt;pre-commit&lt;/a&gt; integration (#103, #104)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed reporting on &lt;code&gt;--check&lt;/code&gt; with multiple files (#101, #102)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed removing backslash escapes from raw strings (#100, #105)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-184a0" class="anchor" aria-hidden="true" href="#184a0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.4a0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--diff&lt;/code&gt; (#87)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;add line breaks before all delimiters, except in cases like commas, to better comply
with PEP 8 (#73)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;standardize string literals to use double quotes (almost) everywhere (#75)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed handling of standalone comments within nested bracketed expressions; &lt;em&gt;Black&lt;/em&gt;
will no longer produce super long lines or put all standalone comments at the end of
the expression (#22)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed 18.3a4 regression: don't crash and burn on empty lines with trailing whitespace
(#80)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed 18.3a4 regression: &lt;code&gt;# yapf: disable&lt;/code&gt; usage as trailing comment would cause
&lt;em&gt;Black&lt;/em&gt; to not emit the rest of the file (#95)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;when CTRL+C is pressed while formatting many files, &lt;em&gt;Black&lt;/em&gt; no longer freaks out with
a flurry of asyncio-related exceptions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only allow up to two empty lines on module level and only single empty lines within
functions (#74)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a4" class="anchor" aria-hidden="true" href="#183a4"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a4&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;# fmt: off&lt;/code&gt; and &lt;code&gt;# fmt: on&lt;/code&gt; are implemented (#5)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;automatic detection of deprecated Python 2 forms of print statements and exec
statements in the formatted file (#49)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;use proper spaces for complex expressions in default values of typed function
arguments (#60)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only return exit code 1 when --check is used (#50)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;don't remove single trailing commas from square bracket indexing (#59)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;don't omit whitespace if the previous factor leaf wasn't a math operator (#55)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;omit extra space in kwarg unpacking if it's the first argument (#46)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;omit extra space in
&lt;a href="http://www.sphinx-doc.org/en/stable/ext/autodoc.html#directive-autoattribute" rel="nofollow"&gt;Sphinx auto-attribute comments&lt;/a&gt;
(#68)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a3" class="anchor" aria-hidden="true" href="#183a3"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a3&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;don't remove single empty lines outside of bracketed expressions (#19)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;added ability to pipe formatting from stdin to stdin (#25)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;restored ability to format code with legacy usage of &lt;code&gt;async&lt;/code&gt; as a name (#20, #42)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;even better handling of numpy-style array indexing (#33, again)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a2" class="anchor" aria-hidden="true" href="#183a2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a2&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;changed positioning of binary operators to occur at beginning of lines instead of at
the end, following
&lt;a href="https://github.com/python/peps/commit/c59c4376ad233a62ca4b3a6060c81368bd21e85b"&gt;a recent change to PEP 8&lt;/a&gt;
(#21)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ignore empty bracket pairs while splitting. This avoids very weirdly looking
formattings (#34, #35)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;remove a trailing comma if there is a single argument to a call&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if top level functions were separated by a comment, don't put four empty lines after
the upper function&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unstable formatting of newlines with imports&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed unintentional folding of post scriptum standalone comments into last statement
if it was a simple statement (#18, #28)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed missing space in numpy-style array indexing (#33)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after star-based unary expressions (#31)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a1" class="anchor" aria-hidden="true" href="#183a1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a1&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;added &lt;code&gt;--check&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;only put trailing commas in function signatures and calls if it's safe to do so. If
the file is Python 3.6+ it's always safe, otherwise only safe if there are no &lt;code&gt;*args&lt;/code&gt;
or &lt;code&gt;**kwargs&lt;/code&gt; used in the signature or call. (#8)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid spacing of dots in relative imports (#6, #13)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed invalid splitting after comma on unpacked variables in for-loops (#23)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space in parenthesized set expressions (#7)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after opening parentheses and in default arguments (#14, #17)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed spurious space after unary operators when the operand was a complex expression
(#15)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-183a0" class="anchor" aria-hidden="true" href="#183a0"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;18.3a0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;first published version, Happy &lt;g-emoji class="g-emoji" alias="cake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f370.png"&gt;üç∞&lt;/g-emoji&gt; Day 2018!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;alpha quality&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;date-versioned (see: &lt;a href="https://calver.org/" rel="nofollow"&gt;https://calver.org/&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-authors" class="anchor" aria-hidden="true" href="#authors"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Authors&lt;/h2&gt;
&lt;p&gt;Glued together by &lt;a href="mailto:lukasz@langa.pl"&gt;≈Åukasz Langa&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Maintained with &lt;a href="mailto:carolcode@willingconsulting.com"&gt;Carol Willing&lt;/a&gt;,
&lt;a href="mailto:carl@oddbird.net"&gt;Carl Meyer&lt;/a&gt;,
&lt;a href="mailto:jelle.zijlstra@gmail.com"&gt;Jelle Zijlstra&lt;/a&gt;,
&lt;a href="mailto:mail@autophagy.io"&gt;Mika Naylor&lt;/a&gt;, and
&lt;a href="mailto:zsol.zsol@gmail.com"&gt;Zsolt Dollenstein&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Multiple contributions by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mailto:cryptolabour@gmail.com"&gt;Abdur-Rahmaan Janhangeer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:me@adamj.eu"&gt;Adam Johnson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@grande.coffee"&gt;Alexander Huynh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:andrew.thorp.dev@gmail.com"&gt;Andrew Thorp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:dyuuus@yandex.ru"&gt;Andrey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:andy@andyfreeland.net"&gt;Andy Freeland&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:asottile@umich.edu"&gt;Anthony Sottile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:arjaan.buijk@gmail.com"&gt;Arjaan Buijk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:proofit404@gmail.com"&gt;Artem Malyshev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:asgerdrewsen@gmail.com"&gt;Asger Hautop Drewsen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:raf@durin42.com"&gt;Augie Fackler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:aviskarkc10@gmail.com"&gt;Aviskar KC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@benjam.info"&gt;Benjamin Woodruff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:brandtbucher@gmail.com"&gt;Brandt Bucher&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Charles Reid&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:christian@python.org"&gt;Christian Heimes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:chuck.wooters@microsoft.com"&gt;Chuck Wooters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:github@thequod.de"&gt;Daniel Hahler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:polycitizen@gmail.com"&gt;Daniel M. Capella&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Daniele Esposti&lt;/li&gt;
&lt;li&gt;dylanjblack&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:eli@treuherz.com"&gt;Eli Treuherz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:fthiery@gmail.com"&gt;Florent Thiery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;hauntsaninja&lt;/li&gt;
&lt;li&gt;Hugo van Kemenade&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ivan.katanic@gmail.com"&gt;Ivan Kataniƒá&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:me@jasonfried.info"&gt;Jason Fried&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ijkl@netc.fr"&gt;jgirardet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:jma353@cornell.edu"&gt;Joe Antonakakis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:jon.dufresne@gmail.com"&gt;Jon Dufresne&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ojiidotch@gmail.com"&gt;Jonas Obrist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:joshbode@fastmail.com"&gt;Josh Bode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:hello@juanlu.space"&gt;Juan Luis Cano Rodr√≠guez&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:katie@glasnt.com"&gt;Katie McLaughlin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lawrence Chan&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:mail@linusgroh.de"&gt;Linus Groh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:luka.sterbic@gmail.com"&gt;Luka Sterbic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mariatta&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:vaneseltine@gmail.com"&gt;Matt VanEseltine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:michael.flaxman@gmail.com"&gt;Michael Flaxman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sully@msully.net"&gt;Michael J. Sullivan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:michael@mcclimon.org"&gt;Michael McClimon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:miggaiowski@gmail.com"&gt;Miguel Gaiowski&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:roshi@fedoraproject.org"&gt;Mike&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:minho42@gmail.com"&gt;Min ho Kim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:miroslav@miki725.com"&gt;Miroslav Shubernetskiy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:neraste.herr10@gmail.com"&gt;Neraste&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ofekmeister@gmail.com"&gt;Ofek Lev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:osaetindaniel@gmail.com"&gt;Osaetin Daniel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:Pablogsal@gmail.com"&gt;Pablo Galindo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:mail@peterbe.com"&gt;Peter Bengtsson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;pmacosta&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:rishijha424@gmail.com"&gt;Rishikesh Jha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:hi@stavros.io"&gt;Stavros Korokithakis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sirosen@globus.org"&gt;Stephen Rosen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:snlkapil@gmail.com"&gt;Sunil Kapil&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:thomas.c.lu@gmail.com"&gt;Thom Lu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:tom@tomchristie.com"&gt;Tom Christie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:uranusjr@gmail.com"&gt;Tzu-ping Chung&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:ukshah2@illinois.edu"&gt;Utsav Shah&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;vezeli&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:sharma.vishwas88@gmail.com"&gt;Vishwas B Sharma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:yngve@hoiseth.net"&gt;Yngve H√∏iseth&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mailto:1998uriyyo@gmail.com"&gt;Yurii Karabas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>psf</author><guid isPermaLink="false">https://github.com/psf/black</guid><pubDate>Tue, 19 Nov 2019 00:24:00 GMT</pubDate></item><item><title>pytorch/examples #25 in Python, This month</title><link>https://github.com/pytorch/examples</link><description>&lt;p&gt;&lt;i&gt;A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-pytorch-examples" class="anchor" aria-hidden="true" href="#pytorch-examples"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;PyTorch Examples&lt;/h1&gt;
&lt;p&gt;A repository showcasing examples of using &lt;a href="https://github.com/pytorch/pytorch"&gt;PyTorch&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="mnist"&gt;Image classification (MNIST) using Convnets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="word_language_model"&gt;Word level Language Modeling using LSTM RNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="imagenet"&gt;Training Imagenet Classifiers with Residual Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="dcgan"&gt;Generative Adversarial Networks (DCGAN)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="vae"&gt;Variational Auto-Encoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="super_resolution"&gt;Superresolution using an efficient sub-pixel convolutional neural network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="mnist_hogwild"&gt;Hogwild training of shared ConvNets across multiple processes on MNIST&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="reinforcement_learning"&gt;Training a CartPole to balance in OpenAI Gym with actor-critic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="snli"&gt;Natural Language Inference (SNLI) with GloVe vectors, LSTMs, and torchtext&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="time_sequence_prediction"&gt;Time sequence prediction - use an LSTM to learn Sine waves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="fast_neural_style"&gt;Implement the Neural Style Transfer algorithm on images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="cpp"&gt;Several examples illustrating the C++ Frontend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, a list of good examples hosted in their own repositories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/OpenNMT/OpenNMT-py"&gt;Neural Machine Translation using sequence-to-sequence RNN with attention (OpenNMT)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>pytorch</author><guid isPermaLink="false">https://github.com/pytorch/examples</guid><pubDate>Tue, 19 Nov 2019 00:25:00 GMT</pubDate></item></channel></rss>