<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: MATLAB, This month</title><link>https://github.com/trending/matlab?since=monthly</link><description>The top repositories on GitHub for matlab, measured monthly</description><pubDate>Fri, 03 Jan 2020 01:04:14 GMT</pubDate><lastBuildDate>Fri, 03 Jan 2020 01:04:14 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>rasmusbergpalm/DeepLearnToolbox #1 in MATLAB, This month</title><link>https://github.com/rasmusbergpalm/DeepLearnToolbox</link><description>&lt;p&gt;&lt;i&gt;Matlab/Octave toolbox for deep learning. Includes Deep Belief Nets, Stacked Autoencoders, Convolutional Neural Nets, Convolutional Autoencoders and vanilla Neural Nets. Each method has examples to get you started.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-deprecation-notice" class="anchor" aria-hidden="true" href="#deprecation-notice"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Deprecation notice.&lt;/h2&gt;
&lt;p&gt;This toolbox is outdated and no longer maintained.&lt;/p&gt;
&lt;p&gt;There are much better tools available for deep learning than this toolbox, e.g. &lt;a href="http://deeplearning.net/software/theano/" rel="nofollow"&gt;Theano&lt;/a&gt;, &lt;a href="http://torch.ch/" rel="nofollow"&gt;torch&lt;/a&gt; or &lt;a href="http://www.tensorflow.org/" rel="nofollow"&gt;tensorflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I would suggest you use one of the tools mentioned above rather than use this toolbox.&lt;/p&gt;
&lt;p&gt;Best, Rasmus.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-deeplearntoolbox" class="anchor" aria-hidden="true" href="#deeplearntoolbox"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;DeepLearnToolbox&lt;/h1&gt;
&lt;p&gt;A Matlab toolbox for Deep Learning.&lt;/p&gt;
&lt;p&gt;Deep Learning is a new subfield of machine learning that focuses on learning deep hierarchical models of data.
It is inspired by the human brain's apparent deep (layered, hierarchical) architecture.
A good overview of the theory of Deep Learning theory is
&lt;a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf" rel="nofollow"&gt;Learning Deep Architectures for AI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For a more informal introduction, see the following videos by Geoffrey Hinton and Andrew Ng.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.youtube.com/watch?v=AyzOUbkUf3M" rel="nofollow"&gt;The Next Generation of Neural Networks&lt;/a&gt; (Hinton, 2007)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.youtube.com/watch?v=VdIURAu1-aU" rel="nofollow"&gt;Recent Developments in Deep Learning&lt;/a&gt; (Hinton, 2010)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.youtube.com/watch?v=ZmNOAtZIgIk" rel="nofollow"&gt;Unsupervised Feature Learning and Deep Learning&lt;/a&gt; (Ng, 2011)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you use this toolbox in your research please cite &lt;a href="http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6284" rel="nofollow"&gt;Prediction as a candidate for learning deep hierarchical models of data&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@MASTERSTHESIS\{IMM2012-06284,
    author       = "R. B. Palm",
    title        = "Prediction as a candidate for learning deep hierarchical models of data",
    year         = "2012",
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Contact: rasmusbergpalm at gmail dot com&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-directories-included-in-the-toolbox" class="anchor" aria-hidden="true" href="#directories-included-in-the-toolbox"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Directories included in the toolbox&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;NN/&lt;/code&gt;   - A library for Feedforward Backpropagation Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;code&gt;CNN/&lt;/code&gt;  - A library for Convolutional Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;code&gt;DBN/&lt;/code&gt;  - A library for Deep Belief Networks&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SAE/&lt;/code&gt;  - A library for Stacked Auto-Encoders&lt;/p&gt;
&lt;p&gt;&lt;code&gt;CAE/&lt;/code&gt; - A library for Convolutional Auto-Encoders&lt;/p&gt;
&lt;p&gt;&lt;code&gt;util/&lt;/code&gt; - Utility functions used by the libraries&lt;/p&gt;
&lt;p&gt;&lt;code&gt;data/&lt;/code&gt; - Data used by the examples&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tests/&lt;/code&gt; - unit tests to verify toolbox is working&lt;/p&gt;
&lt;p&gt;For references on each library check REFS.md&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-setup" class="anchor" aria-hidden="true" href="#setup"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download.&lt;/li&gt;
&lt;li&gt;addpath(genpath('DeepLearnToolbox'));&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-example-deep-belief-network" class="anchor" aria-hidden="true" href="#example-deep-belief-network"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example: Deep Belief Network&lt;/h2&gt;
&lt;div class="highlight highlight-source-matlab"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;function&lt;/span&gt; &lt;span class="pl-en"&gt;test_example_DBN&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-smi"&gt;load&lt;/span&gt; mnist_uint8;&lt;/span&gt;

train_x = double(train_x) &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;255&lt;/span&gt;;
test_x  = double(test_x)  &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;255&lt;/span&gt;;
train_y = double(train_y);
test_y  = double(test_y);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt;  ex1 train a 100 hidden unit RBM and visualize its weights&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
dbn.sizes = [&lt;span class="pl-c1"&gt;100&lt;/span&gt;];
opts.numepochs =   &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;
opts.momentum  =   &lt;span class="pl-c1"&gt;0&lt;/span&gt;;
opts.alpha     =   &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
dbn = dbnsetup(dbn, train_x, opts);
dbn = dbntrain(dbn, train_x, opts);
figure; visualize(dbn.rbm{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}.&lt;span class="pl-k"&gt;W'&lt;/span&gt;);   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Visualize the RBM weights&lt;/span&gt;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt;  ex2 train a 100-100 hidden unit DBN and use its weights to initialize a NN&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;train dbn&lt;/span&gt;
dbn.sizes = [&lt;span class="pl-c1"&gt;100&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt;];
opts.numepochs =   &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;
opts.momentum  =   &lt;span class="pl-c1"&gt;0&lt;/span&gt;;
opts.alpha     =   &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
dbn = dbnsetup(dbn, train_x, opts);
dbn = dbntrain(dbn, train_x, opts);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;unfold dbn to nn&lt;/span&gt;
nn = dbnunfoldtonn(dbn, &lt;span class="pl-c1"&gt;10&lt;/span&gt;);
nn.activation_function = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;sigm&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;train nn&lt;/span&gt;
opts.numepochs =  &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;
nn = nntrain(nn, train_x, train_y, opts);
[er, bad] = nntest(nn, test_x, test_y);

assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.10&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-example-stacked-auto-encoders" class="anchor" aria-hidden="true" href="#example-stacked-auto-encoders"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example: Stacked Auto-Encoders&lt;/h2&gt;
&lt;div class="highlight highlight-source-matlab"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;function&lt;/span&gt; &lt;span class="pl-en"&gt;test_example_SAE&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-smi"&gt;load&lt;/span&gt; mnist_uint8;&lt;/span&gt;

train_x = double(train_x)/&lt;span class="pl-c1"&gt;255&lt;/span&gt;;
test_x  = double(test_x)/&lt;span class="pl-c1"&gt;255&lt;/span&gt;;
train_y = double(train_y);
test_y  = double(test_y);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt;  ex1 train a 100 hidden unit SDAE and use it to initialize a FFNN&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Setup and train a stacked denoising autoencoder (SDAE)&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
sae = saesetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt;]);
sae.ae{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}.activation_function       = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;sigm&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;
sae.ae{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}.learningRate              = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
sae.ae{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}.inputZeroMaskedFraction   = &lt;span class="pl-c1"&gt;0.5&lt;/span&gt;;
opts.numepochs =   &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;
sae = saetrain(sae, train_x, opts);
visualize(sae.ae{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}.W{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}(:,&lt;span class="pl-c1"&gt;2&lt;/span&gt;:&lt;span class="pl-k"&gt;end&lt;/span&gt;)&lt;span class="pl-k"&gt;'&lt;/span&gt;)

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt; Use the SDAE to initialize a FFNN&lt;/span&gt;
nn = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);
nn.activation_function              = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;sigm&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;
nn.learningRate                     = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
nn.W{&lt;span class="pl-c1"&gt;1&lt;/span&gt;} = sae.ae{&lt;span class="pl-c1"&gt;1&lt;/span&gt;}.W{&lt;span class="pl-c1"&gt;1&lt;/span&gt;};

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt; Train the FFNN&lt;/span&gt;
opts.numepochs =   &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;
nn = nntrain(nn, train_x, train_y, opts);
[er, bad] = nntest(nn, test_x, test_y);
assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.16&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-example-convolutional-neural-nets" class="anchor" aria-hidden="true" href="#example-convolutional-neural-nets"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example: Convolutional Neural Nets&lt;/h2&gt;
&lt;div class="highlight highlight-source-matlab"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;function&lt;/span&gt; &lt;span class="pl-en"&gt;test_example_CNN&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-smi"&gt;load&lt;/span&gt; mnist_uint8;&lt;/span&gt;

train_x = double(reshape(&lt;span class="pl-k"&gt;train_x'&lt;/span&gt;,&lt;span class="pl-c1"&gt;28&lt;/span&gt;,&lt;span class="pl-c1"&gt;28&lt;/span&gt;,&lt;span class="pl-c1"&gt;60000&lt;/span&gt;))/&lt;span class="pl-c1"&gt;255&lt;/span&gt;;
test_x = double(reshape(&lt;span class="pl-k"&gt;test_x'&lt;/span&gt;,&lt;span class="pl-c1"&gt;28&lt;/span&gt;,&lt;span class="pl-c1"&gt;28&lt;/span&gt;,&lt;span class="pl-c1"&gt;10000&lt;/span&gt;))/&lt;span class="pl-c1"&gt;255&lt;/span&gt;;
train_y = double(&lt;span class="pl-k"&gt;train_y'&lt;/span&gt;);
test_y = double(&lt;span class="pl-k"&gt;test_y'&lt;/span&gt;);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex1 Train a 6c-2s-12c-2s Convolutional neural network &lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;will run 1 epoch in about 200 second and get around 11% error. &lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;With 100 epochs you'll get around 1.2% error&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
cnn.layers = {
    struct(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;type&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;i&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;input layer&lt;/span&gt;
    struct(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;type&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;c&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;outputmaps&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;6&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;kernelsize&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;convolution layer&lt;/span&gt;
    struct(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;type&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;scale&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;sub sampling layer&lt;/span&gt;
    struct(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;type&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;c&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;outputmaps&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;12&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;kernelsize&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;5&lt;/span&gt;) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;convolution layer&lt;/span&gt;
    struct(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;type&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;s&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;scale&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class="pl-c1"&gt;2&lt;/span&gt;) &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;subsampling layer&lt;/span&gt;
};
cnn = cnnsetup(cnn, train_x, train_y);

opts.alpha = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;
opts.batchsize = &lt;span class="pl-c1"&gt;50&lt;/span&gt;;
opts.numepochs = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;

cnn = cnntrain(cnn, train_x, train_y, opts);

[er, bad] = cnntest(cnn, test_x, test_y);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;plot mean squared error&lt;/span&gt;
figure; plot(cnn.rL);

assert(er&amp;lt;0.12, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a id="user-content-example-neural-networks" class="anchor" aria-hidden="true" href="#example-neural-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Example: Neural Networks&lt;/h2&gt;
&lt;div class="highlight highlight-source-matlab"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;function&lt;/span&gt; &lt;span class="pl-en"&gt;test_example_NN&lt;/span&gt;
&lt;span class="pl-s"&gt;&lt;span class="pl-smi"&gt;load&lt;/span&gt; mnist_uint8;&lt;/span&gt;

train_x = double(train_x) &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;255&lt;/span&gt;;
test_x  = double(test_x)  &lt;span class="pl-k"&gt;/&lt;/span&gt; &lt;span class="pl-c1"&gt;255&lt;/span&gt;;
train_y = double(train_y);
test_y  = double(test_y);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt; normalize&lt;/span&gt;
[train_x, mu, sigma] = zscore(train_x);
test_x = normalize(test_x, mu, sigma);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex1 vanilla neural net&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
nn = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);
opts.numepochs =  &lt;span class="pl-c1"&gt;1&lt;/span&gt;;   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Number of full sweeps through data&lt;/span&gt;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Take a mean gradient step over this many samples&lt;/span&gt;
[nn, L] = nntrain(nn, train_x, train_y, opts);

[er, bad] = nntest(nn, test_x, test_y);

assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.08&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex2 neural net with L2 weight decay&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
nn = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);

nn.weightPenaltyL2 = &lt;span class="pl-c1"&gt;1e-4&lt;/span&gt;;  &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  L2 weight decay&lt;/span&gt;
opts.numepochs =  &lt;span class="pl-c1"&gt;1&lt;/span&gt;;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Number of full sweeps through data&lt;/span&gt;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;       &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Take a mean gradient step over this many samples&lt;/span&gt;

nn = nntrain(nn, train_x, train_y, opts);

[er, bad] = nntest(nn, test_x, test_y);
assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.1&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);


&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex3 neural net with dropout&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
nn = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);

nn.dropoutFraction = &lt;span class="pl-c1"&gt;0.5&lt;/span&gt;;   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Dropout fraction &lt;/span&gt;
opts.numepochs =  &lt;span class="pl-c1"&gt;1&lt;/span&gt;;        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Number of full sweeps through data&lt;/span&gt;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;       &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Take a mean gradient step over this many samples&lt;/span&gt;

nn = nntrain(nn, train_x, train_y, opts);

[er, bad] = nntest(nn, test_x, test_y);
assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.1&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex4 neural net with sigmoid activation function&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
nn = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;100&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);

nn.activation_function = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;sigm&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Sigmoid activation function&lt;/span&gt;
nn.learningRate = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;                &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Sigm require a lower learning rate&lt;/span&gt;
opts.numepochs =  &lt;span class="pl-c1"&gt;1&lt;/span&gt;;                &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Number of full sweeps through data&lt;/span&gt;
opts.batchsize = &lt;span class="pl-c1"&gt;100&lt;/span&gt;;               &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Take a mean gradient step over this many samples&lt;/span&gt;

nn = nntrain(nn, train_x, train_y, opts);

[er, bad] = nntest(nn, test_x, test_y);
assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.1&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex5 plotting functionality&lt;/span&gt;
rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
nn = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;20&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);
opts.numepochs         = &lt;span class="pl-c1"&gt;5&lt;/span&gt;;            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Number of full sweeps through data&lt;/span&gt;
nn.output              = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;softmax&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;    &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  use softmax output&lt;/span&gt;
opts.batchsize         = &lt;span class="pl-c1"&gt;1000&lt;/span&gt;;         &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Take a mean gradient step over this many samples&lt;/span&gt;
opts.plot              = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;            &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  enable plotting&lt;/span&gt;

nn = nntrain(nn, train_x, train_y, opts);

[er, bad] = nntest(nn, test_x, test_y);
assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.1&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);

&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%%&lt;/span&gt; ex6 neural net with sigmoid activation and plotting of validation and training error&lt;/span&gt;
&lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt; split training data into training and validation data&lt;/span&gt;
vx   = train_x(&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;10000&lt;/span&gt;,:);
tx = train_x(&lt;span class="pl-c1"&gt;10001&lt;/span&gt;:&lt;span class="pl-k"&gt;end&lt;/span&gt;,:);
vy   = train_y(&lt;span class="pl-c1"&gt;1&lt;/span&gt;:&lt;span class="pl-c1"&gt;10000&lt;/span&gt;,:);
ty = train_y(&lt;span class="pl-c1"&gt;10001&lt;/span&gt;:&lt;span class="pl-k"&gt;end&lt;/span&gt;,:);

rand(&lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;state&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;,&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
nn                      = nnsetup([&lt;span class="pl-c1"&gt;784&lt;/span&gt; &lt;span class="pl-c1"&gt;20&lt;/span&gt; &lt;span class="pl-c1"&gt;10&lt;/span&gt;]);     
nn.output               = &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;softmax&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;;                   &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  use softmax output&lt;/span&gt;
opts.numepochs          = &lt;span class="pl-c1"&gt;5&lt;/span&gt;;                           &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Number of full sweeps through data&lt;/span&gt;
opts.batchsize          = &lt;span class="pl-c1"&gt;1000&lt;/span&gt;;                        &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  Take a mean gradient step over this many samples&lt;/span&gt;
opts.plot               = &lt;span class="pl-c1"&gt;1&lt;/span&gt;;                           &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  enable plotting&lt;/span&gt;
nn = nntrain(nn, tx, ty, opts, vx, vy);                &lt;span class="pl-c"&gt;&lt;span class="pl-c"&gt;%&lt;/span&gt;  nntrain takes validation set as last two arguments (optionally)&lt;/span&gt;

[er, bad] = nntest(nn, test_x, test_y);
assert(er &lt;span class="pl-k"&gt;&amp;lt;&lt;/span&gt; &lt;span class="pl-c1"&gt;0.1&lt;/span&gt;, &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'&lt;/span&gt;Too big error&lt;span class="pl-pds"&gt;'&lt;/span&gt;&lt;/span&gt;);
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href="https://bitdeli.com/free" title="Bitdeli Badge" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/00e5bbbe78ce5418547b4672181b673cccc1c89c/68747470733a2f2f64327765637a68766c38323376302e636c6f756466726f6e742e6e65742f7261736d75736265726770616c6d2f646565706c6561726e746f6f6c626f782f7472656e642e706e67" alt="Bitdeli Badge" data-canonical-src="https://d2weczhvl823v0.cloudfront.net/rasmusbergpalm/deeplearntoolbox/trend.png" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>rasmusbergpalm</author><guid isPermaLink="false">https://github.com/rasmusbergpalm/DeepLearnToolbox</guid><pubDate>Fri, 03 Jan 2020 00:01:00 GMT</pubDate></item><item><title>atinesh-s/Coursera-Machine-Learning-Stanford #2 in MATLAB, This month</title><link>https://github.com/atinesh-s/Coursera-Machine-Learning-Stanford</link><description>&lt;p&gt;&lt;i&gt;Machine learning-Stanford University&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-coursera" class="anchor" aria-hidden="true" href="#machine-learning-coursera"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Machine Learning (Coursera)&lt;/h1&gt;
&lt;p&gt;This is my solution to all the programming assignments and quizzes of Machine-Learning (Coursera) taught by Andrew Ng. After completing this course you will get a broad idea of Machine learning algorithms. Try to solve all the assignments by yourself first, but if you get stuck somewhere then feel free to browse the code.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Lectures Slides&lt;/li&gt;
&lt;li&gt;Solution to programming assignment&lt;/li&gt;
&lt;li&gt;Solution to Quizzes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-certificate" class="anchor" aria-hidden="true" href="#certificate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Certificate&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.coursera.org/account/accomplishments/certificate/GDDBFB572MUQ" rel="nofollow"&gt;Verified Certificate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-references" class="anchor" aria-hidden="true" href="#references"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.coursera.org/learn/machine-learning" rel="nofollow"&gt;[1] Machine Learning - Stanford University&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>atinesh-s</author><guid isPermaLink="false">https://github.com/atinesh-s/Coursera-Machine-Learning-Stanford</guid><pubDate>Fri, 03 Jan 2020 00:02:00 GMT</pubDate></item><item><title>HuangCongQing/Algorithms_MathModels #3 in MATLAB, This month</title><link>https://github.com/HuangCongQing/Algorithms_MathModels</link><description>&lt;p&gt;&lt;i&gt;【国赛】【美赛】数学建模相关算法 MATLAB实现（2018年初整理）&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-algorithms_mathmodels" class="anchor" aria-hidden="true" href="#algorithms_mathmodels"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Algorithms_MathModels&lt;/h1&gt;
&lt;p&gt;数学建模相关算法 MATLAB实现&lt;/p&gt;
&lt;p&gt;Fork或借鉴请注明出处 &lt;a href="https://github.com/HuangCongQing"&gt;@ChungKing&lt;/a&gt; . Thx&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;国赛论文资料： 链接：&lt;a href="https://pan.baidu.com/s/1xz8kbFauskpzenlgv4koiQ" rel="nofollow"&gt;https://pan.baidu.com/s/1xz8kbFauskpzenlgv4koiQ&lt;/a&gt;
提取码：3kjt&lt;/li&gt;
&lt;li&gt;美赛资料：链接：&lt;a href="https://github.com/HuangCongQing/Algorithms_MathModels/issues/2#issuecomment-565820094"&gt;https://github.com/HuangCongQing/Algorithms_MathModels/issues/2#issuecomment-565820094&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;数模学习视频：链接：&lt;a href="https://pan.baidu.com/s/1TcL5q1he6YfYFNBi9ZzSlw" rel="nofollow"&gt;https://pan.baidu.com/s/1TcL5q1he6YfYFNBi9ZzSlw&lt;/a&gt;
提取码：osbm&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;本项目为数学建模竞赛中所学习使用的相关算法的MATLAB实现。部分参考于&lt;a href="https://github.com/NarcissusHliangZhao/Algorithm_Implementation_in_MatModel"&gt;NarcissusHliangZhao&lt;/a&gt; 。Thanks。 具体内容包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;《MATLAB 神经网络30个案例分析》&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;《基于MATLAB的高等数学问题求解》&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模拟退火算法-最优路径&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;层次分析法(AHP)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;元胞自动机(Cellular Automata)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模糊数学模型(Fuzzy Mathematical Model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;目标规划(Goal Programming)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;图论(Graph Theory)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;灰色系统建模(Grey System)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启发式算法(Heuristic Algorithm)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;免疫算法(Immune Algorithm)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;整数规划(Integer Programming)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;《MATLAB智能算法案例》(Intelligence Algorithm)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;插值(Interpolation)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;线性规划(Linear Programming)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多元分析(Multivarite Analysis)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;神经网络(Neural Network)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;非线性规划(Non Linear Programming)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;常微分方程(Oridinary Differential Equation)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;偏微分方程(Partial Differential Equation)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;偏最小二乘法(Partial Least Squares)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;《模式识别与机器学习》(Pattern Recognition and Machine Learning)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;回归分析(Regression Analysis)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;时间序列模型(Time Series)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a id="user-content-下载单个文件夹或文件" class="anchor" aria-hidden="true" href="#下载单个文件夹或文件"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;下载单个文件夹或文件&lt;/h5&gt;
&lt;p&gt;&lt;a href="http://downgit.zhoudaxiaa.com/" rel="nofollow"&gt;http://downgit.zhoudaxiaa.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;将github文件夹或文件&lt;strong&gt;链接&lt;/strong&gt;复制粘贴入DownGit中，选择download即可;&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;&lt;a id="user-content-related-links" class="anchor" aria-hidden="true" href="#related-links"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Related links&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.zhihu.com/question/19714813/answer/18748623" rel="nofollow"&gt;如何入门参与&lt;em&gt;数学建模&lt;/em&gt;？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;论坛——&lt;a href="http://www.mathor.com/forum.php" rel="nofollow"&gt;校苑数模|数学建模&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-大学生相关" class="anchor" aria-hidden="true" href="#大学生相关"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;大学生相关&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/J2r4WQ3GFUy_t0mNe_s5IA" rel="nofollow"&gt;给我们大学生推荐的自学网站（App）&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://mp.weixin.qq.com/s/kWJi___BTCdj5STTxaAz8Q" rel="nofollow"&gt;大学毕业生采访&amp;amp;&amp;amp;大学四年总结分享&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;希望对大家有所帮助！&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;LICENSE&lt;/h3&gt;
&lt;p&gt;本项目全部内容遵守 MIT 许可协议.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://camo.githubusercontent.com/0cf016a535bd9d48eeddd9a867838339defd455a/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f343334303737322d313539363566646135636465303238312e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430"&gt;&lt;img src="https://camo.githubusercontent.com/0cf016a535bd9d48eeddd9a867838339defd455a/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f343334303737322d313539363566646135636465303238312e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f7374726970253743696d61676556696577322f322f772f31323430" alt="" data-canonical-src="https://upload-images.jianshu.io/upload_images/4340772-15965fda5cde0281.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>HuangCongQing</author><guid isPermaLink="false">https://github.com/HuangCongQing/Algorithms_MathModels</guid><pubDate>Fri, 03 Jan 2020 00:03:00 GMT</pubDate></item><item><title>AvaisP/machine-learning-programming-assignments-coursera-andrew-ng #4 in MATLAB, This month</title><link>https://github.com/AvaisP/machine-learning-programming-assignments-coursera-andrew-ng</link><description>&lt;p&gt;&lt;i&gt;Solutions to Andrew NG's machine learning course on Coursera&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-programming-assignments-coursera-andrew-ng" class="anchor" aria-hidden="true" href="#machine-learning-programming-assignments-coursera-andrew-ng"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;machine-learning-programming-assignments-coursera-andrew-ng&lt;/h1&gt;
&lt;p&gt;Solutions to Andrew NG's machine learning course on Coursera&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>AvaisP</author><guid isPermaLink="false">https://github.com/AvaisP/machine-learning-programming-assignments-coursera-andrew-ng</guid><pubDate>Fri, 03 Jan 2020 00:04:00 GMT</pubDate></item><item><title>Borye/machine-learning-coursera-1 #5 in MATLAB, This month</title><link>https://github.com/Borye/machine-learning-coursera-1</link><description>&lt;p&gt;&lt;i&gt;This repo is specially created for all the work done my me as a part of Coursera's Machine Learning Course.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-machine-learning-coursera" class="anchor" aria-hidden="true" href="#machine-learning-coursera"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;machine-learning-coursera&lt;/h1&gt;
&lt;p&gt;This repo is specially created for all the work done my me as a part of Coursera's Machine Learning Course.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>Borye</author><guid isPermaLink="false">https://github.com/Borye/machine-learning-coursera-1</guid><pubDate>Fri, 03 Jan 2020 00:05:00 GMT</pubDate></item><item><title>cszn/DnCNN #6 in MATLAB, This month</title><link>https://github.com/cszn/DnCNN</link><description>&lt;p&gt;&lt;i&gt;Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising (TIP, 2017)&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-dncnn" class="anchor" aria-hidden="true" href="#dncnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://www4.comp.polyu.edu.hk/~cslzhang/paper/DnCNN.pdf" rel="nofollow"&gt;DnCNN&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a id="user-content-beyond-a-gaussian-denoiser-residual-learning-of-deep-cnn-for-image-denoising" class="anchor" aria-hidden="true" href="#beyond-a-gaussian-denoiser-residual-learning-of-deep-cnn-for-image-denoising"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="http://ieeexplore.ieee.org/document/7839189/" rel="nofollow"&gt;Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a id="user-content-new-training-and-testing-codes-pytorch---18122019" class="anchor" aria-hidden="true" href="#new-training-and-testing-codes-pytorch---18122019"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New training and testing codes (&lt;a href="https://github.com/cszn/KAIR"&gt;PyTorch&lt;/a&gt;) - 18/12/2019&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/cszn/KAIR/blob/master/main_train_dncnn.py"&gt;main_train_dncnn.py&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/cszn/KAIR/blob/master/main_test_dncnn.py"&gt;main_test_dncnn.py&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-training-and-testing-codes-keras-and-pytorch" class="anchor" aria-hidden="true" href="#training-and-testing-codes-keras-and-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training and Testing Codes (&lt;a href="https://keras.io/" rel="nofollow"&gt;Keras&lt;/a&gt; and &lt;a href="https://pytorch.org/" rel="nofollow"&gt;PyTorch&lt;/a&gt;)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/cszn/DnCNN/tree/master/TrainingCodes/dncnn_keras"&gt;DnCNN-keras&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/cszn/DnCNN/tree/master/TrainingCodes/dncnn_pytorch"&gt;DnCNN-pytorch&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-merge-batch-normalization-pytorch" class="anchor" aria-hidden="true" href="#merge-batch-normalization-pytorch"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Merge batch normalization (PyTorch)&lt;/h2&gt;
&lt;div class="highlight highlight-source-python"&gt;&lt;pre&gt;&lt;span class="pl-k"&gt;import&lt;/span&gt; torch
&lt;span class="pl-k"&gt;import&lt;/span&gt; torch.nn &lt;span class="pl-k"&gt;as&lt;/span&gt; nn


&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;merge_bn&lt;/span&gt;(&lt;span class="pl-smi"&gt;model&lt;/span&gt;):
    &lt;span class="pl-s"&gt;&lt;span class="pl-pds"&gt;'''&lt;/span&gt; merge all 'Conv+BN' (or 'TConv+BN') into 'Conv' (or 'TConv')&lt;/span&gt;
&lt;span class="pl-s"&gt;    based on https://github.com/pytorch/pytorch/pull/901&lt;/span&gt;
&lt;span class="pl-s"&gt;    by Kai Zhang (cskaizhang@gmail.com) &lt;/span&gt;
&lt;span class="pl-s"&gt;    https://github.com/cszn/DnCNN&lt;/span&gt;
&lt;span class="pl-s"&gt;    01/01/2019&lt;/span&gt;
&lt;span class="pl-s"&gt;    &lt;span class="pl-pds"&gt;'''&lt;/span&gt;&lt;/span&gt;
    prev_m &lt;span class="pl-k"&gt;=&lt;/span&gt; &lt;span class="pl-c1"&gt;None&lt;/span&gt;
    &lt;span class="pl-k"&gt;for&lt;/span&gt; k, m &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(model.named_children()):
        &lt;span class="pl-k"&gt;if&lt;/span&gt; (&lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(m, nn.BatchNorm2d) &lt;span class="pl-k"&gt;or&lt;/span&gt; &lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(m, nn.BatchNorm1d)) &lt;span class="pl-k"&gt;and&lt;/span&gt; (&lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(prev_m, nn.Conv2d) &lt;span class="pl-k"&gt;or&lt;/span&gt; &lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(prev_m, nn.Linear) &lt;span class="pl-k"&gt;or&lt;/span&gt; &lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(prev_m, nn.ConvTranspose2d)):

            w &lt;span class="pl-k"&gt;=&lt;/span&gt; prev_m.weight.data

            &lt;span class="pl-k"&gt;if&lt;/span&gt; prev_m.bias &lt;span class="pl-k"&gt;is&lt;/span&gt; &lt;span class="pl-c1"&gt;None&lt;/span&gt;:
                zeros &lt;span class="pl-k"&gt;=&lt;/span&gt; torch.Tensor(prev_m.out_channels).zero_().type(w.type())
                prev_m.bias &lt;span class="pl-k"&gt;=&lt;/span&gt; nn.Parameter(zeros)
            b &lt;span class="pl-k"&gt;=&lt;/span&gt; prev_m.bias.data

            invstd &lt;span class="pl-k"&gt;=&lt;/span&gt; m.running_var.clone().add_(m.eps).pow_(&lt;span class="pl-k"&gt;-&lt;/span&gt;&lt;span class="pl-c1"&gt;0.5&lt;/span&gt;)
            &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(prev_m, nn.ConvTranspose2d):
                w.mul_(invstd.view(&lt;span class="pl-c1"&gt;1&lt;/span&gt;, w.size(&lt;span class="pl-c1"&gt;1&lt;/span&gt;), &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;).expand_as(w))
            &lt;span class="pl-k"&gt;else&lt;/span&gt;:
                w.mul_(invstd.view(w.size(&lt;span class="pl-c1"&gt;0&lt;/span&gt;), &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;).expand_as(w))
            b.add_(&lt;span class="pl-k"&gt;-&lt;/span&gt;m.running_mean).mul_(invstd)
            &lt;span class="pl-k"&gt;if&lt;/span&gt; m.affine:
                &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(prev_m, nn.ConvTranspose2d):
                    w.mul_(m.weight.data.view(&lt;span class="pl-c1"&gt;1&lt;/span&gt;, w.size(&lt;span class="pl-c1"&gt;1&lt;/span&gt;), &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;).expand_as(w))
                &lt;span class="pl-k"&gt;else&lt;/span&gt;:
                    w.mul_(m.weight.data.view(w.size(&lt;span class="pl-c1"&gt;0&lt;/span&gt;), &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;, &lt;span class="pl-c1"&gt;1&lt;/span&gt;).expand_as(w))
                b.mul_(m.weight.data).add_(m.bias.data)

            &lt;span class="pl-k"&gt;del&lt;/span&gt; model._modules[k]
        prev_m &lt;span class="pl-k"&gt;=&lt;/span&gt; m
        merge_bn(m)


&lt;span class="pl-k"&gt;def&lt;/span&gt; &lt;span class="pl-en"&gt;tidy_sequential&lt;/span&gt;(&lt;span class="pl-smi"&gt;model&lt;/span&gt;):
    &lt;span class="pl-k"&gt;for&lt;/span&gt; k, m &lt;span class="pl-k"&gt;in&lt;/span&gt; &lt;span class="pl-c1"&gt;list&lt;/span&gt;(model.named_children()):
        &lt;span class="pl-k"&gt;if&lt;/span&gt; &lt;span class="pl-c1"&gt;isinstance&lt;/span&gt;(m, nn.Sequential):
            &lt;span class="pl-k"&gt;if&lt;/span&gt; m.&lt;span class="pl-c1"&gt;__len__&lt;/span&gt;() &lt;span class="pl-k"&gt;==&lt;/span&gt; &lt;span class="pl-c1"&gt;1&lt;/span&gt;:
                model._modules[k] &lt;span class="pl-k"&gt;=&lt;/span&gt; m.&lt;span class="pl-c1"&gt;__getitem__&lt;/span&gt;(&lt;span class="pl-c1"&gt;0&lt;/span&gt;)
        tidy_sequential(m)
&lt;/pre&gt;&lt;/div&gt;
&lt;h1&gt;&lt;a id="user-content-training-matconvnet" class="anchor" aria-hidden="true" href="#training-matconvnet"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training (&lt;a href="http://www.vlfeat.org/matconvnet/" rel="nofollow"&gt;MatConvNet&lt;/a&gt;)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.vlfeat.org/matconvnet/wrappers/" rel="nofollow"&gt;Simplenn&lt;/a&gt; version&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/cszn/DnCNN/tree/master/TrainingCodes/DnCNN_TrainingCodes_v1.1"&gt;DnCNN_TrainingCodes_v1.1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.vlfeat.org/matconvnet/wrappers/" rel="nofollow"&gt;DagNN&lt;/a&gt; version&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/cszn/DnCNN/tree/master/TrainingCodes/DnCNN_TrainingCodes_DagNN_v1.1"&gt;DnCNN_TrainingCodes_DagNN_v1.1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-testing-matconvnet-or-matlab" class="anchor" aria-hidden="true" href="#testing-matconvnet-or-matlab"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing (&lt;a href="http://www.vlfeat.org/matconvnet/" rel="nofollow"&gt;MatConvNet&lt;/a&gt; or Matlab)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[demos]  &lt;code&gt;Demo_test_DnCNN-.m&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[models]  including the trained models for Gaussian denoising; a single model for Gaussian denoising, single image super-resolution (SISR) and deblocking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[testsets]  BSD68 and Set10 for Gaussian denoising evaluation; Set5, Set14, BSD100 and Urban100 datasets for SISR evaluation; Classic5 and LIVE1 for JPEG image deblocking evaluation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-new-fdncnn-models" class="anchor" aria-hidden="true" href="#new-fdncnn-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;New FDnCNN Models&lt;/h1&gt;
&lt;p&gt;I have trained new Flexible DnCNN (FDnCNN) models based on &lt;a href="https://github.com/cszn/FFDNet"&gt;FFDNet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;FDnCNN can handle noise level range of [0, 75] via a single model.&lt;/p&gt;
&lt;p&gt;&lt;a href="Demo_FDnCNN_Gray.m"&gt;Demo_FDnCNN_Gray.m&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="Demo_FDnCNN_Gray_Clip.m"&gt;Demo_FDnCNN_Gray_Clip.m&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="Demo_FDnCNN_Color.m"&gt;Demo_FDnCNN_Color.m&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="Demo_FDnCNN_Color_Clip.m"&gt;Demo_FDnCNN_Color_Clip.m&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-network-architecture-and-design-rationale" class="anchor" aria-hidden="true" href="#network-architecture-and-design-rationale"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Network Architecture and Design Rationale&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Network Architecture&lt;/p&gt;
 &lt;a target="_blank" rel="noopener noreferrer" href="figs/dncnn.png"&gt;&lt;img src="figs/dncnn.png" width="800px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Batch normalization and residual learning are beneficial to Gaussian denoising (especially for a single noise level). The residual of a noisy image corrupted by additive white Gaussian noise (AWGN) follows a constant Gaussian distribution which stablizes batch normalization during training.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Histogram of noisy patches, clean patches, and residual (noise) patches from a batch of training. The noise level is 25, the patch size is 40x40, the batch size is 128.&lt;/li&gt;
&lt;/ul&gt;
 &lt;a target="_blank" rel="noopener noreferrer" href="figs/batch1.png"&gt;&lt;img src="figs/batch1.png" width="800px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Histogram of noisy patches, clean patches, and residual (noise) patches from another batch of training. The noise level is 25, the patch size is 40x40, the batch size is 128.&lt;/li&gt;
&lt;/ul&gt;
   &lt;a target="_blank" rel="noopener noreferrer" href="figs/batch2.png"&gt;&lt;img src="figs/batch2.png" width="800px" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Noise-free image super-resolution does not have this property.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Predicting the residual can be interpreted as performing one gradient descent inference step at starting point (i.e., noisy image).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The parameters in DnCNN are mainly representing the image priors (task-independent), thus it is possible to learn a single model for different tasks, such as image denoising, image super-resolution and JPEG image deblocking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The left is the input image corrupted by different degradations, the right is the restored image by DnCNN-3.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="figs/input.png"&gt;&lt;img src="figs/input.png" width="390px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="figs/output.png"&gt;&lt;img src="figs/output.png" width="390px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a id="user-content-results" class="anchor" aria-hidden="true" href="#results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Results&lt;/h1&gt;
&lt;h2&gt;&lt;a id="user-content-gaussian-denoising" class="anchor" aria-hidden="true" href="#gaussian-denoising"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gaussian Denoising&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The average PSNR(dB) results of different methods on the BSD68 dataset.&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Noise Level&lt;/th&gt;
&lt;th align="center"&gt;BM3D&lt;/th&gt;
&lt;th align="center"&gt;WNNM&lt;/th&gt;
&lt;th align="center"&gt;EPLL&lt;/th&gt;
&lt;th align="center"&gt;MLP&lt;/th&gt;
&lt;th align="center"&gt;CSF&lt;/th&gt;
&lt;th align="center"&gt;TNRD&lt;/th&gt;
&lt;th align="center"&gt;DnCNN&lt;/th&gt;
&lt;th align="center"&gt;DnCNN-B&lt;/th&gt;
&lt;th align="center"&gt;FDnCNN&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;15&lt;/td&gt;
&lt;td align="center"&gt;31.07&lt;/td&gt;
&lt;td align="center"&gt;31.37&lt;/td&gt;
&lt;td align="center"&gt;31.21&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;31.24&lt;/td&gt;
&lt;td align="center"&gt;31.42&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;31.73&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;31.61&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;31.69&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;25&lt;/td&gt;
&lt;td align="center"&gt;28.57&lt;/td&gt;
&lt;td align="center"&gt;28.83&lt;/td&gt;
&lt;td align="center"&gt;28.68&lt;/td&gt;
&lt;td align="center"&gt;28.96&lt;/td&gt;
&lt;td align="center"&gt;28.74&lt;/td&gt;
&lt;td align="center"&gt;28.92&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;29.23&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;29.16&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;29.22&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;50&lt;/td&gt;
&lt;td align="center"&gt;25.62&lt;/td&gt;
&lt;td align="center"&gt;  25.87  &lt;/td&gt;
&lt;td align="center"&gt;25.67&lt;/td&gt;
&lt;td align="center"&gt;26.03&lt;/td&gt;
&lt;td align="center"&gt;   -  &lt;/td&gt;
&lt;td align="center"&gt;25.97&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;26.23&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;26.23&lt;/strong&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;strong&gt;26.27&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Visual Results&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The left is the noisy image corrupted by AWGN, the middle is the denoised image by DnCNN, the right is the ground-truth.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="figs/05_25_noisy.png"&gt;&lt;img src="figs/05_25_noisy.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="figs/05_25.png"&gt;&lt;img src="figs/05_25.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="testsets/Set12/05.png"&gt;&lt;img src="testsets/Set12/05.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="figs/02_25_noisy.png"&gt;&lt;img src="figs/02_25_noisy.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="figs/02_25.png"&gt;&lt;img src="figs/02_25.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="testsets/Set12/02.png"&gt;&lt;img src="testsets/Set12/02.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="figs/102061_noisy.png"&gt;&lt;img src="figs/102061_noisy.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="figs/102061_dncnn.png"&gt;&lt;img src="figs/102061_dncnn.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt; &lt;a target="_blank" rel="noopener noreferrer" href="figs/102061.png"&gt;&lt;img src="figs/102061.png" width="270px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-gaussian-denoising-single-imagesuper-resolution-and-jpeg-image-deblocking-via-a-single-dncnn-3-model" class="anchor" aria-hidden="true" href="#gaussian-denoising-single-imagesuper-resolution-and-jpeg-image-deblocking-via-a-single-dncnn-3-model"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gaussian Denoising, Single ImageSuper-Resolution and JPEG Image Deblocking via a Single (DnCNN-3) Model&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Average PSNR(dB)/SSIM results of different methods for Gaussian denoising with noise level 15, 25 and 50 on BSD68 dataset, single image super-resolution with
upscaling factors 2, 3 and 40 on Set5, Set14, BSD100 and Urban100 datasets, JPEG image deblocking with quality factors 10, 20, 30 and 40 on Classic5 and LIVE11 datasets.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-gaussian-denoising-1" class="anchor" aria-hidden="true" href="#gaussian-denoising-1"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Gaussian Denoising&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Noise Level&lt;/th&gt;
&lt;th align="center"&gt;BM3D&lt;/th&gt;
&lt;th align="center"&gt;TNRD&lt;/th&gt;
&lt;th align="center"&gt;DnCNN-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;15&lt;/td&gt;
&lt;td align="center"&gt;31.08 / 0.8722&lt;/td&gt;
&lt;td align="center"&gt;31.42 / 0.8826&lt;/td&gt;
&lt;td align="center"&gt;31.46 / 0.8826&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;BSD68&lt;/td&gt;
&lt;td align="center"&gt;25&lt;/td&gt;
&lt;td align="center"&gt;28.57 / 0.8017&lt;/td&gt;
&lt;td align="center"&gt;28.92 / 0.8157&lt;/td&gt;
&lt;td align="center"&gt;29.02 / 0.8190&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;50&lt;/td&gt;
&lt;td align="center"&gt;25.62 / 0.6869&lt;/td&gt;
&lt;td align="center"&gt;25.97 / 0.7029&lt;/td&gt;
&lt;td align="center"&gt;26.10 / 0.7076&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-single-image-super-resolution" class="anchor" aria-hidden="true" href="#single-image-super-resolution"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Single Image Super-Resolution&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Upscaling Factor&lt;/th&gt;
&lt;th align="center"&gt;TNRD&lt;/th&gt;
&lt;th align="center"&gt;VDSR&lt;/th&gt;
&lt;th align="center"&gt;DnCNN-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td align="center"&gt;36.86 / 0.9556&lt;/td&gt;
&lt;td align="center"&gt;37.56 / 0.9591&lt;/td&gt;
&lt;td align="center"&gt;37.58 / 0.9590&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Set5&lt;/td&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td align="center"&gt;33.18 / 0.9152&lt;/td&gt;
&lt;td align="center"&gt;33.67 / 0.9220&lt;/td&gt;
&lt;td align="center"&gt;33.75 / 0.9222&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;30.85 / 0.8732&lt;/td&gt;
&lt;td align="center"&gt;31.35 / 0.8845&lt;/td&gt;
&lt;td align="center"&gt;31.40 / 0.8845&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td align="center"&gt;32.51 / 0.9069&lt;/td&gt;
&lt;td align="center"&gt;33.02 / 0.9128&lt;/td&gt;
&lt;td align="center"&gt;33.03 / 0.9128&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Set14&lt;/td&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td align="center"&gt;29.43 / 0.8232&lt;/td&gt;
&lt;td align="center"&gt;29.77 / 0.8318&lt;/td&gt;
&lt;td align="center"&gt;29.81 / 0.8321&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;27.66 / 0.7563&lt;/td&gt;
&lt;td align="center"&gt;27.99 / 0.7659&lt;/td&gt;
&lt;td align="center"&gt;28.04 / 0.7672&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td align="center"&gt;31.40 / 0.8878&lt;/td&gt;
&lt;td align="center"&gt;31.89 / 0.8961&lt;/td&gt;
&lt;td align="center"&gt;31.90 / 0.8961&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;BSD100&lt;/td&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td align="center"&gt;28.50 / 0.7881&lt;/td&gt;
&lt;td align="center"&gt;28.82 / 0.7980&lt;/td&gt;
&lt;td align="center"&gt;28.85 / 0.7981&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;27.00 / 0.7140&lt;/td&gt;
&lt;td align="center"&gt;27.28 / 0.7256&lt;/td&gt;
&lt;td align="center"&gt;27.29 / 0.7253&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;2&lt;/td&gt;
&lt;td align="center"&gt;29.70 / 0.8994&lt;/td&gt;
&lt;td align="center"&gt;30.76 / 0.9143&lt;/td&gt;
&lt;td align="center"&gt;30.74 / 0.9139&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Urban100&lt;/td&gt;
&lt;td align="center"&gt;3&lt;/td&gt;
&lt;td align="center"&gt;26.42 / 0.8076&lt;/td&gt;
&lt;td align="center"&gt;27.13 / 0.8283&lt;/td&gt;
&lt;td align="center"&gt;27.15 / 0.8276&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;24.61 / 0.7291&lt;/td&gt;
&lt;td align="center"&gt;25.17 / 0.7528&lt;/td&gt;
&lt;td align="center"&gt;25.20 / 0.7521&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;&lt;a id="user-content-jpeg-image-deblocking" class="anchor" aria-hidden="true" href="#jpeg-image-deblocking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;JPEG Image Deblocking&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;Dataset&lt;/th&gt;
&lt;th align="center"&gt;Quality Factor&lt;/th&gt;
&lt;th align="center"&gt;AR-CNN&lt;/th&gt;
&lt;th align="center"&gt;TNRD&lt;/th&gt;
&lt;th align="center"&gt;DnCNN-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Classic5&lt;/td&gt;
&lt;td align="center"&gt;10&lt;/td&gt;
&lt;td align="center"&gt;29.03 / 0.7929&lt;/td&gt;
&lt;td align="center"&gt;29.28 / 0.7992&lt;/td&gt;
&lt;td align="center"&gt;29.40 / 0.8026&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;20&lt;/td&gt;
&lt;td align="center"&gt;31.15 / 0.8517&lt;/td&gt;
&lt;td align="center"&gt;31.47 / 0.8576&lt;/td&gt;
&lt;td align="center"&gt;31.63 / 0.8610&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;30&lt;/td&gt;
&lt;td align="center"&gt;32.51 / 0.8806&lt;/td&gt;
&lt;td align="center"&gt;32.78 / 0.8837&lt;/td&gt;
&lt;td align="center"&gt;32.91 / 0.8861&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;40&lt;/td&gt;
&lt;td align="center"&gt;33.34 / 0.8953&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;33.77 / 0.9003&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;LIVE1&lt;/td&gt;
&lt;td align="center"&gt;10&lt;/td&gt;
&lt;td align="center"&gt;28.96 / 0.8076&lt;/td&gt;
&lt;td align="center"&gt;29.15 / 0.8111&lt;/td&gt;
&lt;td align="center"&gt;29.19 / 0.8123&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;20&lt;/td&gt;
&lt;td align="center"&gt;31.29 / 0.8733&lt;/td&gt;
&lt;td align="center"&gt;31.46 / 0.8769&lt;/td&gt;
&lt;td align="center"&gt;31.59 / 0.8802&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;30&lt;/td&gt;
&lt;td align="center"&gt;32.67 / 0.9043&lt;/td&gt;
&lt;td align="center"&gt;32.84 / 0.9059&lt;/td&gt;
&lt;td align="center"&gt;32.98 / 0.9090&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="center"&gt;40&lt;/td&gt;
&lt;td align="center"&gt;33.63 / 0.9198&lt;/td&gt;
&lt;td align="center"&gt;-&lt;/td&gt;
&lt;td align="center"&gt;33.96 / 0.9247&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1&gt;&lt;a id="user-content-requirements-and-dependencies" class="anchor" aria-hidden="true" href="#requirements-and-dependencies"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements and Dependencies&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;MATLAB R2015b&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.nvidia.com/cuda-toolkit-archive" rel="nofollow"&gt;Cuda&lt;/a&gt;-8.0 &amp;amp; &lt;a href="https://developer.nvidia.com/cudnn" rel="nofollow"&gt;cuDNN&lt;/a&gt; v-5.1&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.vlfeat.org/matconvnet/" rel="nofollow"&gt;MatConvNet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;or just MATLAB R2015b to test the model.
&lt;a href="https://github.com/cszn/DnCNN/blob/4a4b5b8bcac5a5ac23433874d4362329b25522ba/Demo_test_DnCNN.m#L64-L65"&gt;https://github.com/cszn/DnCNN/blob/4a4b5b8bcac5a5ac23433874d4362329b25522ba/Demo_test_DnCNN.m#L64-L65&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;@article{zhang2017beyond,
  title={Beyond a {Gaussian} denoiser: Residual learning of deep {CNN} for image denoising},
  author={Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},
  journal={IEEE Transactions on Image Processing},
  year={2017},
  volume={26}, 
  number={7}, 
  pages={3142-3155}, 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;====================================================================&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-convolutional-neural-networks-for-image-denoising-and-restoration" class="anchor" aria-hidden="true" href="#convolutional-neural-networks-for-image-denoising-and-restoration"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="https://link.springer.com/chapter/10.1007/978-3-319-96029-6_4" rel="nofollow"&gt;Convolutional Neural Networks for Image Denoising and Restoration&lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@Inbook{zuo2018convolutional,
author={Zuo, Wangmeng and Zhang, Kai and Zhang, Lei},
editor={Bertalm{\'i}o, Marcelo},
title={Convolutional Neural Networks for Image Denoising and Restoration},
bookTitle={Denoising of Photographic Images and Video: Fundamentals, Open Challenges and New Trends},
year={2018},
publisher={Springer International Publishing},
address={Cham},
pages={93--123},
isbn={978-3-319-96029-6},
doi={10.1007/978-3-319-96029-6_4},
url={https://doi.org/10.1007/978-3-319-96029-6_4}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-challenges-and-possible-solutions-from-the-above-book-chapter" class="anchor" aria-hidden="true" href="#challenges-and-possible-solutions-from-the-above-book-chapter"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Challenges and Possible Solutions (from the above book chapter)&lt;/h3&gt;
&lt;p&gt;While the image denoising for AWGN removal has been well-studied, little work has been done on real image denoising.
The main difficulty arises from the fact that real noises are much more complex than AWGN and it is not an easy task to
thoroughly evaluate the performance of a denoiser. Fig. 4.15 shows four typical noise types in real world.
It can be seen that the characteristics of those noises are very different and a single noise level may be not enough to parameterize those noise types. In most cases, a denoiser can only work well under a certain noise model.
For example, a denoising model trained for AWGN removal is not effective for mixed Gaussian and Poisson noise removal.
This is intuitively reasonable because the CNN-based methods can be treated as general case of Eq. (4.3) and the important data fidelity term corresponds to the degradation process. In spite of this, the image denoising for AWGN removal is valuable due to the following reasons. First, it is an ideal test bed to evaluate the effectiveness of different CNN-based denoising methods.
Second, in the unrolled inference via variable splitting techniques, many image restoration problems can be addressed by sequentially solving a series of Gaussian denoising subproblems, which further broadens the application fields.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="figs/noisetype.png"&gt;&lt;img src="figs/noisetype.png" width="800px" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To improve the practicability of a CNN denoiser, perhaps the most straightforward way is to capture adequate amounts of real noisy-clean training pairs for training so that the real degradation space can be covered. This solution has advantage that there is no need to know the complex degradation process. However, deriving the corresponding clean image of a noisy one is not a trivial task due to the need of careful post-processing steps, such as spatial alignment and illumination correction. Alternatively, one can simulate the real degradation process to synthesize noisy images for a clean one. However, it is not easy to accurately model the complex degradation process. In particular, the noise model can be different across different cameras. Nevertheless, it is practically preferable to roughly model a certain noise type for training and then use the learned CNN model for type-specific denoising.&lt;/p&gt;
&lt;p&gt;Besides the training data, the robust architecture and robust training also play vital roles for the success of a CNN denoiser.
For the robust architecture, designing a deep multiscale CNN which involves a coarse-to-fine procedure is a promising direction.
Such a network is expected to inherit the merits of multiscale:
(i) the noise level decreases at larger scales;
(ii) the ubiquitous low-frequency noise can be alleviated by multiscale procedure;
and (iii) downsampling the image before denoising can effectively enlarge the receptive filed.
For the robust training, the effectiveness of the denoiser trained with generative adversarial networks (GAN) for real image denoising still remains further investigation. The main idea of GAN-based denoising is to introduce an adversarial loss to improve the perceptual quality of denoised image. Besides, a distinctive advantage of GAN is that it can do unsupervised learning. More specifically,
the noisy image without ground truth can be used in the training. So far, we have provided several possible solutions to improve the practicability of a CNN denoiser. We should note that those solutions can be combined to further improve the performance.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>cszn</author><guid isPermaLink="false">https://github.com/cszn/DnCNN</guid><pubDate>Fri, 03 Jan 2020 00:06:00 GMT</pubDate></item></channel></rss>