<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: MATLAB, This month</title><link>https://github.com/trending/matlab?since=monthly</link><description>The top repositories on GitHub for matlab, measured monthly</description><pubDate>Sun, 03 Nov 2019 00:03:04 GMT</pubDate><lastBuildDate>Sun, 03 Nov 2019 00:03:04 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>720</ttl><item><title>PRML/PRMLT #1 in MATLAB, This month</title><link>https://github.com/PRML/PRMLT</link><description>&lt;p&gt;&lt;i&gt;Matlab code for machine learning algorithms in book PRML&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This Matlab package implements machine learning algorithms described in the great textbook:
Pattern Recognition and Machine Learning by C. Bishop (&lt;a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/" rel="nofollow"&gt;PRML&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;It is written purely in Matlab language. It is self-contained. There is no external dependency.&lt;/p&gt;
&lt;p&gt;Note: this package requires Matlab &lt;strong&gt;R2016b&lt;/strong&gt; or latter, since it utilizes a new Matlab syntax called &lt;a href="https://cn.mathworks.com/help/matlab/release-notes.html?rntext=implicit+expansion&amp;amp;startrelease=R2016b&amp;amp;endrelease=R2016b&amp;amp;groupby=release&amp;amp;sortby=descending" rel="nofollow"&gt;Implicit expansion&lt;/a&gt; (a.k.a. broadcasting). It also requires Statistics Toolbox (for some simple random number generator) and Image Processing Toolbox (for reading image data).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-design-goal" class="anchor" aria-hidden="true" href="#design-goal"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Design Goal&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Succinct: The code is extremely compact. Minimizing code length is a major goal. As a result, the core of the algorithms can be easily spotted.&lt;/li&gt;
&lt;li&gt;Efficient: Many tricks to speedup Matlab code are applied (eg. vectorization, matrix factorization, etc.). Usually, functions in this package are orders faster than Matlab builtin ones (e.g. kmeans).&lt;/li&gt;
&lt;li&gt;Robust: Many tricks for numerical stability are applied, such as computing probability in log domain, square root matrix update to enforce matrix symmetry\PD, etc.&lt;/li&gt;
&lt;li&gt;Readable: The code is heavily commented. Corresponding formulas in PRML are annoted. Symbols are in sync with the book.&lt;/li&gt;
&lt;li&gt;Practical: The package is not only readable, but also meant to be easily used and modified to facilitate ML research. Many functions in this package are already widely used (see &lt;a href="http://www.mathworks.com/matlabcentral/fileexchange/?term=authorid%3A49739" rel="nofollow"&gt;Matlab file exchange&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-installation" class="anchor" aria-hidden="true" href="#installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Installation&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Download the package to a local folder (e.g. ~/PRMLT/) by running:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight highlight-text-shell-session"&gt;&lt;pre&gt;&lt;span class="pl-c1"&gt;git clone https://github.com/PRML/PRMLT.git&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;
&lt;p&gt;Run Matlab and navigate to the folder (~/PRMLT/), then run the init.m script.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run some demos in ~/PRMLT/demo folder. Enjoy!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;&lt;a id="user-content-feedback" class="anchor" aria-hidden="true" href="#feedback"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FeedBack&lt;/h2&gt;
&lt;p&gt;If you find any bug or have any suggestion, please do file issues. I am graceful for any feedback and will do my best to improve this package.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h2&gt;
&lt;p&gt;Released under MIT license&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-contact" class="anchor" aria-hidden="true" href="#contact"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contact&lt;/h2&gt;
&lt;p&gt;sth4nth at gmail dot com&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>PRML</author><guid isPermaLink="false">https://github.com/PRML/PRMLT</guid><pubDate>Sun, 03 Nov 2019 00:01:00 GMT</pubDate></item><item><title>TadasBaltrusaitis/OpenFace #2 in MATLAB, This month</title><link>https://github.com/TadasBaltrusaitis/OpenFace</link><description>&lt;p&gt;&lt;i&gt;OpenFace – a state-of-the art tool intended for facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation.&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-openface-220-a-facial-behavior-analysis-toolkit" class="anchor" aria-hidden="true" href="#openface-220-a-facial-behavior-analysis-toolkit"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;OpenFace 2.2.0: a facial behavior analysis toolkit&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/TadasBaltrusaitis/OpenFace" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/4243d567083c3a651ecbadc77c77a00ba697fb40/68747470733a2f2f7472617669732d63692e6f72672f546164617342616c7472757361697469732f4f70656e466163652e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/TadasBaltrusaitis/OpenFace.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://ci.appveyor.com/project/TadasBaltrusaitis/openface/branch/master" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/8c6fb6db38385292d352f1b17205f42469d06fda/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f386d73696b6c786662686c6e736d78702f6272616e63682f6d61737465723f7376673d74727565" alt="Build status" data-canonical-src="https://ci.appveyor.com/api/projects/status/8msiklxfbhlnsmxp/branch/master?svg=true" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Over the past few years, there has been an increased interest in automatic facial behavior analysis
and understanding. We present OpenFace – a tool intended for computer vision and machine learning
researchers, affective computing community and people interested in building interactive
applications based on facial behavior analysis. OpenFace is the ﬁrst toolkit capable of facial
landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation
with available source code for both running and training the models. The computer vision algorithms
which represent the core of OpenFace demonstrate state-of-the-art results in all of the above
mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a
simple webcam without any specialist hardware.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/muticomp_logo_black.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/muticomp_logo_black.png" alt="Multicomp logo" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OpenFace was originally developed by Tadas Baltrušaitis in collaboration with CMU MultiComp Lab led by Prof. Louis-Philippe Morency. Some of the original algorithms were created while at Rainbow Group, Cambridge University. The OpenFace library is still actively developed at the CMU MultiComp Lab in collaboration with Tadas Baltršaitis. Special thanks to researcher who helped developing, implementing and testing the algorithms present in OpenFace: Amir Zadeh and Yao Chong Lim on work on the CE-CLM model and Erroll Wood for the gaze estimation work.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-wiki" class="anchor" aria-hidden="true" href="#wiki"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;WIKI&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;For instructions of how to install/compile/use the project please see &lt;a href="https://github.com/TadasBaltrusaitis/OpenFace/wiki"&gt;WIKI&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-functionality" class="anchor" aria-hidden="true" href="#functionality"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Functionality&lt;/h2&gt;
&lt;p&gt;The system is capable of performing a number of facial analysis tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facial Landmark Detection&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/multi_face_img.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/multi_face_img.png" alt="Sample facial landmark detection image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facial Landmark and head pose tracking (links to YouTube videos)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=V7rV0uy7heQ" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/861e54570c553cb53c797dd8bc57a24f29152655/687474703a2f2f696d672e796f75747562652e636f6d2f76692f56377256307579376865512f302e6a7067" alt="Multiple Face Tracking" width="240" height="180" border="10" data-canonical-src="http://img.youtube.com/vi/V7rV0uy7heQ/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a href="https://www.youtube.com/watch?v=vYOa8Pif5lY" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/a9137826a1740e8df2125ef9b76541445e32957e/687474703a2f2f696d672e796f75747562652e636f6d2f76692f76594f6138506966356c592f302e6a7067" alt="Multiple Face Tracking" width="240" height="180" border="10" data-canonical-src="http://img.youtube.com/vi/vYOa8Pif5lY/0.jpg" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facial Action Unit Recognition&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/au_sample.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/au_sample.png" height="280" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gaze tracking (image of it in action)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/gaze_ex.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/gaze_ex.png" height="182" width="600" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facial Feature Extraction (aligned faces and HOG features)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://github.com/TadasBaltrusaitis/OpenFace/blob/master/imgs/appearance.png"&gt;&lt;img src="https://github.com/TadasBaltrusaitis/OpenFace/raw/master/imgs/appearance.png" alt="Sample aligned face and HOG image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-citation" class="anchor" aria-hidden="true" href="#citation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use any of the resources provided on this page in any of your publications we ask you to cite the following work and the work for a relevant submodule you used.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-overall-system" class="anchor" aria-hidden="true" href="#overall-system"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Overall system&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;OpenFace 2.0: Facial Behavior Analysis Toolkit&lt;/strong&gt;
Tadas Baltrušaitis, Amir Zadeh, Yao Chong Lim, and Louis-Philippe Morency,
&lt;em&gt;IEEE International Conference on Automatic Face and Gesture Recognition&lt;/em&gt;, 2018&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-facial-landmark-detection-and-tracking" class="anchor" aria-hidden="true" href="#facial-landmark-detection-and-tracking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Facial landmark detection and tracking&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Convolutional experts constrained local model for facial landmark detection&lt;/strong&gt;
A. Zadeh, T. Baltrušaitis, and Louis-Philippe Morency.
&lt;em&gt;Computer Vision and Pattern Recognition Workshops&lt;/em&gt;, 2017&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Constrained Local Neural Fields for robust facial landmark detection in the wild&lt;/strong&gt;
Tadas Baltrušaitis, Peter Robinson, and Louis-Philippe Morency.
in IEEE Int. &lt;em&gt;Conference on Computer Vision Workshops, 300 Faces in-the-Wild Challenge&lt;/em&gt;, 2013.&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-eye-gaze-tracking" class="anchor" aria-hidden="true" href="#eye-gaze-tracking"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Eye gaze tracking&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Rendering of Eyes for Eye-Shape Registration and Gaze Estimation&lt;/strong&gt;
Erroll Wood, Tadas Baltrušaitis, Xucong Zhang, Yusuke Sugano, Peter Robinson, and Andreas Bulling
in &lt;em&gt;IEEE International Conference on Computer Vision (ICCV)&lt;/em&gt;, 2015&lt;/p&gt;
&lt;h4&gt;&lt;a id="user-content-facial-action-unit-detection" class="anchor" aria-hidden="true" href="#facial-action-unit-detection"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Facial Action Unit detection&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Cross-dataset learning and person-specific normalisation for automatic Action Unit detection&lt;/strong&gt;
Tadas Baltrušaitis, Marwa Mahmoud, and Peter Robinson
in &lt;em&gt;Facial Expression Recognition and Analysis Challenge&lt;/em&gt;,
&lt;em&gt;IEEE International Conference on Automatic Face and Gesture Recognition&lt;/em&gt;, 2015&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-commercial-license" class="anchor" aria-hidden="true" href="#commercial-license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Commercial license&lt;/h1&gt;
&lt;p&gt;For inquiries about the commercial licensing of the OpenFace toolkit please visit &lt;a href="https://www.flintbox.com/public/project/50632/" rel="nofollow"&gt;https://www.flintbox.com/public/project/50632/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-final-remarks" class="anchor" aria-hidden="true" href="#final-remarks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Final remarks&lt;/h1&gt;
&lt;p&gt;I did my best to make sure that the code runs out of the box but there are always issues and I would be grateful for your understanding that this is research code and a research project. If you encounter any problems/bugs/issues please contact me on github or by emailing me at &lt;a href="mailto:tadyla@gmail.com"&gt;tadyla@gmail.com&lt;/a&gt; for any bug reports/questions/suggestions. I prefer questions and bug reports on github as that provides visibility to others who might be encountering same issues or who have the same questions.&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-copyright" class="anchor" aria-hidden="true" href="#copyright"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Copyright&lt;/h1&gt;
&lt;p&gt;Copyright can be found in the Copyright.txt&lt;/p&gt;
&lt;p&gt;You have to respect dlib, OpenBLAS, and OpenCV licenses.&lt;/p&gt;
&lt;p&gt;Furthermore you have to respect the licenses of the datasets used for model training - &lt;a href="https://github.com/TadasBaltrusaitis/OpenFace/wiki/Datasets"&gt;https://github.com/TadasBaltrusaitis/OpenFace/wiki/Datasets&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>TadasBaltrusaitis</author><guid isPermaLink="false">https://github.com/TadasBaltrusaitis/OpenFace</guid><pubDate>Sun, 03 Nov 2019 00:02:00 GMT</pubDate></item><item><title>ShaoqingRen/faster_rcnn #3 in MATLAB, This month</title><link>https://github.com/ShaoqingRen/faster_rcnn</link><description>&lt;p&gt;&lt;i&gt;Faster R-CNN&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h2&gt;&lt;a id="user-content-this-repo-has-been-deprecated-please-see-detectron-which-includes-an-implementation-of-mask-r-cnn" class="anchor" aria-hidden="true" href="#this-repo-has-been-deprecated-please-see-detectron-which-includes-an-implementation-of-mask-r-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;This repo has been deprecated. Please see &lt;a href="https://github.com/facebookresearch/Detectron"&gt;Detectron&lt;/a&gt;, which includes an implementation of &lt;a href="https://arxiv.org/abs/1703.06870" rel="nofollow"&gt;Mask R-CNN&lt;/a&gt;.&lt;/h2&gt;
&lt;h1&gt;&lt;a id="user-content-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks" class="anchor" aria-hidden="true" href="#faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;em&gt;Faster&lt;/em&gt; R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/h1&gt;
&lt;p&gt;By Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun at Microsoft Research&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Faster&lt;/strong&gt; R-CNN is an object detection framework based on deep convolutional networks, which includes a Region Proposal Network (RPN) and an Object Detection Network. Both networks are trained for sharing convolutional layers for fast testing.&lt;/p&gt;
&lt;p&gt;Faster R-CNN was initially described in an &lt;a href="http://arxiv.org/abs/1506.01497" rel="nofollow"&gt;arXiv tech report&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This repo contains a MATLAB re-implementation of Fast R-CNN. Details about Fast R-CNN are in: &lt;a href="https://github.com/rbgirshick/fast-rcnn"&gt;rbgirshick/fast-rcnn&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This code has been tested on Windows 7/8 64-bit, Windows Server 2012 R2, and Linux, and on MATLAB 2014a.&lt;/p&gt;
&lt;p&gt;Python version is available at &lt;a href="https://github.com/rbgirshick/py-faster-rcnn"&gt;py-faster-rcnn&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-license" class="anchor" aria-hidden="true" href="#license"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;License&lt;/h3&gt;
&lt;p&gt;Faster R-CNN is released under the MIT License (refer to the LICENSE file for details).&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-citing-faster-r-cnn" class="anchor" aria-hidden="true" href="#citing-faster-r-cnn"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Citing Faster R-CNN&lt;/h3&gt;
&lt;p&gt;If you find Faster R-CNN useful in your research, please consider citing:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{ren15fasterrcnn,
    Author = {Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun},
    Title = {{Faster R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
    Journal = {arXiv preprint arXiv:1506.01497},
    Year = {2015}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="user-content-main-results" class="anchor" aria-hidden="true" href="#main-results"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Main Results&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align="center"&gt;training data&lt;/th&gt;
&lt;th align="center"&gt;test data&lt;/th&gt;
&lt;th align="center"&gt;mAP&lt;/th&gt;
&lt;th align="center"&gt;time/img&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Faster RCNN, VGG-16&lt;/td&gt;
&lt;td align="center"&gt;VOC 2007 trainval&lt;/td&gt;
&lt;td align="center"&gt;VOC 2007 test&lt;/td&gt;
&lt;td align="center"&gt;69.9%&lt;/td&gt;
&lt;td align="center"&gt;198ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Faster RCNN, VGG-16&lt;/td&gt;
&lt;td align="center"&gt;VOC 2007 trainval + 2012 trainval&lt;/td&gt;
&lt;td align="center"&gt;VOC 2007 test&lt;/td&gt;
&lt;td align="center"&gt;73.2%&lt;/td&gt;
&lt;td align="center"&gt;198ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Faster RCNN, VGG-16&lt;/td&gt;
&lt;td align="center"&gt;VOC 2012 trainval&lt;/td&gt;
&lt;td align="center"&gt;VOC 2012 test&lt;/td&gt;
&lt;td align="center"&gt;67.0%&lt;/td&gt;
&lt;td align="center"&gt;198ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Faster RCNN, VGG-16&lt;/td&gt;
&lt;td align="center"&gt;VOC 2007 trainval&amp;amp;test + 2012 trainval&lt;/td&gt;
&lt;td align="center"&gt;VOC 2012 test&lt;/td&gt;
&lt;td align="center"&gt;70.4%&lt;/td&gt;
&lt;td align="center"&gt;198ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The mAP results are subject to random variations. We have run 5 times independently for ZF net, and the mAPs are 59.9 (as in the paper), 60.4, 59.5, 60.1, and 59.5, with a mean of 59.88 and std 0.39.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-contents" class="anchor" aria-hidden="true" href="#contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contents&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;&lt;a href="#requirements-software"&gt;Requirements: software&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#requirements-hardware"&gt;Requirements: hardware&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#preparation-for-testing"&gt;Preparation for Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#testing-demo"&gt;Testing Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#preparation-for-training"&gt;Preparation for Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#training"&gt;Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-requirements-software" class="anchor" aria-hidden="true" href="#requirements-software"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements: software&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;&lt;code&gt;Caffe&lt;/code&gt; build for Faster R-CNN (included in this repository, see &lt;code&gt;external/caffe&lt;/code&gt;)
&lt;ul&gt;
&lt;li&gt;If you are using Windows, you may download a compiled mex file by running &lt;code&gt;fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If you are using Linux or you want to compile for Windows, please follow the &lt;a href="https://github.com/ShaoqingRen/caffe/tree/faster-R-CNN"&gt;instructions&lt;/a&gt; on our Caffe branch.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MATLAB&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-requirements-hardware" class="anchor" aria-hidden="true" href="#requirements-hardware"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Requirements: hardware&lt;/h3&gt;
&lt;p&gt;GPU: Titan, Titan Black, Titan X, K20, K40, K80.&lt;/p&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Region Proposal Network (RPN)
&lt;ul&gt;
&lt;li&gt;2GB GPU memory for ZF net&lt;/li&gt;
&lt;li&gt;5GB GPU memory for VGG-16 net&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Object Detection Network (Fast R-CNN)
&lt;ul&gt;
&lt;li&gt;3GB GPU memory for ZF net&lt;/li&gt;
&lt;li&gt;8GB GPU memory for VGG-16 net&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-preparation-for-testing" class="anchor" aria-hidden="true" href="#preparation-for-testing"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preparation for Testing:&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Run &lt;code&gt;fetch_data/fetch_caffe_mex_windows_vs2013_cuda65.m&lt;/code&gt; to download a compiled Caffe mex (for Windows only).&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;faster_rcnn_build.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;startup.m&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-testing-demo" class="anchor" aria-hidden="true" href="#testing-demo"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Testing Demo:&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;fetch_data/fetch_faster_rcnn_final_model.m&lt;/code&gt; to download our trained models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;experiments/script_faster_rcnn_demo.m&lt;/code&gt; to test a single demo image.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You will see the timing information as below. We get the following running time on K40 @ 875 MHz and Intel Xeon CPU E5-2650 v2 @ 2.60GHz for the demo images with VGG-16:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;001763.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.201s (resize+conv+proposal: 0.150s, nms+regionwise: 0.052s)
004545.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.201s (resize+conv+proposal: 0.151s, nms+regionwise: 0.050s)
000542.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.192s (resize+conv+proposal: 0.151s, nms+regionwise: 0.041s)
000456.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.202s (resize+conv+proposal: 0.152s, nms+regionwise: 0.050s)
001150.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.194s (resize+conv+proposal: 0.151s, nms+regionwise: 0.043s)
mean time: 0.198s&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and with ZF net:&lt;/p&gt;
&lt;div class="highlight highlight-source-shell"&gt;&lt;pre&gt;001763.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.061s (resize+conv+proposal: 0.032s, nms+regionwise: 0.029s)
004545.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.063s (resize+conv+proposal: 0.034s, nms+regionwise: 0.029s)
000542.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.052s (resize+conv+proposal: 0.034s, nms+regionwise: 0.018s)
000456.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.062s (resize+conv+proposal: 0.034s, nms+regionwise: 0.028s)
001150.jpg (500x375): &lt;span class="pl-k"&gt;time&lt;/span&gt; 0.058s (resize+conv+proposal: 0.034s, nms+regionwise: 0.023s)
mean time: 0.059s&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;The visual results might be different from those in the paper due to numerical variations.&lt;/li&gt;
&lt;li&gt;Running time on other GPUs&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;GPU / mean time&lt;/th&gt;
&lt;th align="center"&gt;VGG-16&lt;/th&gt;
&lt;th align="center"&gt;ZF&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;K40&lt;/td&gt;
&lt;td align="center"&gt;198ms&lt;/td&gt;
&lt;td align="center"&gt;59ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Titan Black&lt;/td&gt;
&lt;td align="center"&gt;174ms&lt;/td&gt;
&lt;td align="center"&gt;56ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Titan X&lt;/td&gt;
&lt;td align="center"&gt;151ms&lt;/td&gt;
&lt;td align="center"&gt;59ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-preparation-for-training" class="anchor" aria-hidden="true" href="#preparation-for-training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Preparation for Training:&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Run &lt;code&gt;fetch_data/fetch_model_ZF.m&lt;/code&gt; to download an ImageNet-pre-trained ZF net.&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;fetch_data/fetch_model_VGG16.m&lt;/code&gt; to download an ImageNet-pre-trained VGG-16 net.&lt;/li&gt;
&lt;li&gt;Download VOC 2007 and 2012 data to ./datasets&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-training" class="anchor" aria-hidden="true" href="#training"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training:&lt;/h3&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Run &lt;code&gt;experiments/script_faster_rcnn_VOC2007_ZF.m&lt;/code&gt; to train a model with ZF net. It runs four steps as follows:
&lt;ul&gt;
&lt;li&gt;Train RPN with conv layers tuned; compute RPN results on the train/test sets.&lt;/li&gt;
&lt;li&gt;Train Fast R-CNN with conv layers tuned using step-1 RPN proposals; evaluate detection mAP.&lt;/li&gt;
&lt;li&gt;Train RPN with conv layers fixed; compute RPN results on the train/test sets.&lt;/li&gt;
&lt;li&gt;Train Fast R-CNN with conv layers fixed using step-3 RPN proposals; evaluate detection mAP.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: the entire training time is ~12 hours on K40.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;experiments/script_faster_rcnn_VOC2007_VGG16.m&lt;/code&gt; to train a model with VGG net.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: the entire training time is ~2 days on K40.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Check other scripts in &lt;code&gt;./experiments&lt;/code&gt; for more settings.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;a id="user-content-resources" class="anchor" aria-hidden="true" href="#resources"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This documentation may contain links to third party websites, which are provided for your convenience only. Such third party websites are not under Microsoft’s control. Microsoft does not endorse or make any representation, guarantee or assurance regarding any third party website, content, service or product. Third party websites may be subject to the third party’s terms, conditions, and privacy statements.&lt;/p&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Experiment logs: &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!110&amp;amp;authkey=!ACpgYZR2MmfklwI&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="https://www.dropbox.com/s/wu841r7zmebjp6r/faster_rcnn_logs.zip?dl=0" rel="nofollow"&gt;DropBox&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1ntJ3dLv" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Regions proposals of our trained RPN:
&lt;ul&gt;
&lt;li&gt;ZF net trained on VOC 07 trainval &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!115&amp;amp;authkey=!AJJMrFJHKLXIg5c&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1pKGBDyz" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ZF net trained on VOC 07/12 trainval &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!117&amp;amp;authkey=!AJiy5F6Cum1iosI&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1jGAgkZW" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VGG net trained on VOC 07 trainval &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!116&amp;amp;authkey=!AH4Zi_KAaun7MhQ&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1qWHv4JU" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VGG net trained on VOC 07/12 trainval &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!118&amp;amp;authkey=!AB_lKk3dbGyr1-I&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1c0fQpqg" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: the proposals are in the format of [left, top, right, bottom, confidence]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the automatic "fetch_data" fails, you may manually download resouces from:&lt;/p&gt;
&lt;ol start="0"&gt;
&lt;li&gt;Pre-complied caffe mex:
&lt;ul&gt;
&lt;li&gt;Windows-based mex complied with VS2013 and Cuda6.5: &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!111&amp;amp;authkey=!AFVWFGTbViiX5tg&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="https://www.dropbox.com/s/m6sg347tiaqpcwy/caffe_mex.zip?dl=0" rel="nofollow"&gt;DropBox&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1i3m0i0H" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ImageNet-pretrained networks:
&lt;ul&gt;
&lt;li&gt;Zeiler &amp;amp; Fergus (ZF) net &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!113&amp;amp;authkey=!AIzdm0sD_SmhUQ4&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="https://www.dropbox.com/s/sw58b2froihzwyf/model_ZF.zip?dl=0" rel="nofollow"&gt;DropBox&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1o6zipPS" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VGG-16 net &lt;a href="https://onedrive.live.com/download?resid=36FEC490FBC32F1A!114&amp;amp;authkey=!AE8uV9B07dREbhM&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="https://www.dropbox.com/s/z5rrji25uskha73/model_VGG16.zip?dl=0" rel="nofollow"&gt;DropBox&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1mgzSnI4" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Final RPN+FastRCNN models: &lt;a href="https://onedrive.live.com/download?resid=D7AF52BADBA8A4BC!114&amp;amp;authkey=!AERHoxZ-iAx_j34&amp;amp;ithint=file%2czip" rel="nofollow"&gt;OneDrive&lt;/a&gt;, &lt;a href="https://www.dropbox.com/s/jswrnkaln47clg2/faster_rcnn_final_model.zip?dl=0" rel="nofollow"&gt;DropBox&lt;/a&gt;, &lt;a href="http://pan.baidu.com/s/1hsFKmeK" rel="nofollow"&gt;BaiduYun&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>ShaoqingRen</author><guid isPermaLink="false">https://github.com/ShaoqingRen/faster_rcnn</guid><pubDate>Sun, 03 Nov 2019 00:03:00 GMT</pubDate></item></channel></rss>