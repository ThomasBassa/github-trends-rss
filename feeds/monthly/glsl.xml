<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>GitHub Trending: GLSL, This month</title><link>https://github.com/trending/glsl?since=monthly</link><description>The top repositories on GitHub for glsl, measured monthly</description><pubDate>Wed, 30 Oct 2019 00:04:38 GMT</pubDate><lastBuildDate>Wed, 30 Oct 2019 00:04:38 GMT</lastBuildDate><generator>PyRSS2Gen-1.1.0</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><ttl>1400</ttl><item><title>bloc97/Anime4K #1 in GLSL, This month</title><link>https://github.com/bloc97/Anime4K</link><description>&lt;p&gt;&lt;i&gt;A High-Quality Real Time Upscaler for Anime Video&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;h1&gt;&lt;a id="user-content-anime4k" class="anchor" aria-hidden="true" href="#anime4k"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Anime4K&lt;/h1&gt;
&lt;p&gt;Anime4K is a state-of-the-art*, open-source, high-quality real-time anime upscaling algorithm that can be implemented in any programming language.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="results/Main.png?raw=true"&gt;&lt;img src="results/Main.png?raw=true" alt="Thumbnail Image" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;*State of the art as of August 2019 in the real-time anime 4K upscaling category, the fastest at achieving reasonable quality. We do not claim this is a superior quality general purpose SISR algorithm compared to machine learning approaches.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Disclaimer: All art assets used are for demonstration and educational purposes. All rights are reserved to their original owners. If you (as a person or a company) own the art and do not wish it to be associated with this project, please contact us at 	&lt;a href="mailto:anime4k.upscale@gmail.com"&gt;anime4k.upscale@gmail.com&lt;/a&gt; and we will gladly take it down.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="results/Comparisons/1_time.png?raw=true"&gt;&lt;img src="results/Comparisons/1_time.png?raw=true" alt="Comparison" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-notice" class="anchor" aria-hidden="true" href="#notice"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Notice&lt;/h2&gt;
&lt;p&gt;We understand that this algorithm is far from perfect, and are working towards a hybrid approach (using Machine Learning) to improve Anime4K.&lt;/p&gt;
&lt;p&gt;The greatest difficulties encountered right now are caused by these issues that other media does not suffer from:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lack of ground truth (No True 4K Anime)&lt;/li&gt;
&lt;li&gt;Few true 1080p anime (Even some anime mastered at 1080p have sprites that were downsampled)&lt;/li&gt;
&lt;li&gt;Non-1080p anime are upsampled to 1080p using simple algorithms, resulting in a blurry 1080p image. Our algorithm has to detect this. (Main reason why waifu2x does not work well on anime)&lt;/li&gt;
&lt;li&gt;UV channels of anime are subsampled (4:2:0), which means the color channels of 1080p anime are in fact 540p, thus there is a lack of 1080p ground truth for the UV channels.&lt;/li&gt;
&lt;li&gt;Simulating H.264/H.265 compression artifacts (for analysis and denoising) is not trivial and is relatively time-consuming.&lt;/li&gt;
&lt;li&gt;Due to the workflow of animation studios and their lack of time/budget, resampling artifacts of individual sprites are present in many modern anime.&lt;/li&gt;
&lt;li&gt;Speed (preferably real-time) is paramount, since we do not want to re-encode video each time the algorithm improves. There is also less risk of permanently altering original content.&lt;/li&gt;
&lt;li&gt;So on...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, we still believe by shrinking the size of VDSR or FSRCNN and using an hybrid approach we can achieve good results.&lt;br&gt;
Stay tuned for more info!&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-v10-release-candidate-2" class="anchor" aria-hidden="true" href="#v10-release-candidate-2"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;v1.0 Release Candidate 2&lt;/h2&gt;
&lt;p&gt;Improved speed.&lt;/p&gt;
&lt;p&gt;Performance is back on par with v0.9 Beta, with only insignificant loss in quality compared to v1.0 RC1. (3ms on RX Vega 64)&lt;/p&gt;
&lt;p&gt;Two more versions are included for less powerful GPUs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Anime4K_Fast (1.5ms)&lt;/li&gt;
&lt;li&gt;Anime4K_UltraFast (1ms) (For potato PCs)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/1.0/RC2_Comparison.png"&gt;&lt;img src="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/1.0/RC2_Comparison.png" alt="ComparisonRC" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;em&gt;Please view in full size on a 4K display for a correct comparison.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-v10-release-candidate" class="anchor" aria-hidden="true" href="#v10-release-candidate"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;v1.0 Release Candidate&lt;/h2&gt;
&lt;p&gt;Reduced texture loss, aliasing and banding in Anime4K v1.0 RC at the cost of performance. It now takes 6ms. +2ms for line detection and +1ms for line targeted FXAA.&lt;/p&gt;
&lt;p&gt;What's new:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A line detection algorithm.&lt;/li&gt;
&lt;li&gt;Gradient maximization is only applied near lines using the line detector, instead of indiscriminately affecting the entire image. This has the effect of ignoring textures and out of focus elements.&lt;/li&gt;
&lt;li&gt;Finally, one iteration of targeted FXAA is applied on the lines using the line detector to reduce aliasing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/0_RC.png"&gt;&lt;img src="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/0_RC.png" alt="ComparisonRC" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/1_RC.png"&gt;&lt;img src="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/1_RC.png" alt="ComparisonRC" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/2_RC.png"&gt;&lt;img src="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/2_RC.png" alt="ComparisonRC" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/3_RC.png"&gt;&lt;img src="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/0.9-1.0/3_RC.png" alt="ComparisonRC" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-hlsl-usage-instructions-mpc-be-with-madvr" class="anchor" aria-hidden="true" href="#hlsl-usage-instructions-mpc-be-with-madvr"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;HLSL Usage Instructions (MPC-BE with madVR)&lt;/h2&gt;
&lt;p&gt;This implementation is &lt;strong&gt;only for Windows&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-hlsl-installation" class="anchor" aria-hidden="true" href="#hlsl-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="HLSL_Instructions.md"&gt;HLSL Installation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Note for developers: For performance, the HLSL shaders use the Alpha channel to store the gradient. You might need to make a backup of the alpha channel before applying these shaders and restore it after if your rendering engine uses the alpha channel for other purposes. (In MPC-BE's case, it gets ignored.)&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-glsl-usage-instructions-mpv" class="anchor" aria-hidden="true" href="#glsl-usage-instructions-mpv"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;GLSL Usage Instructions (MPV)&lt;/h2&gt;
&lt;p&gt;This implementation is &lt;strong&gt;cross platform&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-glsl-installation" class="anchor" aria-hidden="true" href="#glsl-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="GLSL_Instructions.md"&gt;GLSL Installation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Note for developers: For performance, the GLSL shaders use the &lt;code&gt;POSTKERNEL&lt;/code&gt; texture to store the gradient. You might need to make a backup of the &lt;code&gt;POSTKERNEL&lt;/code&gt; texture before applying these shaders and restore it after if your other shaders or rendering engine uses the &lt;code&gt;POSTKERNEL&lt;/code&gt; texture for other purposes. (In MPV's case, it gets ignored.)&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-java-usage-instructions-standalone" class="anchor" aria-hidden="true" href="#java-usage-instructions-standalone"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Java Usage Instructions (Standalone)&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-java-installation" class="anchor" aria-hidden="true" href="#java-installation"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Java_Instructions.md"&gt;Java Installation&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Click on the link above to read Java version installation and usage instructions.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-projects-that-use-anime4k" class="anchor" aria-hidden="true" href="#projects-that-use-anime4k"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Projects that use Anime4K&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/yeataro/TD-Anime4K"&gt;https://github.com/yeataro/TD-Anime4K&lt;/a&gt; (Anime4K for TouchDesigner)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/keijiro/UnityAnime4K"&gt;https://github.com/keijiro/UnityAnime4K&lt;/a&gt; (Anime4K for Unity)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/net2cn/Anime4KSharp"&gt;https://github.com/net2cn/Anime4KSharp&lt;/a&gt; (Anime4K Re-Implemented in C#)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/k4yt3x/video2x"&gt;https://github.com/k4yt3x/video2x&lt;/a&gt; (Anime Video Upscaling Pipeline)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-pseudo-preprint-preview" class="anchor" aria-hidden="true" href="#pseudo-preprint-preview"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Pseudo-Preprint Preview&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-read-full-pseudo-preprint" class="anchor" aria-hidden="true" href="#read-full-pseudo-preprint"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;a href="Preprint.md"&gt;Read Full Pseudo-Preprint&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;B. Peng&lt;br&gt;
August 2019&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ad perpetuam memoriam of all who perished in the Kyoto Animation arson attack.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Table of Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="Preprint.md#abstract"&gt;Abstract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Preprint.md#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Preprint.md#proposed-method"&gt;Proposed Method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Preprint.md#results"&gt;Results and Upscale Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Preprint.md#discussion"&gt;Discussion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="Preprint.md#analysis"&gt;Analysis and Comparison to Other Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="user-content-abstract" class="anchor" aria-hidden="true" href="#abstract"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Abstract&lt;/h3&gt;
&lt;p&gt;We present a state-of-the-art high-quality real-time SISR algorithm designed to work with Japanese animation and cartoons that is extremely fast &lt;em&gt;(~3ms with Vega 64 GPU)&lt;/em&gt;, temporally coherent, simple to implement &lt;em&gt;(~100 lines of code)&lt;/em&gt;, yet very effective. We find it surprising that this method is not currently used 'en masse', since the intuition leading us to this algorithm is very straightforward.&lt;br&gt;
Remarkably, the proposed method does not use any machine-learning or statistical approach, and is tailored to content that puts importance to well defined lines/edges while tolerates a sacrifice of the finer textures. The proposed algorithm can be quickly described as an iterative algorithm that treats color information as a heightmap and 'pushes' pixels towards probable edges using gradient-ascent. This is very likely what learning-based approaches are already doing under the hood (eg. VDSR&lt;sup&gt;&lt;strong&gt;[1]&lt;/strong&gt;&lt;/sup&gt;, waifu2x&lt;sup&gt;&lt;strong&gt;[2]&lt;/strong&gt;&lt;/sup&gt;).&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-faq" class="anchor" aria-hidden="true" href="#faq"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;FAQ&lt;/h2&gt;
&lt;h3&gt;&lt;a id="user-content-why-not-just-use-waifu2x" class="anchor" aria-hidden="true" href="#why-not-just-use-waifu2x"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why not just use waifu2x&lt;/h3&gt;
&lt;p&gt;waifu2x is too slow for real time applications.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-why-not-just-use-madvr-with-ngu" class="anchor" aria-hidden="true" href="#why-not-just-use-madvr-with-ngu"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why not just use madVR with NGU&lt;/h3&gt;
&lt;p&gt;NGU is proprietary, this algorithm is licensed under MIT.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-how-does-fsrcnnx-compare-to-this" class="anchor" aria-hidden="true" href="#how-does-fsrcnnx-compare-to-this"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;How does FSRCNNX compare to this&lt;/h3&gt;
&lt;p&gt;Since it performs poorly (perceptually, for anime) compared to other algorithms, it was left out of our visual comparisons.&lt;/p&gt;
&lt;p&gt;&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/FSRCNNX.png"&gt;&lt;img src="https://raw.githubusercontent.com/bloc97/Anime4K/master/results/Comparisons/FSRCNNX.png" alt="ComparisonRC" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: FSRCNNX was not specifically trained/designed for anime. It is however a good general-purpose SISR algorithm for video.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-where-are-the-psnrssim-metrics" class="anchor" aria-hidden="true" href="#where-are-the-psnrssim-metrics"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Where are the PSNR/SSIM metrics&lt;/h3&gt;
&lt;p&gt;There are no ground truths of 4K anime.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-why-not-do-psnrssim-on-480p-720p-upscaling" class="anchor" aria-hidden="true" href="#why-not-do-psnrssim-on-480p-720p-upscaling"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Why not do PSNR/SSIM on 480p-&amp;gt;720p upscaling&lt;/h3&gt;
&lt;p&gt;&lt;a href="FAQ_Detail.md"&gt;Story Time&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comparing PSNR/SSIM on 480p-&amp;gt;720p upscales does not prove and is not a good indicator of 1080p-&amp;gt;2160p upscaling quality. (Eg. poor performance of waifu2x on 1080p anime) 480p anime images have a lot of high frequency information (lines might be thinner than 1 pixel), while 1080p anime images have a lot of redundant information. 1080p-&amp;gt;2160p upscaling on anime is thus objectively easier than 480p-&amp;gt;720p.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-i-think-the-results-are-worse-than-x" class="anchor" aria-hidden="true" href="#i-think-the-results-are-worse-than-x"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;I think the results are worse than &amp;lt;x&amp;gt;&lt;/h3&gt;
&lt;p&gt;Surely some people like sharper edges, some like softer ones. Do try it yourself on a few anime before reaching a definite conclusion. People &lt;em&gt;tend&lt;/em&gt; to prefer sharper edges. Also, seeing the comparisons on a 1080p screen is not representative of the final results on a 4K screen, the pixel density and sharpness of the final image is simply not comparable.&lt;/p&gt;
&lt;h3&gt;&lt;a id="user-content-note-for-those-who-think-this-is-not-a-upscaling-algorithm" class="anchor" aria-hidden="true" href="#note-for-those-who-think-this-is-not-a-upscaling-algorithm"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Note for those who think this is not a 'upscaling' algorithm.&lt;/h3&gt;
&lt;p&gt;&lt;a href="Upscaling.md"&gt;Explanation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TL;DR&lt;/p&gt;
&lt;p&gt;Sharpening, De-Blurring and Super-Resolution are equivalent.&lt;br&gt;
Anime4K can de-blur, and is equivalent to a SR algorithm.&lt;br&gt;
A Super-Resolution algorithm can do upscaling.&lt;br&gt;
Thus, Anime4K is an upscaling algorithm.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>bloc97</author><guid isPermaLink="false">https://github.com/bloc97/Anime4K</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item><item><title>KhronosGroup/glTF-Sample-Models #2 in GLSL, This month</title><link>https://github.com/KhronosGroup/glTF-Sample-Models</link><description>&lt;p&gt;&lt;i&gt;glTF Sample Models&lt;/i&gt;&lt;/p&gt;&lt;div id="readme" class="instapaper_body md" data-path="README.md"&gt;&lt;article class="markdown-body entry-content p-5" itemprop="text"&gt;&lt;p align="center"&gt;
&lt;a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/KhronosGroup/glTF/master/specification/figures/gltf.png"&gt;&lt;img src="https://raw.githubusercontent.com/KhronosGroup/glTF/master/specification/figures/gltf.png" style="max-width:100%;"&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;h1&gt;&lt;a id="user-content-gltf-sample-models" class="anchor" aria-hidden="true" href="#gltf-sample-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;glTF Sample Models&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://travis-ci.org/KhronosGroup/glTF-Sample-Models" rel="nofollow"&gt;&lt;img src="https://camo.githubusercontent.com/edc8da9ed6c33b2ae384f02508b06bd45b475e50/68747470733a2f2f7472617669732d63692e6f72672f4b68726f6e6f7347726f75702f676c54462d53616d706c652d4d6f64656c732e7376673f6272616e63683d6d6173746572" alt="Build Status" data-canonical-src="https://travis-ci.org/KhronosGroup/glTF-Sample-Models.svg?branch=master" style="max-width:100%;"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="1.0"&gt;glTF 1.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="2.0"&gt;glTF 2.0&lt;/a&gt; - Click for full list.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sample glTF 2.0 models are provided in one or more of the following forms of glTF:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;glTF (&lt;code&gt;.gltf&lt;/code&gt;) with separate resources: &lt;code&gt;.bin&lt;/code&gt; (geometry, animation, skins) and &lt;code&gt;.jpg&lt;/code&gt; or &lt;code&gt;.png&lt;/code&gt; image files.  The supporting files are easily examined when separated like this, but must be kept together with the parent glTF file for the model to work.&lt;/li&gt;
&lt;li&gt;glTF (&lt;code&gt;.gltf&lt;/code&gt;) with embedded resources (as Data URIs).  This form tends to be larger than the others, but Data URIs do have their uses.&lt;/li&gt;
&lt;li&gt;Binary glTF (&lt;code&gt;.glb&lt;/code&gt;) using the &lt;a href="https://github.com/KhronosGroup/glTF/blob/master/specification/2.0/README.md#glb-file-format-specification"&gt;binary container format&lt;/a&gt;.  These are easily shared due to the bundling of all the textures and mesh data into a single file.&lt;/li&gt;
&lt;li&gt;glTF (&lt;code&gt;.gltf&lt;/code&gt;) using the &lt;a href="https://github.com/KhronosGroup/glTF/tree/master/extensions/2.0/Khronos/KHR_materials_pbrSpecularGlossiness"&gt;KHR_materials_pbrSpecularGlossiness&lt;/a&gt; extension.  This is an alternate PBR (Physically Based Rendering) workflow that gives model artists an extra degree of freedom over glTF's core metallic/roughness PBR workflow.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See the &lt;code&gt;README.md&lt;/code&gt; in each model's directory for license information.&lt;/p&gt;
&lt;p&gt;Sample models help the glTF ecosystem, if you are able to contribute a model, see the &lt;a href="#contributing-sample-models"&gt;contributing section&lt;/a&gt; below.&lt;/p&gt;
&lt;h2&gt;&lt;a id="user-content-model-publishing-services-with-gltf-download-capability" class="anchor" aria-hidden="true" href="#model-publishing-services-with-gltf-download-capability"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Model Publishing Services with glTF Download Capability&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://sketchfab.com/features/gltf" rel="nofollow"&gt;Sketchfab&lt;/a&gt; offers auto-conversion of all of its downloadable models, including PBR models, to glTF format.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.remix3d.com/" rel="nofollow"&gt;Microsoft's Remix3D&lt;/a&gt; can download 3D models into Paint3D, which offers a "Save As GLB" export (GLB is the binary form of glTF).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://poly.google.com/" rel="nofollow"&gt;Google's Poly&lt;/a&gt; offers certain 3D assets for download in glTF format.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-other-gltf-models" class="anchor" aria-hidden="true" href="#other-gltf-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Other glTF Models&lt;/h2&gt;
&lt;p&gt;For addition glTF models, see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/KhronosGroup/glTF-Asset-Generator"&gt;Khronos glTF Asset Generator&lt;/a&gt; offers an extensive suite of test models to excersise each part of the glTF specification.&lt;/li&gt;
&lt;li&gt;Cesium's &lt;a href="https://github.com/AnalyticalGraphicsInc/cesium/tree/master/Apps/SampleData/models"&gt;demo models&lt;/a&gt; and &lt;a href="https://github.com/AnalyticalGraphicsInc/cesium/tree/master/Specs/Data/Models"&gt;unit test models&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Flightradar24's &lt;a href="https://github.com/kalmykov/fr24-3d-models"&gt;GitHub repo&lt;/a&gt; of aircrafts.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://kenney.nl/assets?q=3d" rel="nofollow"&gt;Kenney â€¢ Assets&lt;/a&gt; hundreds of themed low-poly assets (nature, space, castle, furniture, etc.) provided by Kenney under CC0 licenses, including &lt;a href="https://kenney.nl/assets/pirate-kit" rel="nofollow"&gt;30+ pirate themed models&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a id="user-content-contributing-sample-models" class="anchor" aria-hidden="true" href="#contributing-sample-models"&gt;&lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"&gt;&lt;path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Contributing Sample Models&lt;/h2&gt;
&lt;p&gt;We appreciate sample model contributions; they help ensure a consistent glTF ecosystem.&lt;/p&gt;
&lt;p&gt;To contribute a model, open a pull request with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A new subdirectory containing
&lt;ul&gt;
&lt;li&gt;The model in as many glTF variations as reasonable (using the same directory structure as the others (&lt;a href="2.0/Box"&gt;example&lt;/a&gt;)). Tools for converting to glTF are &lt;a href="https://github.com/KhronosGroup/glTF#converters"&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;A screenshot of the model, stored in a subdirectory called &lt;code&gt;/screenshot&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A README.md with information about the model. As shown in &lt;a href="https://raw.githubusercontent.com/KhronosGroup/glTF-Sample-Models/master/2.0/Box/README.md" rel="nofollow"&gt;this example&lt;/a&gt;, this file should at least include the following elements:
&lt;ul&gt;
&lt;li&gt;The name or title of the model&lt;/li&gt;
&lt;li&gt;An embedding of the screenshot&lt;/li&gt;
&lt;li&gt;Information about the license under which the model is published. We recommend to use a permissive license like &lt;a href="http://creativecommons.org/licenses/by/4.0/" rel="nofollow"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt; or even &lt;a href="http://creativecommons.org/publicdomain/zero/1.0/" rel="nofollow"&gt;Creative Commons 1.0 Universal Public Domain Dedication &lt;/a&gt;, to allow people to share and adapt the models for their own use.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add the new model to the appropriate table.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you have any questions, submit an &lt;a href="https://github.com/KhronosGroup/glTF-Sample-Models/issues"&gt;issue&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;</description><author>KhronosGroup</author><guid isPermaLink="false">https://github.com/KhronosGroup/glTF-Sample-Models</guid><pubDate>Wed, 30 Oct 2019 00:00:00 GMT</pubDate></item></channel></rss>